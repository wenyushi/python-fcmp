{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swat import *\n",
    "import swat as sw\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import sys\n",
    "\n",
    "sys.path.append('C:\\\\Users\\\\weshiz\\\\Documents\\\\GitHub\\\\modify\\\\python-dlpy')\n",
    "sys.path.append('C:\\\\Users\\\\weshiz\\\\Documents\\\\GitHub\\\\python-fcmp')\n",
    "from dlpy.layers import * \n",
    "from dlpy.applications import *\n",
    "from dlpy import Model, Sequential\n",
    "from dlpy.utils import *\n",
    "from dlpy.splitting import two_way_split\n",
    "from dlpy.images import *\n",
    "from dlpy.model import *\n",
    "from src.parser import *\n",
    "from src.decorator import *\n",
    "from dlpy.lr_scheduler import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Added action set 'deeplearn'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; actionset</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>deeplearn</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.00352s</span> &#183; <span class=\"cas-user\">user 0.00285s</span> &#183; <span class=\"cas-sys\">sys 0.000648s</span> &#183; <span class=\"cas-memory\">mem 0.205MB</span></small></p>"
      ],
      "text/plain": [
       "[actionset]\n",
       "\n",
       " 'deeplearn'\n",
       "\n",
       "+ Elapsed: 0.00352s, user: 0.00285s, sys: 0.000648s, mem: 0.205mb"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = sw.CAS('dlgrd009', 13300)\n",
    "s.loadactionset('deeplearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: 'CASUSER(weshiz)' is now the active caslib.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.000168s</span> &#183; <span class=\"cas-user\">user 8.7e-05s</span> &#183; <span class=\"cas-sys\">sys 7.2e-05s</span> &#183; <span class=\"cas-memory\">mem 0.256MB</span></small></p>"
      ],
      "text/plain": [
       "+ Elapsed: 0.000168s, user: 8.7e-05s, sys: 7.2e-05s, mem: 0.256mb"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.sessionProp.setSessOpt(caslib='CASUSER', cmplib=\"CASUSER.fcmpfunction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCMP Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments, srcY, weights, y_out, are casted to array type\n",
      "Arguments, y_out, are declared as outargs.\n",
      "Arguments, srcY, Y, weights, deltas, gradient_out, srcDeltas_out, are casted to array type\n",
      "Arguments, gradient_out, srcDeltas_out, are declared as outargs.\n"
     ]
    }
   ],
   "source": [
    "@out_args('y_out')\n",
    "@cast_array('srcY', 'weights', 'y_out')\n",
    "def forward_prop(srcHeight, srcWidth, srcDepth, srcY, weights, y_out):\n",
    "    y_out[0] = srcY[0]**2 * weights[0] + weights[1]\n",
    "    return y_out[0]\n",
    "\n",
    "@out_args('gradient_out', 'srcDeltas_out')\n",
    "@cast_array('srcY', 'Y', 'weights', 'deltas', 'gradient_out', 'srcDeltas_out')\n",
    "def back_prop(srcHeight, srcWidth, srcDepth, srcY, Y, weights,\n",
    "              deltas, gradient_out, srcDeltas_out):\n",
    "    gradient_out[0] = deltas[0] * (srcY[0] ** 2)\n",
    "    gradient_out[1] = deltas[0]\n",
    "    srcDeltas_out[0] = deltas[0] * (2 * weights[0] * srcY[0] + weights[1])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('function forward_prop(srcHeight, srcWidth, srcDepth, srcY[*], weights[*], '\n",
      " 'y_out[*]);outargs y_out;\\n'\n",
      " '    y_out[1] = (srcY[1] ** 2 * weights[1] + weights[2]);\\n'\n",
      " '    return (y_out[1]);\\n'\n",
      " 'endsub;\\n')\n"
     ]
    }
   ],
   "source": [
    "# get FCMP code of forward_prop\n",
    "forward_fcmp_code = python_to_fcmp(forward_prop, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('function back_prop(srcHeight, srcWidth, srcDepth, srcY[*], Y[*], weights[*], '\n",
      " 'deltas[*], gradient_out[*], srcDeltas_out[*]);outargs gradient_out, '\n",
      " 'srcDeltas_out;\\n'\n",
      " '    gradient_out[1] = deltas[1] * srcY[1] ** 2;\\n'\n",
      " '    gradient_out[2] = deltas[1];\\n'\n",
      " '    srcDeltas_out[1] = deltas[1] * (2 * weights[1] * srcY[1] + weights[2]);\\n'\n",
      " '    return ;\\n'\n",
      " 'endsub;\\n')\n"
     ]
    }
   ],
   "source": [
    "# get FCMP code of back_prop\n",
    "backward_fcmp_code = python_to_fcmp(back_prop, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services saved the file FCMPFUNCTION.sashdat in caslib CASUSER(weshiz).\n"
     ]
    }
   ],
   "source": [
    "# register forward and backward function together\n",
    "register_fcmp_routines(s, routine_code=forward_fcmp_code + backward_fcmp_code, function_tbl_name='fcmpfunction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(weshiz)</td>\n",
       "      <td>fcmp_model</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>CASTable('fcmp_model', caslib='CASUSER(weshiz)')</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.00969s</span> &#183; <span class=\"cas-user\">user 0.00686s</span> &#183; <span class=\"cas-sys\">sys 0.014s</span> &#183; <span class=\"cas-memory\">mem 5.02MB</span></small></p>"
      ],
      "text/plain": [
       "[OutputCasTables]\n",
       "\n",
       "             casLib        Name  Rows  Columns  \\\n",
       " 0  CASUSER(weshiz)  fcmp_model    34        5   \n",
       " \n",
       "                                            casTable  \n",
       " 0  CASTable('fcmp_model', caslib='CASUSER(weshiz)')  \n",
       "\n",
       "+ Elapsed: 0.00969s, user: 0.00686s, sys: 0.014s, mem: 5.02mb"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.buildmodel(type='cnn', modeltable=dict(name='fcmp_model', replace=1))\n",
    "s.addlayer(layer=dict(type=\"input\"), name=\"input\",\n",
    "           model=\"fcmp_model\")\n",
    "s.addlayer(layer=dict(type=\"FCMP\", forwardFunc=\"forward_prop\", backwardFunc=\"back_prop\", \n",
    "           height=1, width=1, depth=1, nWeights=2), name=\"fcmp_layer\", srcLayers=\"input\",\n",
    "           model=\"fcmp_model\")\n",
    "s.addlayer(layer=dict(type=\"output\", error=\"NORMAL\",  includeBias=False, fullConnect=False, act=\"IDENTITY\"), \n",
    "           srcLayers='fcmp_layer',\n",
    "           name=\"output\", model=\"fcmp_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, b are weights to be learned\n",
    "a = 3.5\n",
    "b = 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33308802, 0.70717241, 0.67213261, 0.48323607, 0.37469729,\n",
       "       0.33251719, 0.67096248, 0.24254784, 0.25777387, 0.07585573,\n",
       "       0.55091958, 0.07513484, 0.62268136, 0.18939629, 0.18061411,\n",
       "       0.56732067, 0.34303274, 0.35467101, 0.54940535, 0.0098571 ,\n",
       "       0.45668146, 0.48609047, 0.27879276, 0.10019084, 0.13812319,\n",
       "       0.16815841, 0.92279466, 0.35668288, 0.42447938, 0.73507596,\n",
       "       0.80456158, 0.60533332, 0.794088  , 0.50830447, 0.07616976,\n",
       "       0.34357565, 0.35655163, 0.52023852, 0.11077409, 0.65796409,\n",
       "       0.12505299, 0.53832619, 0.88139624, 0.92504791, 0.71166064,\n",
       "       0.42843404, 0.88621664, 0.96625217, 0.91323878, 0.07242452])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.random(50)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00792801,  0.00196627, -0.00470699,  0.00166786, -0.00064238,\n",
       "       -0.01800476, -0.00860187, -0.00585783, -0.01175271,  0.00043938,\n",
       "       -0.00428229,  0.0060955 , -0.00358657,  0.01114719,  0.00594957,\n",
       "       -0.0002198 ,  0.00198887, -0.00150801, -0.00037054,  0.002976  ,\n",
       "        0.00654023,  0.01342725,  0.0159679 , -0.00169169,  0.00244786,\n",
       "       -0.00046402, -0.01878729,  0.01444742,  0.01162142, -0.01821125,\n",
       "        0.00631519,  0.00772759, -0.00596985,  0.01044149, -0.00849968,\n",
       "        0.02220993, -0.00590816,  0.02027319, -0.02643683, -0.00382042,\n",
       "        0.014522  ,  0.00441466, -0.01096772, -0.00463771, -0.00308117,\n",
       "        0.0302887 ,  0.01779232, -0.00500663, -0.01324107, -0.00304678])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy = np.random.normal(size=50) / 100\n",
    "noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.49624471, 6.85229113, 6.67646087, 5.9189777 , 5.59075081,\n",
       "       5.46898213, 6.66706541, 5.30004526, 5.32081308, 5.1205787 ,\n",
       "       6.15801103, 5.12585385, 6.45347569, 5.23669553, 5.22012467,\n",
       "       6.22626478, 5.51383899, 5.53876232, 6.1560913 , 5.10331607,\n",
       "       5.83649308, 5.94042105, 5.38800681, 5.13344202, 5.16922092,\n",
       "       5.19850636, 8.06163766, 5.55972678, 5.74226104, 6.97296709,\n",
       "       7.37193286, 6.3902271 , 7.3010453 , 6.0147485 , 5.11180674,\n",
       "       5.53536473, 5.53904357, 6.0675416 , 5.11651132, 6.61138819,\n",
       "       5.16925587, 6.11869745, 7.80803996, 8.09035998, 6.86953185,\n",
       "       5.77273375, 7.86662206, 8.36274476, 8.00577668, 5.11531181])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = a * (x**2) + b + noisy\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data = pd.DataFrame(data={'x': x, 'y': y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services made the uploaded file available as table TRAIN in caslib CASUSER(weshiz).\n",
      "NOTE: The table TRAIN has been created in caslib CASUSER(weshiz) from binary data uploaded to Cloud Analytic Services.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CASTable('TRAIN', caslib='CASUSER(weshiz)')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.upload_frame(data=p_data, casout=dict(name='train', replace=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.5, 5.1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Client to check forward and backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.488316699687349\n",
      "6.850324866270935\n",
      "6.681167866391715\n",
      "5.917309837538583\n",
      "5.59139319853788\n",
      "5.486986891576672\n",
      "6.67566727920412\n",
      "5.305903092007399\n",
      "5.332565784860756\n",
      "5.120139319018674\n",
      "6.162293324414092\n",
      "5.119758353967989\n",
      "6.457062258238812\n",
      "5.225548342167505\n",
      "5.214175100634697\n",
      "6.226484580861884\n",
      "5.51185011230437\n",
      "5.540270327670545\n",
      "6.156461842355472\n",
      "5.10034006863685\n",
      "5.829952846228029\n",
      "5.926993795170359\n",
      "5.372038908444681\n",
      "5.13513371715279\n",
      "5.1667730585675\n",
      "5.1989703765612285\n",
      "8.080424948857413\n",
      "5.545279363752737\n",
      "5.730639616479105\n",
      "6.991178344688045\n",
      "7.365617669458094\n",
      "6.382499505184475\n",
      "7.307015156238544\n",
      "6.0043070066505795\n",
      "5.120306413465875\n",
      "5.513154803545204\n",
      "5.544951728310419\n",
      "6.047268409828857\n",
      "5.142948148212862\n",
      "6.615208608186972\n",
      "5.154733872280525\n",
      "6.1142827914166675\n",
      "7.819007684181701\n",
      "8.094997693254268\n",
      "6.872613028453529\n",
      "5.742445055569717\n",
      "7.848829747503501\n",
      "8.36775139194212\n",
      "8.019017754842462\n",
      "5.1183585893222885\n"
     ]
    }
   ],
   "source": [
    "dummy = 0\n",
    "for i in x:\n",
    "    print(forward_prop(dummy, dummy, dummy, [i], [a, b], [dummy]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 2.\n",
      "NOTE:  The approximate memory cost is 1.00 MB.\n",
      "NOTE:  Loading weights cost       0.00 (s).\n",
      "NOTE:  Initializing each layer cost       0.01 (s).\n",
      "NOTE:  The total number of threads on each worker is 1.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 10.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 10.\n",
      "NOTE:  Target variable: y\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of numeric input variables:      1\n",
      "NOTE:  Number of FCMP layers in model: 1\n",
      "NOTE:  FCMP layer 'fcmp_layer' has input tensor size: width=1, height=1, depth=1\n",
      "NOTE:  FCMP layer 'fcmp_layer' has output tensor size: width=1, height=1, depth=1\n",
      "NOTE:  FCMP layer 'fcmp_layer' has 2 weights.\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1            12.95       25.9     0.00\n",
      "NOTE:      1    10        1           0.2768     0.5535     0.00\n",
      "NOTE:      2    10        1          0.01626    0.03252     0.00\n",
      "NOTE:      3    10        1          0.03096    0.06191     0.00\n",
      "NOTE:      4    10        1           0.0104    0.02081     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  0             1           2.657      5.314     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1          0.01389    0.02779     0.00\n",
      "NOTE:      1    10        1          0.03172    0.06344     0.00\n",
      "NOTE:      2    10        1          0.02288    0.04575     0.00\n",
      "NOTE:      3    10        1         0.005039    0.01008     0.00\n",
      "NOTE:      4    10        1         0.006641    0.01328     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  1             1         0.01603    0.03207     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1         0.008107    0.01621     0.00\n",
      "NOTE:      1    10        1          0.00593    0.01186     0.00\n",
      "NOTE:      2    10        1         0.004375    0.00875     0.00\n",
      "NOTE:      3    10        1         0.003107   0.006214     0.00\n",
      "NOTE:      4    10        1         0.004935   0.009871     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  2             1        0.005291    0.01058     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1         0.004294   0.008589     0.00\n",
      "NOTE:      1    10        1         0.001672   0.003345     0.00\n",
      "NOTE:      2    10        1         0.001503   0.003006     0.00\n",
      "NOTE:      3    10        1         0.002391   0.004781     0.00\n",
      "NOTE:      4    10        1         0.002291   0.004583     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  3             1         0.00243   0.004861     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1         0.001723   0.003445     0.00\n",
      "NOTE:      1    10        1        0.0004118  0.0008237     0.00\n",
      "NOTE:      2    10        1         0.001065   0.002131     0.00\n",
      "NOTE:      3    10        1        0.0006971   0.001394     0.00\n",
      "NOTE:      4    10        1         0.001028   0.002057     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  4             1        0.000985    0.00197     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        0.0008253   0.001651     0.00\n",
      "NOTE:      1    10        1        0.0005841   0.001168     0.00\n",
      "NOTE:      2    10        1        0.0004315   0.000863     0.00\n",
      "NOTE:      3    10        1        0.0006978   0.001396     0.00\n",
      "NOTE:      4    10        1         0.000136   0.000272     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  5             1       0.0005349    0.00107     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        0.0003003  0.0006007     0.00\n",
      "NOTE:      1    10        1        9.429e-05  0.0001886     0.00\n",
      "NOTE:      2    10        1        0.0001682  0.0003364     0.00\n",
      "NOTE:      3    10        1        0.0002505  0.0005009     0.00\n",
      "NOTE:      4    10        1         0.000176   0.000352     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  6             1       0.0001979  0.0003957     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        8.954e-05  0.0001791     0.00\n",
      "NOTE:      1    10        1         8.85e-05   0.000177     0.00\n",
      "NOTE:      2    10        1        6.096e-05  0.0001219     0.00\n",
      "NOTE:      3    10        1        6.867e-05  0.0001373     0.00\n",
      "NOTE:      4    10        1        0.0001042  0.0002083     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  7             1       8.237e-05  0.0001647     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        0.0001427  0.0002854     0.00\n",
      "NOTE:      1    10        1        9.164e-05  0.0001833     0.00\n",
      "NOTE:      2    10        1        0.0001068  0.0002136     0.00\n",
      "NOTE:      3    10        1        6.522e-05  0.0001304     0.00\n",
      "NOTE:      4    10        1        3.908e-05  7.817e-05     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  8             1       8.909e-05  0.0001782     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        6.729e-05  0.0001346     0.00\n",
      "NOTE:      1    10        1        0.0001132  0.0002264     0.00\n",
      "NOTE:      2    10        1        4.974e-05  9.947e-05     0.00\n",
      "NOTE:      3    10        1        2.115e-05   4.23e-05     0.00\n",
      "NOTE:      4    10        1        7.129e-05  0.0001426     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  9             1       6.454e-05  0.0001291     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        4.845e-05   9.69e-05     0.00\n",
      "NOTE:      1    10        1        5.897e-05  0.0001179     0.00\n",
      "NOTE:      2    10        1        6.997e-05  0.0001399     0.00\n",
      "NOTE:      3    10        1        2.693e-05  5.385e-05     0.00\n",
      "NOTE:      4    10        1        6.283e-05  0.0001257     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  10            1       5.343e-05  0.0001069     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        4.478e-05  8.956e-05     0.00\n",
      "NOTE:      1    10        1        0.0001086  0.0002171     0.00\n",
      "NOTE:      2    10        1        5.915e-05  0.0001183     0.00\n",
      "NOTE:      3    10        1        1.868e-05  3.735e-05     0.00\n",
      "NOTE:      4    10        1        2.609e-05  5.218e-05     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  11            1       5.145e-05  0.0001029     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        3.467e-05  6.934e-05     0.00\n",
      "NOTE:      1    10        1        3.571e-05  7.143e-05     0.00\n",
      "NOTE:      2    10        1        0.0001272  0.0002545     0.00\n",
      "NOTE:      3    10        1        4.365e-05   8.73e-05     0.00\n",
      "NOTE:      4    10        1        5.929e-05  0.0001186     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  12            1       6.011e-05  0.0001202     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        3.628e-05  7.256e-05     0.00\n",
      "NOTE:      1    10        1        1.738e-05  3.476e-05     0.00\n",
      "NOTE:      2    10        1        4.967e-05  9.935e-05     0.00\n",
      "NOTE:      3    10        1        2.822e-05  5.644e-05     0.00\n",
      "NOTE:      4    10        1         4.75e-05  9.501e-05     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  13            1       3.581e-05  7.162e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        5.867e-05  0.0001173     0.00\n",
      "NOTE:      1    10        1        2.686e-05  5.372e-05     0.00\n",
      "NOTE:      2    10        1         4.35e-05    8.7e-05     0.00\n",
      "NOTE:      3    10        1          5.3e-05   0.000106     0.00\n",
      "NOTE:      4    10        1         2.48e-05   4.96e-05     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  14            1       4.137e-05  8.273e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        4.176e-05  8.352e-05     0.00\n",
      "NOTE:      1    10        1        5.877e-05  0.0001175     0.00\n",
      "NOTE:      2    10        1        4.464e-05  8.928e-05     0.00\n",
      "NOTE:      3    10        1        2.055e-05   4.11e-05     0.00\n",
      "NOTE:      4    10        1        2.492e-05  4.984e-05     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  15            1       3.813e-05  7.625e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        4.771e-05  9.541e-05     0.00\n",
      "NOTE:      1    10        1         4.26e-05  8.519e-05     0.00\n",
      "NOTE:      2    10        1        5.411e-05  0.0001082     0.00\n",
      "NOTE:      3    10        1        3.148e-05  6.295e-05     0.00\n",
      "NOTE:      4    10        1         3.38e-05  6.759e-05     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  16            1       4.194e-05  8.387e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        3.237e-05  6.473e-05     0.00\n",
      "NOTE:      1    10        1        2.217e-05  4.434e-05     0.00\n",
      "NOTE:      2    10        1        4.847e-05  9.693e-05     0.00\n",
      "NOTE:      3    10        1         2.31e-05   4.62e-05     0.00\n",
      "NOTE:      4    10        1        3.039e-05  6.079e-05     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  17            1        3.13e-05   6.26e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1         3.27e-05   6.54e-05     0.00\n",
      "NOTE:      1    10        1        9.312e-06  1.862e-05     0.00\n",
      "NOTE:      2    10        1        4.723e-05  9.447e-05     0.00\n",
      "NOTE:      3    10        1        2.632e-05  5.265e-05     0.00\n",
      "NOTE:      4    10        1        4.678e-05  9.357e-05     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  18            1       3.247e-05  6.494e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        5.751e-05   0.000115     0.00\n",
      "NOTE:      1    10        1        0.0001131  0.0002261     0.00\n",
      "NOTE:      2    10        1        0.0001908  0.0003815     0.00\n",
      "NOTE:      3    10        1        9.355e-05  0.0001871     0.00\n",
      "NOTE:      4    10        1        3.379e-05  6.757e-05     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  19            1       9.773e-05  0.0001955     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        6.693e-05  0.0001339     0.00\n",
      "NOTE:      1    10        1        5.218e-05  0.0001044     0.00\n",
      "NOTE:      2    10        1        3.441e-05  6.882e-05     0.00\n",
      "NOTE:      3    10        1        4.266e-05  8.532e-05     0.00\n",
      "NOTE:      4    10        1        5.109e-05  0.0001022     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  20            1       4.945e-05   9.89e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        2.179e-05  4.359e-05     0.00\n",
      "NOTE:      1    10        1        5.838e-05  0.0001168     0.00\n",
      "NOTE:      2    10        1        5.499e-05    0.00011     0.00\n",
      "NOTE:      3    10        1        3.413e-05  6.826e-05     0.00\n",
      "NOTE:      4    10        1         7.13e-05  0.0001426     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  21            1       4.812e-05  9.624e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        9.554e-05  0.0001911     0.00\n",
      "NOTE:      1    10        1        6.057e-05  0.0001211     0.00\n",
      "NOTE:      2    10        1        1.504e-05  3.008e-05     0.00\n",
      "NOTE:      3    10        1        2.768e-05  5.536e-05     0.00\n",
      "NOTE:      4    10        1        2.113e-05  4.226e-05     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  22            1       4.399e-05  8.798e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        3.389e-05  6.778e-05     0.00\n",
      "NOTE:      1    10        1        3.898e-05  7.796e-05     0.00\n",
      "NOTE:      2    10        1        2.275e-05   4.55e-05     0.00\n",
      "NOTE:      3    10        1        7.135e-05  0.0001427     0.00\n",
      "NOTE:      4    10        1        6.968e-05  0.0001394     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  23            1       4.733e-05  9.466e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        3.495e-05  6.989e-05     0.00\n",
      "NOTE:      1    10        1        5.255e-05  0.0001051     0.00\n",
      "NOTE:      2    10        1        4.933e-05  9.867e-05     0.00\n",
      "NOTE:      3    10        1         4.79e-05   9.58e-05     0.00\n",
      "NOTE:      4    10        1        5.286e-05  0.0001057     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  24            1       4.752e-05  9.504e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        4.016e-05  8.032e-05     0.00\n",
      "NOTE:      1    10        1        3.045e-05   6.09e-05     0.00\n",
      "NOTE:      2    10        1        2.622e-05  5.243e-05     0.00\n",
      "NOTE:      3    10        1        1.993e-05  3.986e-05     0.00\n",
      "NOTE:      4    10        1        5.495e-05  0.0001099     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  25            1       3.434e-05  6.868e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        4.799e-05  9.597e-05     0.00\n",
      "NOTE:      1    10        1        8.012e-05  0.0001602     0.00\n",
      "NOTE:      2    10        1        2.419e-05  4.838e-05     0.00\n",
      "NOTE:      3    10        1        6.011e-05  0.0001202     0.00\n",
      "NOTE:      4    10        1        4.951e-05  9.903e-05     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  26            1       5.238e-05  0.0001048     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        7.343e-05  0.0001469     0.00\n",
      "NOTE:      1    10        1        5.241e-05  0.0001048     0.00\n",
      "NOTE:      2    10        1        2.877e-05  5.754e-05     0.00\n",
      "NOTE:      3    10        1        2.976e-05  5.951e-05     0.00\n",
      "NOTE:      4    10        1         3.17e-05  6.341e-05     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  27            1       4.321e-05  8.643e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        3.984e-05  7.967e-05     0.00\n",
      "NOTE:      1    10        1        5.998e-05    0.00012     0.00\n",
      "NOTE:      2    10        1        4.181e-05  8.361e-05     0.00\n",
      "NOTE:      3    10        1        3.229e-05  6.459e-05     0.00\n",
      "NOTE:      4    10        1        4.289e-05  8.579e-05     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  28            1       4.336e-05  8.672e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        3.023e-05  6.045e-05     0.00\n",
      "NOTE:      1    10        1        5.621e-05  0.0001124     0.00\n",
      "NOTE:      2    10        1         2.88e-05   5.76e-05     0.00\n",
      "NOTE:      3    10        1        3.351e-05  6.701e-05     0.00\n",
      "NOTE:      4    10        1        5.847e-05  0.0001169     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  29            1       4.144e-05  8.288e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1         5.25e-05   0.000105     0.00\n",
      "NOTE:      1    10        1        0.0001563  0.0003126     0.00\n",
      "NOTE:      2    10        1        9.755e-05  0.0001951     0.00\n",
      "NOTE:      3    10        1        2.026e-05  4.052e-05     0.00\n",
      "NOTE:      4    10        1        3.004e-05  6.009e-05     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  30            1       7.132e-05  0.0001426     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        5.323e-05  0.0001065     0.00\n",
      "NOTE:      1    10        1        5.153e-05  0.0001031     0.00\n",
      "NOTE:      2    10        1        4.914e-05  9.828e-05     0.00\n",
      "NOTE:      3    10        1        3.096e-05  6.192e-05     0.00\n",
      "NOTE:      4    10        1        4.034e-05  8.068e-05     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  31            1       4.504e-05  9.008e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        3.352e-05  6.705e-05     0.00\n",
      "NOTE:      1    10        1        5.291e-05  0.0001058     0.00\n",
      "NOTE:      2    10        1        4.478e-05  8.955e-05     0.00\n",
      "NOTE:      3    10        1        3.472e-05  6.945e-05     0.00\n",
      "NOTE:      4    10        1        8.145e-05  0.0001629     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  32            1       4.948e-05  9.895e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        4.352e-05  8.704e-05     0.00\n",
      "NOTE:      1    10        1        1.592e-05  3.184e-05     0.00\n",
      "NOTE:      2    10        1        4.305e-05  8.611e-05     0.00\n",
      "NOTE:      3    10        1        3.084e-05  6.169e-05     0.00\n",
      "NOTE:      4    10        1         4.77e-05   9.54e-05     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  33            1       3.621e-05  7.242e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        6.006e-05  0.0001201     0.00\n",
      "NOTE:      1    10        1        3.903e-05  7.806e-05     0.00\n",
      "NOTE:      2    10        1        7.209e-05  0.0001442     0.00\n",
      "NOTE:      3    10        1        2.783e-05  5.567e-05     0.00\n",
      "NOTE:      4    10        1        2.561e-05  5.123e-05     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  34            1       4.493e-05  8.985e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        4.225e-05   8.45e-05     0.00\n",
      "NOTE:      1    10        1        3.487e-05  6.974e-05     0.00\n",
      "NOTE:      2    10        1        2.519e-05  5.039e-05     0.00\n",
      "NOTE:      3    10        1        3.754e-05  7.508e-05     0.00\n",
      "NOTE:      4    10        1        2.662e-05  5.324e-05     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  35            1       3.329e-05  6.659e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1            4e-05  8.001e-05     0.00\n",
      "NOTE:      1    10        1        6.418e-05  0.0001284     0.00\n",
      "NOTE:      2    10        1        2.388e-05  4.775e-05     0.00\n",
      "NOTE:      3    10        1        8.932e-05  0.0001786     0.00\n",
      "NOTE:      4    10        1        6.566e-05  0.0001313     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  36            1       5.661e-05  0.0001132     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        5.605e-05  0.0001121     0.00\n",
      "NOTE:      1    10        1        4.428e-05  8.855e-05     0.00\n",
      "NOTE:      2    10        1        3.652e-05  7.303e-05     0.00\n",
      "NOTE:      3    10        1        3.985e-05  7.971e-05     0.00\n",
      "NOTE:      4    10        1        3.145e-05   6.29e-05     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  37            1       4.163e-05  8.326e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        3.972e-05  7.945e-05     0.00\n",
      "NOTE:      1    10        1        1.025e-05   2.05e-05     0.00\n",
      "NOTE:      2    10        1        3.797e-05  7.594e-05     0.00\n",
      "NOTE:      3    10        1        8.496e-05  0.0001699     0.00\n",
      "NOTE:      4    10        1        6.338e-05  0.0001268     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  38            1       4.726e-05  9.452e-05     0.00\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    10        1        6.119e-05  0.0001224     0.00\n",
      "NOTE:      1    10        1        4.287e-05  8.574e-05     0.00\n",
      "NOTE:      2    10        1        6.492e-05  0.0001298     0.00\n",
      "NOTE:      3    10        1        6.481e-05  0.0001296     0.00\n",
      "NOTE:      4    10        1        4.691e-05  9.383e-05     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  39            1       5.614e-05  0.0001123     0.00\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is       0.01 (s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Name</td>\n",
       "      <td>fcmp_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Type</td>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number of Layers</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number of Input Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number of Output Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Number of Convolutional Layers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Number of Pooling Layers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Number of Fully Connected Layers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Number of FCMP Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Number of Weight Parameters</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Total Number of Model Parameters</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Approximate Memory Cost for Training (MB)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OptIterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch\">Epoch</th>\n",
       "      <th title=\"LearningRate\">LearningRate</th>\n",
       "      <th title=\"Loss\">Loss</th>\n",
       "      <th title=\"FitError\">FitError</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.657177</td>\n",
       "      <td>5.314354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.016034</td>\n",
       "      <td>0.032069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.010582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002430</td>\n",
       "      <td>0.004861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.001970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.001070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(weshiz)</td>\n",
       "      <td>weights</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>CASTable('weights', caslib='CASUSER(weshiz)')</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.0192s</span> &#183; <span class=\"cas-user\">user 0.0154s</span> &#183; <span class=\"cas-sys\">sys 0.00362s</span> &#183; <span class=\"cas-memory\">mem 4.07MB</span></small></p>"
      ],
      "text/plain": [
       "[ModelInfo]\n",
       "\n",
       "                                         Descr                         Value\n",
       " 0                                  Model Name                    fcmp_model\n",
       " 1                                  Model Type  Convolutional Neural Network\n",
       " 2                            Number of Layers                             3\n",
       " 3                      Number of Input Layers                             1\n",
       " 4                     Number of Output Layers                             1\n",
       " 5              Number of Convolutional Layers                             0\n",
       " 6                    Number of Pooling Layers                             0\n",
       " 7            Number of Fully Connected Layers                             0\n",
       " 8                       Number of FCMP Layers                             1\n",
       " 9                 Number of Weight Parameters                             2\n",
       " 10           Total Number of Model Parameters                             2\n",
       " 11  Approximate Memory Cost for Training (MB)                             1\n",
       "\n",
       "[OptIterHistory]\n",
       "\n",
       "     Epoch  LearningRate      Loss  FitError\n",
       " 0     0.0           1.0  2.657177  5.314354\n",
       " 1     1.0           1.0  0.016034  0.032069\n",
       " 2     2.0           1.0  0.005291  0.010582\n",
       " 3     3.0           1.0  0.002430  0.004861\n",
       " 4     4.0           1.0  0.000985  0.001970\n",
       " 5     5.0           1.0  0.000535  0.001070\n",
       " 6     6.0           1.0  0.000198  0.000396\n",
       " 7     7.0           1.0  0.000082  0.000165\n",
       " 8     8.0           1.0  0.000089  0.000178\n",
       " 9     9.0           1.0  0.000065  0.000129\n",
       " 10   10.0           1.0  0.000053  0.000107\n",
       " 11   11.0           1.0  0.000051  0.000103\n",
       " 12   12.0           1.0  0.000060  0.000120\n",
       " 13   13.0           1.0  0.000036  0.000072\n",
       " 14   14.0           1.0  0.000041  0.000083\n",
       " 15   15.0           1.0  0.000038  0.000076\n",
       " 16   16.0           1.0  0.000042  0.000084\n",
       " 17   17.0           1.0  0.000031  0.000063\n",
       " 18   18.0           1.0  0.000032  0.000065\n",
       " 19   19.0           1.0  0.000098  0.000195\n",
       " 20   20.0           1.0  0.000049  0.000099\n",
       " 21   21.0           1.0  0.000048  0.000096\n",
       " 22   22.0           1.0  0.000044  0.000088\n",
       " 23   23.0           1.0  0.000047  0.000095\n",
       " 24   24.0           1.0  0.000048  0.000095\n",
       " 25   25.0           1.0  0.000034  0.000069\n",
       " 26   26.0           1.0  0.000052  0.000105\n",
       " 27   27.0           1.0  0.000043  0.000086\n",
       " 28   28.0           1.0  0.000043  0.000087\n",
       " 29   29.0           1.0  0.000041  0.000083\n",
       " 30   30.0           1.0  0.000071  0.000143\n",
       " 31   31.0           1.0  0.000045  0.000090\n",
       " 32   32.0           1.0  0.000049  0.000099\n",
       " 33   33.0           1.0  0.000036  0.000072\n",
       " 34   34.0           1.0  0.000045  0.000090\n",
       " 35   35.0           1.0  0.000033  0.000067\n",
       " 36   36.0           1.0  0.000057  0.000113\n",
       " 37   37.0           1.0  0.000042  0.000083\n",
       " 38   38.0           1.0  0.000047  0.000095\n",
       " 39   39.0           1.0  0.000056  0.000112\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib     Name  Rows  Columns  \\\n",
       " 0  CASUSER(weshiz)  weights     2        3   \n",
       " \n",
       "                                         casTable  \n",
       " 0  CASTable('weights', caslib='CASUSER(weshiz)')  \n",
       "\n",
       "+ Elapsed: 0.0192s, user: 0.0154s, sys: 0.00362s, mem: 4.07mb"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer=dict(miniBatchSize=10, logLevel=3, \n",
    "               maxEpochs=40,# regL2=0.01,\n",
    "               algorithm=dict(method='VANILLA', \n",
    "                              clipGradMax=100, clipGradMin=-100,\n",
    "                              learningRate=1, gamma=0.1, \n",
    "                              lrpolicy='step', stepsize=50\n",
    "                             )\n",
    "              )\n",
    "\n",
    "s.dltrain(table='train', inputs='x',\n",
    "          target='y',\n",
    "          modelweights=dict(name='weights', replace=1),\n",
    "          modeltable='fcmp_model', nthreads=1, recordseed=13309,\n",
    "          optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; Fetch</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><caption>Selected Rows from Table WEIGHTS</caption>\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"_LayerID_\">_LayerID_</th>\n",
       "      <th title=\"_WeightID_\">_WeightID_</th>\n",
       "      <th title=\"_Weight_\">_Weight_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.499745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.098710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.00072s</span> &#183; <span class=\"cas-user\">user 0.000337s</span> &#183; <span class=\"cas-sys\">sys 0.000363s</span> &#183; <span class=\"cas-memory\">mem 0.975MB</span></small></p>"
      ],
      "text/plain": [
       "[Fetch]\n",
       "\n",
       " Selected Rows from Table WEIGHTS\n",
       " \n",
       "    _LayerID_  _WeightID_  _Weight_\n",
       " 0        1.0         0.0  3.499745\n",
       " 1        1.0         1.0  5.098710\n",
       "\n",
       "+ Elapsed: 0.00072s, user: 0.000337s, sys: 0.000363s, mem: 0.975mb"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.fetch('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 9.9e-05s</span> &#183; <span class=\"cas-sys\">sys 8.8e-05s</span> &#183; <span class=\"cas-memory\">mem 0.203MB</span></small></p>"
      ],
      "text/plain": [
       "+ Elapsed: 9.9e-05s, sys: 8.8e-05s, mem: 0.203mb"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.endsession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
