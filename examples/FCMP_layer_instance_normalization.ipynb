{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swat import *\n",
    "import swat as sw\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import sys\n",
    "\n",
    "sys.path.append('C:\\\\Users\\\\weshiz\\\\Documents\\\\GitHub\\\\modify\\\\python-dlpy')\n",
    "sys.path.append('C:\\\\Users\\\\weshiz\\\\Documents\\\\GitHub\\\\python-fcmp')\n",
    "from dlpy.layers import * \n",
    "from dlpy.applications import *\n",
    "from dlpy import Model, Sequential\n",
    "from dlpy.utils import *\n",
    "from dlpy.splitting import two_way_split\n",
    "from dlpy.images import *\n",
    "from dlpy.model import *\n",
    "from python_fcmp.parser import *\n",
    "from python_fcmp.decorator import *\n",
    "from python_fcmp import fcmp\n",
    "from dlpy.lr_scheduler import *\n",
    "from dlpy.network import *\n",
    "\n",
    "import numpy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Added action set 'deeplearn'.\n",
      "NOTE: Added action set 'clustering'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; actionset</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>clustering</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.00171s</span> &#183; <span class=\"cas-user\">user 0.000395s</span> &#183; <span class=\"cas-sys\">sys 0.0013s</span> &#183; <span class=\"cas-memory\">mem 0.209MB</span></small></p>"
      ],
      "text/plain": [
       "[actionset]\n",
       "\n",
       " 'clustering'\n",
       "\n",
       "+ Elapsed: 0.00171s, user: 0.000395s, sys: 0.0013s, mem: 0.209mb"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = sw.CAS('dlgrd009', 13300)\n",
    "s.loadactionset('deeplearn')\n",
    "s.loadactionset('clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: 'CASUSER(weshiz)' is now the active caslib.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.000199s</span> &#183; <span class=\"cas-user\">user 0.000189s</span> &#183; <span class=\"cas-memory\">mem 0.256MB</span></small></p>"
      ],
      "text/plain": [
       "+ Elapsed: 0.000199s, user: 0.000189s, mem: 0.256mb"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://go.documentation.sas.com/?docsetId=lesysoptsref&docsetTarget=n16rhscxem9ljwn1kuhn5170xkbg.htm&docsetVersion=9.4&locale=en\n",
    "s.sessionProp.setSessOpt(caslib='CASUSER', cmplib=\"CASUSER.fcmpfunction\", cmpopt=\"ALL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define forward and backward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments, srcY, weights, y_out, are casted to array type\n",
      "Arguments, y_out, are declared as outargs.\n",
      "Arguments, srcY, Y, weights, deltas, gradient_out, srcDeltas_out, are casted to array type\n",
      "Arguments, gradient_out, srcDeltas_out, are declared as outargs.\n"
     ]
    }
   ],
   "source": [
    "@out_args('y_out')  # pass by reference\n",
    "@cast_array('srcY', 'weights', 'y_out')  # declare the arguments as array type\n",
    "def forward_prop(srcHeight, srcWidth, srcDepth, srcY, weights, y_out):\n",
    "    mean = numpy.zeros((6)) # initialize mean and var with length\n",
    "    var = numpy.zeros((6))\n",
    "    \n",
    "    wd = fcmp.reduce_axis((0, srcWidth)) # create an iterator which iterate from 0 to srcWidth\n",
    "    ht = fcmp.reduce_axis((0, srcHeight))\n",
    "    eps = 0.0001 # avoid deviding by 0\n",
    "    srcY = fcmp.reshape(srcY, (srcDepth, srcHeight, srcWidth)) # reshape the array and access each element via multiple subscripts\n",
    "    mean = fcmp.compute((srcDepth), lambda i: fcmp.sum(srcY[i, ht, wd] / srcHeight / srcWidth, [ht, wd])) # generate a new variable and describle the computation rule\n",
    "    var = fcmp.compute((srcDepth), lambda i: fcmp.sum((srcY[i, ht, wd] - mean[i]) ** 2 / srcHeight / srcWidth, [ht, wd]))\n",
    "    y_out = fcmp.compute((srcDepth, srcHeight, srcWidth), lambda i, j, m: (srcY[i, j, m] - mean[i]) / (var[i] + eps) ** 0.5)\n",
    "    \n",
    "    return\n",
    "\n",
    "@out_args('gradient_out', 'srcDeltas_out')\n",
    "@cast_array('srcY', 'Y', 'weights', 'deltas', 'gradient_out', 'srcDeltas_out')\n",
    "def back_prop(srcHeight, srcWidth, srcDepth, srcY, Y, weights, deltas, gradient_out, srcDeltas_out):\n",
    "    dmean = numpy.zeros((6))\n",
    "    dvar = numpy.zeros((6))\n",
    "    mean = numpy.zeros((6))\n",
    "    var = numpy.zeros((6))\n",
    "    grad_sigma = numpy.zeros((6))\n",
    "    grad_mean = numpy.zeros((6))\n",
    "    grad_mean_2 = numpy.zeros((6))\n",
    "    for i in range(srcHeight*srcWidth*srcDepth):\n",
    "        srcDeltas_out[i] = 0\n",
    "    \n",
    "    by_norm = 1 / (srcHeight * srcWidth)\n",
    "    \n",
    "    wd = fcmp.reduce_axis((0, srcWidth))\n",
    "    ht = fcmp.reduce_axis((0, srcHeight))\n",
    "    \n",
    "    srcY = fcmp.reshape(srcY, (srcDepth, srcHeight, srcWidth))\n",
    "    deltas = fcmp.reshape(deltas, (srcDepth, srcHeight, srcWidth))\n",
    "    \n",
    "    eps = 0.0001\n",
    "    \n",
    "    mean = fcmp.compute((srcDepth), lambda i: fcmp.sum(srcY[i, ht, wd] / srcHeight / srcWidth, [ht, wd]))\n",
    "    var = fcmp.compute((srcDepth), lambda i: fcmp.sum((srcY[i, ht, wd] - mean[i]) ** 2 / srcHeight / srcWidth, [ht, wd]))\n",
    "    \n",
    "    grad_sigma = fcmp.compute((srcDepth), lambda i: fcmp.sum(deltas[i, ht, wd] * (srcY[i] - mean[i]), [ht, wd]))\n",
    "    grad_sigma = fcmp.compute((srcDepth), lambda i: grad_sigma[i] * (-0.5) * (var[i] + eps) ** -1.5)\n",
    "    \n",
    "    grad_mean = fcmp.compute((srcDepth), lambda i: fcmp.sum(deltas[i, ht, wd] * (-1 * (var[i] + eps) ** 0.5), [ht, wd]))\n",
    "    grad_mean_2 = fcmp.compute((srcDepth), lambda i: fcmp.sum(srcY[i, ht, wd] - mean[i], [ht, wd]))\n",
    "    grad_mean = fcmp.compute((srcDepth), lambda i: grad_mean[i] + grad_sigma[i] * by_norm * 2 * grad_mean_2[i] * -1)\n",
    "    \n",
    "    srcDeltas_out = fcmp.compute((srcDepth), lambda i: fcmp.sum(srcY[i, ht, wd] - mean[i], [ht, wd]))\n",
    "    srcDeltas_out = fcmp.compute((srcDepth), lambda i: deltas[i] * (1 / (var[i] + eps)** 0.5) + grad_sigma[i] * by_norm * 2 + srcDeltas_out[i] + grad_mean[i] * by_norm)\n",
    "    \n",
    "    return\n",
    "\n",
    "def dummy_loss(t, target):\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate forward FCMP code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('function forward_prop(srcHeight, srcWidth, srcDepth, srcY[*], weights[*], y_out[*]);outargs y_out;\\n'\n",
      " '    array mean[6]; do i = 1 to 6 by 1; mean[i] = 0; end;\\n'\n",
      " '    array var[6]; do i = 1 to 6 by 1; var[i] = 0; end;\\n'\n",
      " '    eps = 0.0001;\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        do ht = 0 to (srcHeight - 1) by 1;\\n'\n",
      " '            do wd = 0 to (srcWidth - 1) by 1;\\n'\n",
      " '                mean[i + 1] = (mean[i + 1] + ((srcY[(((srcHeight * srcWidth) * i) + (srcWidth * ht)) + wd + 1] / srcHeight) / srcWidth));\\n'\n",
      " '            end;\\n'\n",
      " '        end;\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        do ht = 0 to (srcHeight - 1) by 1;\\n'\n",
      " '            do wd = 0 to (srcWidth - 1) by 1;\\n'\n",
      " '                var[i + 1] = (var[i + 1] + (((srcY[(((srcHeight * srcWidth) * i) + (srcWidth * ht)) + wd + 1] - mean[i + 1]) ** 2 / srcHeight) / srcWidth));\\n'\n",
      " '            end;\\n'\n",
      " '        end;\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        do j = 0 to (srcHeight - 1) by 1;\\n'\n",
      " '            do m = 0 to (srcWidth - 1) by 1;\\n'\n",
      " '                y_out[(((srcHeight * srcWidth) * i) + (srcWidth * j)) + m + 1] = ((srcY[(((srcHeight * srcWidth) * i) + (srcWidth * j)) + m + 1] - mean[i + 1]) / (var[i + 1] + eps) ** 0.5);\\n'\n",
      " '            end;\\n'\n",
      " '        end;\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    return ;\\n'\n",
      " 'endsub;\\n')\n"
     ]
    }
   ],
   "source": [
    "forward_fcmp_code = python_to_fcmp(func=forward_prop, print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate backward FCMP code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('function back_prop(srcHeight, srcWidth, srcDepth, srcY[*], Y[*], weights[*], deltas[*], gradient_out[*], srcDeltas_out[*]);outargs gradient_out, srcDeltas_out;\\n'\n",
      " '    array dmean[6]; do i = 1 to 6 by 1; dmean[i] = 0; end;\\n'\n",
      " '    array dvar[6]; do i = 1 to 6 by 1; dvar[i] = 0; end;\\n'\n",
      " '    array mean[6]; do i = 1 to 6 by 1; mean[i] = 0; end;\\n'\n",
      " '    array var[6]; do i = 1 to 6 by 1; var[i] = 0; end;\\n'\n",
      " '    array grad_sigma[6]; do i = 1 to 6 by 1; grad_sigma[i] = 0; end;\\n'\n",
      " '    array grad_mean[6]; do i = 1 to 6 by 1; grad_mean[i] = 0; end;\\n'\n",
      " '    array grad_mean_2[6]; do i = 1 to 6 by 1; grad_mean_2[i] = 0; end;\\n'\n",
      " '    do i = 0 to ((srcHeight * srcWidth) * srcDepth) - 1 by 1;\\n'\n",
      " '        srcDeltas_out[i + 1] = 0;\\n'\n",
      " '    end;\\n'\n",
      " '    by_norm = (1 / (srcHeight * srcWidth));\\n'\n",
      " '    eps = 0.0001;\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        do ht = 0 to (srcHeight - 1) by 1;\\n'\n",
      " '            do wd = 0 to (srcWidth - 1) by 1;\\n'\n",
      " '                mean[i + 1] = (mean[i + 1] + ((srcY[(((srcHeight * srcWidth) * i) + (srcWidth * ht)) + wd + 1] / srcHeight) / srcWidth));\\n'\n",
      " '            end;\\n'\n",
      " '        end;\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        do ht = 0 to (srcHeight - 1) by 1;\\n'\n",
      " '            do wd = 0 to (srcWidth - 1) by 1;\\n'\n",
      " '                var[i + 1] = (var[i + 1] + (((srcY[(((srcHeight * srcWidth) * i) + (srcWidth * ht)) + wd + 1] - mean[i + 1]) ** 2 / srcHeight) / srcWidth));\\n'\n",
      " '            end;\\n'\n",
      " '        end;\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        do ht = 0 to (srcHeight - 1) by 1;\\n'\n",
      " '            do wd = 0 to (srcWidth - 1) by 1;\\n'\n",
      " '                grad_sigma[i + 1] = (grad_sigma[i + 1] + (deltas[(((srcHeight * srcWidth) * i) + (srcWidth * ht)) + wd + 1] * (srcY[i + 1] - mean[i + 1])));\\n'\n",
      " '            end;\\n'\n",
      " '        end;\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        grad_sigma[i + 1] = ((grad_sigma[i + 1] * -0.5) * (var[i + 1] + eps) ** -1.5);\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        do ht = 0 to (srcHeight - 1) by 1;\\n'\n",
      " '            do wd = 0 to (srcWidth - 1) by 1;\\n'\n",
      " '                grad_mean[i + 1] = (grad_mean[i + 1] + (deltas[(((srcHeight * srcWidth) * i) + (srcWidth * ht)) + wd + 1] * (-1 * (var[i + 1] + eps) ** 0.5)));\\n'\n",
      " '            end;\\n'\n",
      " '        end;\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        do ht = 0 to (srcHeight - 1) by 1;\\n'\n",
      " '            do wd = 0 to (srcWidth - 1) by 1;\\n'\n",
      " '                grad_mean_2[i + 1] = (grad_mean_2[i + 1] + (srcY[(((srcHeight * srcWidth) * i) + (srcWidth * ht)) + wd + 1] - mean[i + 1]));\\n'\n",
      " '            end;\\n'\n",
      " '        end;\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        grad_mean[i + 1] = (grad_mean[i + 1] + ((((grad_sigma[i + 1] * by_norm) * 2) * grad_mean_2[i + 1]) * -1));\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        do ht = 0 to (srcHeight - 1) by 1;\\n'\n",
      " '            do wd = 0 to (srcWidth - 1) by 1;\\n'\n",
      " '                srcDeltas_out[i + 1] = (srcDeltas_out[i + 1] + (srcY[(((srcHeight * srcWidth) * i) + (srcWidth * ht)) + wd + 1] - mean[i + 1]));\\n'\n",
      " '            end;\\n'\n",
      " '        end;\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        srcDeltas_out[i + 1] = ((((deltas[i + 1] * (1 / (var[i + 1] + eps) ** 0.5)) + ((grad_sigma[i + 1] * by_norm) * 2)) + srcDeltas_out[i + 1]) + (grad_mean[i + 1] * by_norm));\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    return ;\\n'\n",
      " 'endsub;\\n')\n"
     ]
    }
   ],
   "source": [
    "backward_fcmp_code = python_to_fcmp(func=back_prop, print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services saved the file FCMPFUNCTION.sashdat in caslib CASUSER(weshiz).\n"
     ]
    }
   ],
   "source": [
    "# register forward and backward function together\n",
    "register_fcmp_routines(s,\n",
    "                       routine_code= forward_fcmp_code + backward_fcmp_code,\n",
    "                       function_tbl_name='fcmpfunction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training data\n",
    "In the case, we have Fashion MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ImageTable.load_files(conn=s, path='/cas/DeepLearn/weshiz/fashion-mnist/testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ColumnInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Column\">Column</th>\n",
       "      <th title=\"Label\">Label</th>\n",
       "      <th title=\"ID\">ID</th>\n",
       "      <th title=\"Type\">Type</th>\n",
       "      <th title=\"RawLength\">RawLength</th>\n",
       "      <th title=\"FormattedLength\">FormattedLength</th>\n",
       "      <th title=\"Format\">Format</th>\n",
       "      <th title=\"NFL\">NFL</th>\n",
       "      <th title=\"NFD\">NFD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_image_</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>varbinary(image)</td>\n",
       "      <td>837</td>\n",
       "      <td>837</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_label_</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>varchar</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_filename_0</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>varchar</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_id_</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>int64</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.000535s</span> &#183; <span class=\"cas-user\">user 0.000369s</span> &#183; <span class=\"cas-sys\">sys 0.00015s</span> &#183; <span class=\"cas-memory\">mem 0.802MB</span></small></p>"
      ],
      "text/plain": [
       "[ColumnInfo]\n",
       "\n",
       "         Column Label  ID              Type  RawLength  FormattedLength Format  \\\n",
       " 0      _image_         1  varbinary(image)        837              837          \n",
       " 1      _label_         2           varchar          1                1          \n",
       " 2  _filename_0         3           varchar          8                8          \n",
       " 3         _id_         4             int64          8               12          \n",
       " \n",
       "    NFL  NFD  \n",
       " 0    0    0  \n",
       " 1    0    0  \n",
       " 2    0    0  \n",
       " 3    0    0  \n",
       "\n",
       "+ Elapsed: 0.000535s, user: 0.000369s, sys: 0.00015s, mem: 0.802mb"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Input layer added.\n",
      "NOTE: Convolution layer added.\n",
      "NOTE: FCMP layer added.\n",
      "NOTE: Pooling layer added.\n",
      "NOTE: Convolution layer added.\n",
      "NOTE: Pooling layer added.\n",
      "NOTE: Fully-connected layer added.\n",
      "NOTE: Fully-connected layer added.\n",
      "NOTE: Output layer added.\n",
      "NOTE: Model compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(conn=s, model_table='lenet5')\n",
    "\n",
    "model.add(InputLayer(n_channels=1, width=28, height=28, scale=1.0/255, offsets=None,\n",
    "                     random_flip=None, random_crop=None))\n",
    "\n",
    "model.add(Conv2d(n_filters=6, width=3, height=3, stride=1))\n",
    "\n",
    "# instance normalizationi\n",
    "model.add(FCMPLayer(width=28, height=28, depth=6, n_weights=0, \n",
    "                    forward_func='forward_prop', backward_func='back_prop',\n",
    "                    name='FCMPLayer1'))\n",
    "\n",
    "model.add(Pooling(width=2, height=2, stride=2, pool='max'))\n",
    "\n",
    "model.add(Conv2d(n_filters=16, width=5, height=5, stride=1))\n",
    "model.add(Pooling(width=2, height=2, stride=2, pool='max'))\n",
    "\n",
    "model.add(Dense(n=120))\n",
    "model.add(Dense(n=84))\n",
    "\n",
    "model.add(OutputLayer(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = VanillaSolver(clip_grad_max=100, clip_grad_min=-100, learning_rate=0.001,\n",
    "                       learning_rate_policy='step', gamma=0.1, step_size=30)\n",
    "optimizer = Optimizer(algorithm=solver, seed=13309, max_epochs=15, log_level=3, mini_batch_size=32, reg_l2=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Training from scratch.\n",
      "NOTE:  Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 107690.\n",
      "NOTE:  The approximate memory cost is 43.00 MB.\n",
      "NOTE:  Loading weights cost       0.00 (s).\n",
      "NOTE:  Initializing each layer cost       0.04 (s).\n",
      "NOTE:  The total number of threads on each worker is 8.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 32.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 256.\n",
      "NOTE:  Target variable: _label_\n",
      "NOTE:  Number of levels for the target variable:     10\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: 0\n",
      "NOTE:  Level      1: 1\n",
      "NOTE:  Level      2: 2\n",
      "NOTE:  Level      3: 3\n",
      "NOTE:  Level      4: 4\n",
      "NOTE:  Level      5: 5\n",
      "NOTE:  Level      6: 6\n",
      "NOTE:  Level      7: 7\n",
      "NOTE:  Level      8: 8\n",
      "NOTE:  Level      9: 9\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of numeric input variables:      1\n",
      "NOTE:  Number of FCMP layers in model: 1\n",
      "NOTE:  FCMP layer 'fcmplayer1' has input tensor size: width=28, height=28, depth=6\n",
      "NOTE:  FCMP layer 'fcmplayer1' has output tensor size: width=28, height=28, depth=6\n",
      "NOTE:  FCMP layer 'fcmplayer1' has 0 weights.\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001            2.588     0.8984    0.02386     0.10\n",
      "NOTE:      1   256    0.001            2.603     0.8906    0.02386     0.06\n",
      "NOTE:      2   256    0.001            2.532     0.8789    0.02386     0.06\n",
      "NOTE:      3   256    0.001            2.563     0.9102    0.02386     0.06\n",
      "NOTE:      4   256    0.001            2.462     0.8672    0.02386     0.06\n",
      "NOTE:      5   256    0.001            2.502     0.9023    0.02386     0.06\n",
      "NOTE:      6   256    0.001            2.428     0.9258    0.02386     0.06\n",
      "NOTE:      7   256    0.001            2.483     0.8984    0.02386     0.06\n",
      "NOTE:      8   256    0.001            2.357     0.8672    0.02386     0.06\n",
      "NOTE:      9   256    0.001            2.376     0.8398    0.02386     0.06\n",
      "NOTE:     10   256    0.001            2.417     0.8672    0.02386     0.06\n",
      "NOTE:     11   256    0.001            2.428      0.875    0.02386     0.06\n",
      "NOTE:     12   256    0.001            2.401      0.875    0.02386     0.06\n",
      "NOTE:     13   256    0.001              2.4     0.8789    0.02386     0.06\n",
      "NOTE:     14   256    0.001            2.378     0.8516    0.02386     0.06\n",
      "NOTE:     15   256    0.001            2.358     0.8516    0.02386     0.06\n",
      "NOTE:     16   256    0.001            2.329     0.8398    0.02386     0.07\n",
      "NOTE:     17   256    0.001            2.295      0.875    0.02386     0.06\n",
      "NOTE:     18   256    0.001            2.323     0.8594    0.02386     0.06\n",
      "NOTE:     19   256    0.001            2.308     0.8555    0.02386     0.06\n",
      "NOTE:     20   256    0.001            2.279     0.8516    0.02386     0.06\n",
      "NOTE:     21   256    0.001            2.295     0.8672    0.02386     0.06\n",
      "NOTE:     22   256    0.001            2.283     0.8203    0.02386     0.06\n",
      "NOTE:     23   256    0.001            2.284     0.8633    0.02386     0.06\n",
      "NOTE:     24   256    0.001            2.274     0.8594    0.02386     0.06\n",
      "NOTE:     25   256    0.001            2.251     0.8633    0.02385     0.06\n",
      "NOTE:     26   256    0.001            2.258     0.8164    0.02385     0.06\n",
      "NOTE:     27   256    0.001            2.251     0.8633    0.02385     0.06\n",
      "NOTE:     28   256    0.001            2.252     0.8086    0.02385     0.06\n",
      "NOTE:     29   256    0.001            2.282     0.8164    0.02385     0.06\n",
      "NOTE:     30   256    0.001            2.187     0.7813    0.02385     0.06\n",
      "NOTE:     31   256    0.001            2.229     0.8086    0.02385     0.06\n",
      "NOTE:     32   256    0.001            2.196     0.7773    0.02385     0.06\n",
      "NOTE:     33   256    0.001              2.2     0.8008    0.02385     0.06\n",
      "NOTE:     34   256    0.001            2.187     0.8047    0.02385     0.06\n",
      "NOTE:     35   256    0.001            2.188     0.7891    0.02385     0.06\n",
      "NOTE:     36   256    0.001            2.192      0.793    0.02385     0.06\n",
      "NOTE:     37   256    0.001            2.142     0.7383    0.02385     0.06\n",
      "NOTE:     38   256    0.001            2.194      0.793    0.02385     0.06\n",
      "NOTE:     39   256    0.001            2.174     0.7539    0.02385     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  0         0.001           2.328     0.8444     2.47\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001            2.158     0.7266    0.02385     0.06\n",
      "NOTE:      1   256    0.001            2.201     0.8086    0.02385     0.06\n",
      "NOTE:      2   256    0.001            2.127     0.7539    0.02385     0.06\n",
      "NOTE:      3   256    0.001            2.148     0.7539    0.02385     0.06\n",
      "NOTE:      4   256    0.001            2.098     0.6992    0.02385     0.06\n",
      "NOTE:      5   256    0.001            2.104     0.7383    0.02386     0.06\n",
      "NOTE:      6   256    0.001            2.131     0.7617    0.02386     0.06\n",
      "NOTE:      7   256    0.001            2.146       0.75    0.02386     0.06\n",
      "NOTE:      8   256    0.001              2.1      0.707    0.02386     0.06\n",
      "NOTE:      9   256    0.001            2.079      0.707    0.02386     0.06\n",
      "NOTE:     10   256    0.001            2.054     0.6602    0.02386     0.06\n",
      "NOTE:     11   256    0.001             2.09     0.6914    0.02386     0.06\n",
      "NOTE:     12   256    0.001            2.052     0.6953    0.02386     0.06\n",
      "NOTE:     13   256    0.001            2.089     0.7305    0.02386     0.06\n",
      "NOTE:     14   256    0.001            2.053     0.6523    0.02386     0.06\n",
      "NOTE:     15   256    0.001            2.104     0.7188    0.02386     0.06\n",
      "NOTE:     16   256    0.001            2.057     0.6836    0.02386     0.06\n",
      "NOTE:     17   256    0.001            2.047     0.7266    0.02386     0.06\n",
      "NOTE:     18   256    0.001            2.054     0.6641    0.02386     0.06\n",
      "NOTE:     19   256    0.001            2.011      0.668    0.02386     0.06\n",
      "NOTE:     20   256    0.001            1.992     0.6563    0.02386     0.06\n",
      "NOTE:     21   256    0.001            2.004     0.6602    0.02386     0.06\n",
      "NOTE:     22   256    0.001            2.005     0.6602    0.02386     0.06\n",
      "NOTE:     23   256    0.001            2.021      0.668    0.02386     0.06\n",
      "NOTE:     24   256    0.001            1.991     0.6484    0.02386     0.06\n",
      "NOTE:     25   256    0.001            1.992     0.6484    0.02386     0.06\n",
      "NOTE:     26   256    0.001            2.021     0.6953    0.02386     0.06\n",
      "NOTE:     27   256    0.001            2.008      0.668    0.02386     0.06\n",
      "NOTE:     28   256    0.001            1.952     0.5898    0.02386     0.06\n",
      "NOTE:     29   256    0.001            2.022     0.6758    0.02386     0.06\n",
      "NOTE:     30   256    0.001            2.031     0.7109    0.02386     0.06\n",
      "NOTE:     31   256    0.001            1.984     0.6836    0.02386     0.06\n",
      "NOTE:     32   256    0.001            1.949     0.6133    0.02386     0.06\n",
      "NOTE:     33   256    0.001            1.956     0.6016    0.02386     0.06\n",
      "NOTE:     34   256    0.001            1.979     0.6484    0.02386     0.06\n",
      "NOTE:     35   256    0.001            1.964     0.6445    0.02386     0.06\n",
      "NOTE:     36   256    0.001            1.943     0.5938    0.02386     0.06\n",
      "NOTE:     37   256    0.001            1.975     0.6406    0.02386     0.06\n",
      "NOTE:     38   256    0.001            1.895      0.543    0.02386     0.06\n",
      "NOTE:     39   256    0.001             1.89      0.582    0.02386     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  1         0.001           2.037     0.6782     2.42\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001            1.933     0.6133    0.02386     0.06\n",
      "NOTE:      1   256    0.001            1.925     0.6445    0.02386     0.06\n",
      "NOTE:      2   256    0.001            1.884     0.5625    0.02386     0.06\n",
      "NOTE:      3   256    0.001            1.851     0.5742    0.02386     0.06\n",
      "NOTE:      4   256    0.001            1.855     0.5586    0.02386     0.06\n",
      "NOTE:      5   256    0.001            1.874     0.5469    0.02386     0.06\n",
      "NOTE:      6   256    0.001             1.95     0.6328    0.02386     0.06\n",
      "NOTE:      7   256    0.001            1.931     0.6094    0.02386     0.06\n",
      "NOTE:      8   256    0.001            1.815     0.5313    0.02386     0.06\n",
      "NOTE:      9   256    0.001            1.892     0.6367    0.02386     0.06\n",
      "NOTE:     10   256    0.001            1.927     0.6563    0.02386     0.06\n",
      "NOTE:     11   256    0.001             1.89     0.5977    0.02386     0.06\n",
      "NOTE:     12   256    0.001            1.899      0.582    0.02386     0.06\n",
      "NOTE:     13   256    0.001            1.827     0.5352    0.02386     0.06\n",
      "NOTE:     14   256    0.001            1.826      0.543    0.02386     0.06\n",
      "NOTE:     15   256    0.001            1.877     0.6016    0.02386     0.06\n",
      "NOTE:     16   256    0.001            1.842     0.5625    0.02386     0.06\n",
      "NOTE:     17   256    0.001             1.82     0.5586    0.02386     0.06\n",
      "NOTE:     18   256    0.001            1.828     0.5391    0.02386     0.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:     19   256    0.001             1.86     0.5547    0.02386     0.06\n",
      "NOTE:     20   256    0.001            1.826      0.582    0.02386     0.06\n",
      "NOTE:     21   256    0.001             1.84     0.5352    0.02386     0.06\n",
      "NOTE:     22   256    0.001            1.842     0.5859    0.02387     0.06\n",
      "NOTE:     23   256    0.001            1.872     0.6133    0.02387     0.06\n",
      "NOTE:     24   256    0.001            1.805     0.5078    0.02387     0.06\n",
      "NOTE:     25   256    0.001            1.766      0.543    0.02387     0.06\n",
      "NOTE:     26   256    0.001            1.764     0.4961    0.02387     0.06\n",
      "NOTE:     27   256    0.001            1.844     0.5664    0.02387     0.06\n",
      "NOTE:     28   256    0.001             1.77     0.5234    0.02387     0.06\n",
      "NOTE:     29   256    0.001            1.741     0.5156    0.02387     0.06\n",
      "NOTE:     30   256    0.001            1.784     0.5117    0.02387     0.06\n",
      "NOTE:     31   256    0.001            1.814     0.5195    0.02387     0.06\n",
      "NOTE:     32   256    0.001            1.766     0.5313    0.02387     0.06\n",
      "NOTE:     33   256    0.001            1.759     0.4961    0.02387     0.06\n",
      "NOTE:     34   256    0.001            1.743     0.4883    0.02387     0.06\n",
      "NOTE:     35   256    0.001            1.771     0.5234    0.02387     0.06\n",
      "NOTE:     36   256    0.001            1.746     0.4688    0.02387     0.06\n",
      "NOTE:     37   256    0.001            1.719     0.4922    0.02387     0.06\n",
      "NOTE:     38   256    0.001            1.768     0.4961    0.02387     0.06\n",
      "NOTE:     39   256    0.001            1.794     0.5508    0.02387     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  2         0.001           1.831     0.5547     2.42\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001             1.71     0.4766    0.02387     0.06\n",
      "NOTE:      1   256    0.001            1.753     0.5313    0.02387     0.06\n",
      "NOTE:      2   256    0.001            1.704     0.5234    0.02387     0.06\n",
      "NOTE:      3   256    0.001            1.717     0.5078    0.02387     0.06\n",
      "NOTE:      4   256    0.001            1.748     0.5039    0.02387     0.06\n",
      "NOTE:      5   256    0.001            1.709     0.4922    0.02387     0.06\n",
      "NOTE:      6   256    0.001            1.673     0.4492    0.02387     0.06\n",
      "NOTE:      7   256    0.001            1.653     0.4375    0.02387     0.06\n",
      "NOTE:      8   256    0.001            1.699     0.5039    0.02387     0.06\n",
      "NOTE:      9   256    0.001            1.725     0.4922    0.02387     0.06\n",
      "NOTE:     10   256    0.001            1.724     0.4648    0.02387     0.06\n",
      "NOTE:     11   256    0.001            1.676      0.457    0.02387     0.06\n",
      "NOTE:     12   256    0.001            1.683     0.5117    0.02387     0.06\n",
      "NOTE:     13   256    0.001            1.639     0.4531    0.02387     0.06\n",
      "NOTE:     14   256    0.001            1.697     0.4688    0.02388     0.06\n",
      "NOTE:     15   256    0.001            1.624     0.4727    0.02388     0.06\n",
      "NOTE:     16   256    0.001            1.691     0.5234    0.02388     0.06\n",
      "NOTE:     17   256    0.001            1.658     0.4531    0.02388     0.06\n",
      "NOTE:     18   256    0.001            1.698     0.5117    0.02388     0.06\n",
      "NOTE:     19   256    0.001             1.67     0.4531    0.02388     0.06\n",
      "NOTE:     20   256    0.001            1.663     0.5078    0.02388     0.07\n",
      "NOTE:     21   256    0.001            1.617     0.4766    0.02388     0.06\n",
      "NOTE:     22   256    0.001             1.62     0.4219    0.02388     0.06\n",
      "NOTE:     23   256    0.001            1.602     0.4258    0.02388     0.06\n",
      "NOTE:     24   256    0.001            1.612     0.4414    0.02388     0.06\n",
      "NOTE:     25   256    0.001            1.683     0.4688    0.02388     0.06\n",
      "NOTE:     26   256    0.001            1.615     0.4375    0.02388     0.06\n",
      "NOTE:     27   256    0.001            1.573     0.4063    0.02388     0.06\n",
      "NOTE:     28   256    0.001            1.652     0.4688    0.02388     0.06\n",
      "NOTE:     29   256    0.001            1.623     0.4688    0.02388     0.06\n",
      "NOTE:     30   256    0.001            1.655     0.4961    0.02388     0.06\n",
      "NOTE:     31   256    0.001            1.653     0.4805    0.02388     0.06\n",
      "NOTE:     32   256    0.001              1.6     0.4492    0.02388     0.06\n",
      "NOTE:     33   256    0.001            1.591     0.4414    0.02388     0.06\n",
      "NOTE:     34   256    0.001            1.563     0.3789    0.02388     0.06\n",
      "NOTE:     35   256    0.001            1.559     0.3906    0.02388     0.06\n",
      "NOTE:     36   256    0.001            1.544     0.4141    0.02388     0.06\n",
      "NOTE:     37   256    0.001            1.595     0.4531    0.02388     0.06\n",
      "NOTE:     38   256    0.001            1.636      0.457    0.02388     0.06\n",
      "NOTE:     39   256    0.001            1.591     0.5078    0.02388     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  3         0.001           1.652      0.467     2.43\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001            1.651      0.457    0.02388     0.06\n",
      "NOTE:      1   256    0.001            1.587     0.3867    0.02388     0.06\n",
      "NOTE:      2   256    0.001            1.579     0.4063    0.02389     0.06\n",
      "NOTE:      3   256    0.001            1.623      0.457    0.02389     0.06\n",
      "NOTE:      4   256    0.001              1.6     0.4492    0.02389     0.06\n",
      "NOTE:      5   256    0.001            1.523     0.3945    0.02389     0.06\n",
      "NOTE:      6   256    0.001            1.632     0.5156    0.02389     0.06\n",
      "NOTE:      7   256    0.001             1.56     0.4219    0.02389     0.06\n",
      "NOTE:      8   256    0.001            1.551     0.4063    0.02389     0.06\n",
      "NOTE:      9   256    0.001            1.559     0.4414    0.02389     0.06\n",
      "NOTE:     10   256    0.001            1.534     0.4102    0.02389     0.06\n",
      "NOTE:     11   256    0.001            1.574     0.4531    0.02389     0.06\n",
      "NOTE:     12   256    0.001            1.603     0.4531    0.02389     0.06\n",
      "NOTE:     13   256    0.001            1.578     0.4531    0.02389     0.06\n",
      "NOTE:     14   256    0.001             1.55     0.4375    0.02389     0.06\n",
      "NOTE:     15   256    0.001             1.58      0.418    0.02389     0.06\n",
      "NOTE:     16   256    0.001            1.557     0.4375    0.02389     0.06\n",
      "NOTE:     17   256    0.001            1.496     0.3867    0.02389     0.06\n",
      "NOTE:     18   256    0.001            1.556      0.457    0.02389     0.06\n",
      "NOTE:     19   256    0.001             1.51     0.4258    0.02389     0.06\n",
      "NOTE:     20   256    0.001            1.498     0.4141    0.02389     0.09\n",
      "NOTE:     21   256    0.001            1.529     0.4102    0.02389     0.06\n",
      "NOTE:     22   256    0.001            1.523     0.4531    0.02389     0.06\n",
      "NOTE:     23   256    0.001            1.506     0.4063    0.02389     0.06\n",
      "NOTE:     24   256    0.001            1.551     0.4453    0.02389     0.06\n",
      "NOTE:     25   256    0.001            1.438     0.3906    0.02389     0.06\n",
      "NOTE:     26   256    0.001            1.502     0.4375    0.02389     0.06\n",
      "NOTE:     27   256    0.001            1.479     0.4063    0.02389     0.06\n",
      "NOTE:     28   256    0.001            1.407     0.3633     0.0239     0.06\n",
      "NOTE:     29   256    0.001            1.492     0.4063     0.0239     0.06\n",
      "NOTE:     30   256    0.001             1.44     0.4063     0.0239     0.06\n",
      "NOTE:     31   256    0.001             1.46     0.3789     0.0239     0.06\n",
      "NOTE:     32   256    0.001            1.464     0.3711     0.0239     0.06\n",
      "NOTE:     33   256    0.001             1.51     0.4453     0.0239     0.06\n",
      "NOTE:     34   256    0.001            1.514     0.4414     0.0239     0.06\n",
      "NOTE:     35   256    0.001             1.48     0.4492     0.0239     0.06\n",
      "NOTE:     36   256    0.001            1.482     0.4453     0.0239     0.06\n",
      "NOTE:     37   256    0.001            1.485     0.4063     0.0239     0.06\n",
      "NOTE:     38   256    0.001            1.441     0.3711     0.0239     0.06\n",
      "NOTE:     39   256    0.001            1.418     0.3828     0.0239     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  4         0.001           1.526     0.4225     2.44\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001            1.416     0.3516     0.0239     0.06\n",
      "NOTE:      1   256    0.001            1.403     0.3477     0.0239     0.06\n",
      "NOTE:      2   256    0.001            1.422     0.3789     0.0239     0.06\n",
      "NOTE:      3   256    0.001             1.49     0.3945     0.0239     0.06\n",
      "NOTE:      4   256    0.001            1.396     0.3555     0.0239     0.06\n",
      "NOTE:      5   256    0.001            1.437     0.3945     0.0239     0.06\n",
      "NOTE:      6   256    0.001            1.488      0.457     0.0239     0.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      7   256    0.001            1.445      0.418     0.0239     0.06\n",
      "NOTE:      8   256    0.001             1.44     0.3867     0.0239     0.06\n",
      "NOTE:      9   256    0.001            1.446      0.418     0.0239     0.06\n",
      "NOTE:     10   256    0.001            1.381     0.3477     0.0239     0.06\n",
      "NOTE:     11   256    0.001            1.471     0.4336     0.0239     0.06\n",
      "NOTE:     12   256    0.001            1.387     0.3438     0.0239     0.06\n",
      "NOTE:     13   256    0.001            1.441     0.3984    0.02391     0.06\n",
      "NOTE:     14   256    0.001            1.367     0.3711    0.02391     0.06\n",
      "NOTE:     15   256    0.001            1.464     0.4609    0.02391     0.06\n",
      "NOTE:     16   256    0.001            1.367      0.375    0.02391     0.06\n",
      "NOTE:     17   256    0.001             1.39     0.3555    0.02391     0.06\n",
      "NOTE:     18   256    0.001            1.356     0.3477    0.02391     0.06\n",
      "NOTE:     19   256    0.001            1.381     0.3516    0.02391     0.06\n",
      "NOTE:     20   256    0.001            1.381     0.3359    0.02391     0.06\n",
      "NOTE:     21   256    0.001            1.418     0.3984    0.02391     0.06\n",
      "NOTE:     22   256    0.001             1.41     0.3516    0.02391     0.06\n",
      "NOTE:     23   256    0.001            1.374     0.3281    0.02391     0.06\n",
      "NOTE:     24   256    0.001            1.436      0.418    0.02391     0.06\n",
      "NOTE:     25   256    0.001            1.437     0.4375    0.02391     0.06\n",
      "NOTE:     26   256    0.001             1.46     0.4531    0.02391     0.06\n",
      "NOTE:     27   256    0.001            1.342     0.3359    0.02391     0.06\n",
      "NOTE:     28   256    0.001            1.365     0.3789    0.02391     0.06\n",
      "NOTE:     29   256    0.001            1.337     0.3203    0.02391     0.06\n",
      "NOTE:     30   256    0.001            1.348      0.375    0.02391     0.06\n",
      "NOTE:     31   256    0.001            1.327     0.4102    0.02391     0.06\n",
      "NOTE:     32   256    0.001            1.342     0.3359    0.02391     0.06\n",
      "NOTE:     33   256    0.001            1.337     0.3242    0.02391     0.06\n",
      "NOTE:     34   256    0.001            1.333     0.3516    0.02391     0.06\n",
      "NOTE:     35   256    0.001            1.285      0.332    0.02391     0.06\n",
      "NOTE:     36   256    0.001              1.3     0.3086    0.02391     0.06\n",
      "NOTE:     37   256    0.001            1.445     0.4297    0.02392     0.06\n",
      "NOTE:     38   256    0.001            1.401     0.3789    0.02392     0.06\n",
      "NOTE:     39   256    0.001            1.346     0.3477    0.02392     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  5         0.001           1.395      0.376     2.42\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001            1.337     0.3633    0.02392     0.06\n",
      "NOTE:      1   256    0.001            1.356     0.3711    0.02392     0.06\n",
      "NOTE:      2   256    0.001            1.325     0.3359    0.02392     0.06\n",
      "NOTE:      3   256    0.001            1.403     0.4023    0.02392     0.06\n",
      "NOTE:      4   256    0.001            1.325     0.3594    0.02392     0.06\n",
      "NOTE:      5   256    0.001            1.328     0.3672    0.02392     0.06\n",
      "NOTE:      6   256    0.001            1.261     0.3125    0.02392     0.06\n",
      "NOTE:      7   256    0.001            1.309     0.3438    0.02392     0.06\n",
      "NOTE:      8   256    0.001            1.244     0.2852    0.02392     0.06\n",
      "NOTE:      9   256    0.001            1.328     0.3672    0.02392     0.06\n",
      "NOTE:     10   256    0.001            1.322     0.4023    0.02392     0.06\n",
      "NOTE:     11   256    0.001            1.206     0.2891    0.02392     0.06\n",
      "NOTE:     12   256    0.001            1.328      0.375    0.02392     0.06\n",
      "NOTE:     13   256    0.001            1.312     0.3633    0.02392     0.06\n",
      "NOTE:     14   256    0.001            1.263     0.3281    0.02392     0.06\n",
      "NOTE:     15   256    0.001             1.29     0.3516    0.02392     0.06\n",
      "NOTE:     16   256    0.001            1.238     0.3242    0.02392     0.06\n",
      "NOTE:     17   256    0.001            1.327     0.3477    0.02392     0.06\n",
      "NOTE:     18   256    0.001            1.317     0.3828    0.02392     0.06\n",
      "NOTE:     19   256    0.001            1.304     0.3594    0.02392     0.06\n",
      "NOTE:     20   256    0.001            1.292      0.375    0.02392     0.06\n",
      "NOTE:     21   256    0.001            1.326     0.3164    0.02393     0.06\n",
      "NOTE:     22   256    0.001            1.279     0.3164    0.02393     0.06\n",
      "NOTE:     23   256    0.001            1.231     0.2969    0.02393     0.06\n",
      "NOTE:     24   256    0.001            1.257     0.3516    0.02393     0.06\n",
      "NOTE:     25   256    0.001            1.099     0.2539    0.02393     0.06\n",
      "NOTE:     26   256    0.001            1.261     0.3477    0.02393     0.06\n",
      "NOTE:     27   256    0.001            1.246     0.3633    0.02393     0.06\n",
      "NOTE:     28   256    0.001            1.252     0.3164    0.02393     0.06\n",
      "NOTE:     29   256    0.001            1.172      0.293    0.02393     0.06\n",
      "NOTE:     30   256    0.001            1.272     0.3633    0.02393     0.06\n",
      "NOTE:     31   256    0.001            1.241     0.3164    0.02393     0.06\n",
      "NOTE:     32   256    0.001            1.166     0.2813    0.02393     0.06\n",
      "NOTE:     33   256    0.001            1.299     0.3789    0.02393     0.06\n",
      "NOTE:     34   256    0.001            1.231     0.3008    0.02393     0.06\n",
      "NOTE:     35   256    0.001            1.282     0.3477    0.02393     0.06\n",
      "NOTE:     36   256    0.001             1.22     0.3516    0.02393     0.06\n",
      "NOTE:     37   256    0.001            1.254     0.3594    0.02393     0.06\n",
      "NOTE:     38   256    0.001            1.249     0.3477    0.02393     0.06\n",
      "NOTE:     39   256    0.001            1.265     0.3555    0.02393     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  6         0.001           1.275     0.3416     2.41\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001            1.221     0.3164    0.02393     0.06\n",
      "NOTE:      1   256    0.001            1.226      0.332    0.02393     0.06\n",
      "NOTE:      2   256    0.001            1.302     0.3867    0.02393     0.06\n",
      "NOTE:      3   256    0.001            1.201     0.3164    0.02393     0.06\n",
      "NOTE:      4   256    0.001            1.219     0.3359    0.02393     0.06\n",
      "NOTE:      5   256    0.001            1.246     0.3477    0.02394     0.06\n",
      "NOTE:      6   256    0.001            1.213      0.332    0.02394     0.06\n",
      "NOTE:      7   256    0.001            1.172     0.3125    0.02394     0.06\n",
      "NOTE:      8   256    0.001            1.135     0.2969    0.02394     0.06\n",
      "NOTE:      9   256    0.001            1.196     0.3477    0.02394     0.06\n",
      "NOTE:     10   256    0.001            1.097     0.2656    0.02394     0.06\n",
      "NOTE:     11   256    0.001            1.129     0.2617    0.02394     0.06\n",
      "NOTE:     12   256    0.001            1.221     0.3477    0.02394     0.06\n",
      "NOTE:     13   256    0.001            1.144     0.3242    0.02394     0.06\n",
      "NOTE:     14   256    0.001             1.12     0.2578    0.02394     0.06\n",
      "NOTE:     15   256    0.001            1.168     0.3047    0.02394     0.06\n",
      "NOTE:     16   256    0.001             1.22      0.332    0.02394     0.06\n",
      "NOTE:     17   256    0.001            1.162     0.3242    0.02394     0.06\n",
      "NOTE:     18   256    0.001            1.206     0.3281    0.02394     0.06\n",
      "NOTE:     19   256    0.001            1.179     0.3125    0.02394     0.06\n",
      "NOTE:     20   256    0.001            1.079     0.2969    0.02394     0.06\n",
      "NOTE:     21   256    0.001            1.219     0.3477    0.02394     0.06\n",
      "NOTE:     22   256    0.001            1.142     0.3086    0.02394     0.06\n",
      "NOTE:     23   256    0.001            1.196     0.3164    0.02394     0.06\n",
      "NOTE:     24   256    0.001            1.129     0.3047    0.02394     0.06\n",
      "NOTE:     25   256    0.001            1.126     0.3086    0.02394     0.06\n",
      "NOTE:     26   256    0.001            1.183     0.3008    0.02394     0.06\n",
      "NOTE:     27   256    0.001            1.082     0.2422    0.02394     0.06\n",
      "NOTE:     28   256    0.001            1.152     0.3164    0.02395     0.06\n",
      "NOTE:     29   256    0.001            1.153     0.3438    0.02395     0.06\n",
      "NOTE:     30   256    0.001            1.215     0.3281    0.02395     0.06\n",
      "NOTE:     31   256    0.001            1.177     0.3281    0.02395     0.06\n",
      "NOTE:     32   256    0.001            1.143     0.3047    0.02395     0.06\n",
      "NOTE:     33   256    0.001            1.132     0.2891    0.02395     0.06\n",
      "NOTE:     34   256    0.001            1.139     0.3359    0.02395     0.06\n",
      "NOTE:     35   256    0.001            1.147     0.3281    0.02395     0.06\n",
      "NOTE:     36   256    0.001            1.173     0.3242    0.02395     0.06\n",
      "NOTE:     37   256    0.001            1.167     0.3438    0.02395     0.06\n",
      "NOTE:     38   256    0.001            1.092     0.2734    0.02395     0.06\n",
      "NOTE:     39   256    0.001            1.088     0.2813    0.02395     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  7         0.001           1.168     0.3151     2.41\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001            1.045     0.2539    0.02395     0.06\n",
      "NOTE:      1   256    0.001              1.1     0.2891    0.02395     0.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      2   256    0.001            1.122     0.3008    0.02395     0.06\n",
      "NOTE:      3   256    0.001            1.108     0.2734    0.02395     0.06\n",
      "NOTE:      4   256    0.001            1.055     0.2852    0.02395     0.06\n",
      "NOTE:      5   256    0.001            1.135     0.3125    0.02395     0.06\n",
      "NOTE:      6   256    0.001            1.116     0.3086    0.02395     0.06\n",
      "NOTE:      7   256    0.001            1.021     0.2461    0.02395     0.06\n",
      "NOTE:      8   256    0.001            1.098     0.3125    0.02395     0.06\n",
      "NOTE:      9   256    0.001            1.068     0.2773    0.02395     0.06\n",
      "NOTE:     10   256    0.001            1.096     0.2852    0.02395     0.06\n",
      "NOTE:     11   256    0.001            1.147     0.3047    0.02395     0.06\n",
      "NOTE:     12   256    0.001            1.204     0.3672    0.02396     0.06\n",
      "NOTE:     13   256    0.001            1.117     0.2773    0.02396     0.06\n",
      "NOTE:     14   256    0.001            1.083     0.3203    0.02396     0.06\n",
      "NOTE:     15   256    0.001            1.199     0.3359    0.02396     0.06\n",
      "NOTE:     16   256    0.001            1.141     0.3125    0.02396     0.06\n",
      "NOTE:     17   256    0.001            1.106     0.2773    0.02396     0.06\n",
      "NOTE:     18   256    0.001            1.078     0.3086    0.02396     0.06\n",
      "NOTE:     19   256    0.001            1.129      0.332    0.02396     0.06\n",
      "NOTE:     20   256    0.001            1.225     0.3555    0.02396     0.06\n",
      "NOTE:     21   256    0.001            1.101     0.3047    0.02396     0.06\n",
      "NOTE:     22   256    0.001            1.087     0.3008    0.02396     0.06\n",
      "NOTE:     23   256    0.001             1.05     0.2813    0.02396     0.06\n",
      "NOTE:     24   256    0.001            1.088     0.3477    0.02396     0.06\n",
      "NOTE:     25   256    0.001            1.103     0.2695    0.02396     0.06\n",
      "NOTE:     26   256    0.001            1.096     0.2969    0.02396     0.06\n",
      "NOTE:     27   256    0.001            1.127     0.3242    0.02396     0.06\n",
      "NOTE:     28   256    0.001            1.158     0.3359    0.02396     0.06\n",
      "NOTE:     29   256    0.001            1.025     0.2539    0.02396     0.06\n",
      "NOTE:     30   256    0.001            1.046     0.2656    0.02396     0.06\n",
      "NOTE:     31   256    0.001            1.027     0.2852    0.02396     0.06\n",
      "NOTE:     32   256    0.001            1.102     0.3125    0.02396     0.06\n",
      "NOTE:     33   256    0.001            1.058     0.3086    0.02396     0.06\n",
      "NOTE:     34   256    0.001            1.032     0.2656    0.02396     0.06\n",
      "NOTE:     35   256    0.001            1.024     0.2578    0.02396     0.06\n",
      "NOTE:     36   256    0.001            1.052     0.3047    0.02396     0.06\n",
      "NOTE:     37   256    0.001            1.027     0.2422    0.02397     0.06\n",
      "NOTE:     38   256    0.001             1.07     0.2617    0.02397     0.06\n",
      "NOTE:     39   256    0.001            1.039     0.2852    0.02397     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  8         0.001           1.093      0.296     2.41\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001            1.078     0.2852    0.02397     0.06\n",
      "NOTE:      1   256    0.001            1.065     0.2734    0.02397     0.06\n",
      "NOTE:      2   256    0.001            1.067     0.2773    0.02397     0.06\n",
      "NOTE:      3   256    0.001            1.076     0.3047    0.02397     0.06\n",
      "NOTE:      4   256    0.001            1.095     0.3242    0.02397     0.06\n",
      "NOTE:      5   256    0.001            1.069     0.3125    0.02397     0.06\n",
      "NOTE:      6   256    0.001            1.047     0.3047    0.02397     0.06\n",
      "NOTE:      7   256    0.001             1.11     0.3047    0.02397     0.06\n",
      "NOTE:      8   256    0.001            1.067     0.3086    0.02397     0.06\n",
      "NOTE:      9   256    0.001            1.004     0.3008    0.02397     0.06\n",
      "NOTE:     10   256    0.001            1.082     0.3125    0.02397     0.06\n",
      "NOTE:     11   256    0.001           0.9854       0.25    0.02397     0.06\n",
      "NOTE:     12   256    0.001            1.077     0.2969    0.02397     0.06\n",
      "NOTE:     13   256    0.001            1.047     0.2969    0.02397     0.06\n",
      "NOTE:     14   256    0.001            1.031     0.2695    0.02397     0.06\n",
      "NOTE:     15   256    0.001            1.128     0.3789    0.02397     0.06\n",
      "NOTE:     16   256    0.001             1.09     0.3359    0.02397     0.06\n",
      "NOTE:     17   256    0.001            1.044     0.3125    0.02397     0.06\n",
      "NOTE:     18   256    0.001            1.108     0.3281    0.02397     0.06\n",
      "NOTE:     19   256    0.001           0.9694     0.2539    0.02397     0.06\n",
      "NOTE:     20   256    0.001           0.9992      0.293    0.02397     0.06\n",
      "NOTE:     21   256    0.001            1.078     0.3008    0.02397     0.06\n",
      "NOTE:     22   256    0.001           0.9523     0.2617    0.02397     0.06\n",
      "NOTE:     23   256    0.001            1.009     0.2461    0.02398     0.06\n",
      "NOTE:     24   256    0.001            1.023     0.2852    0.02398     0.06\n",
      "NOTE:     25   256    0.001            1.024     0.2734    0.02398     0.06\n",
      "NOTE:     26   256    0.001           0.9652     0.2891    0.02398     0.06\n",
      "NOTE:     27   256    0.001             1.06     0.3125    0.02398     0.06\n",
      "NOTE:     28   256    0.001            1.015     0.2695    0.02398     0.06\n",
      "NOTE:     29   256    0.001           0.9538     0.2695    0.02398     0.06\n",
      "NOTE:     30   256    0.001            1.034     0.2813    0.02398     0.06\n",
      "NOTE:     31   256    0.001            1.058     0.3438    0.02398     0.06\n",
      "NOTE:     32   256    0.001           0.9669     0.2461    0.02398     0.06\n",
      "NOTE:     33   256    0.001           0.9946     0.2891    0.02398     0.06\n",
      "NOTE:     34   256    0.001           0.9932     0.2891    0.02398     0.06\n",
      "NOTE:     35   256    0.001           0.9953     0.3125    0.02398     0.06\n",
      "NOTE:     36   256    0.001            1.042     0.2969    0.02398     0.06\n",
      "NOTE:     37   256    0.001            0.976     0.2695    0.02398     0.06\n",
      "NOTE:     38   256    0.001            1.033     0.2773    0.02398     0.06\n",
      "NOTE:     39   256    0.001           0.9319     0.2227    0.02398     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  9         0.001           1.034     0.2915     2.42\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001            1.006     0.2969    0.02398     0.06\n",
      "NOTE:      1   256    0.001           0.9882     0.3086    0.02398     0.06\n",
      "NOTE:      2   256    0.001            1.083     0.3125    0.02398     0.06\n",
      "NOTE:      3   256    0.001           0.9093     0.2188    0.02398     0.06\n",
      "NOTE:      4   256    0.001           0.9765     0.2734    0.02398     0.06\n",
      "NOTE:      5   256    0.001           0.9283     0.2305    0.02398     0.06\n",
      "NOTE:      6   256    0.001           0.9317     0.2656    0.02398     0.06\n",
      "NOTE:      7   256    0.001            1.092     0.3164    0.02398     0.06\n",
      "NOTE:      8   256    0.001             1.01     0.2695    0.02398     0.06\n",
      "NOTE:      9   256    0.001            1.039     0.3086    0.02399     0.06\n",
      "NOTE:     10   256    0.001            1.002     0.3008    0.02399     0.06\n",
      "NOTE:     11   256    0.001           0.9992     0.2656    0.02399     0.06\n",
      "NOTE:     12   256    0.001            1.003     0.2813    0.02399     0.06\n",
      "NOTE:     13   256    0.001           0.8561     0.2305    0.02399     0.06\n",
      "NOTE:     14   256    0.001                1      0.293    0.02399     0.06\n",
      "NOTE:     15   256    0.001             0.95     0.2813    0.02399     0.06\n",
      "NOTE:     16   256    0.001           0.9896     0.2969    0.02399     0.06\n",
      "NOTE:     17   256    0.001           0.9029     0.2188    0.02399     0.06\n",
      "NOTE:     18   256    0.001           0.9148     0.2617    0.02399     0.06\n",
      "NOTE:     19   256    0.001           0.9729     0.2578    0.02399     0.06\n",
      "NOTE:     20   256    0.001           0.9731     0.2578    0.02399     0.06\n",
      "NOTE:     21   256    0.001           0.9845     0.2891    0.02399     0.06\n",
      "NOTE:     22   256    0.001           0.9885     0.3047    0.02399     0.06\n",
      "NOTE:     23   256    0.001           0.9097     0.2617    0.02399     0.06\n",
      "NOTE:     24   256    0.001           0.9892     0.2578    0.02399     0.06\n",
      "NOTE:     25   256    0.001           0.9917     0.2617    0.02399     0.06\n",
      "NOTE:     26   256    0.001           0.9502     0.2852    0.02399     0.06\n",
      "NOTE:     27   256    0.001             1.11     0.3555    0.02399     0.06\n",
      "NOTE:     28   256    0.001           0.8969     0.2109    0.02399     0.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:     29   256    0.001           0.9941      0.293    0.02399     0.06\n",
      "NOTE:     30   256    0.001           0.9265       0.25    0.02399     0.06\n",
      "NOTE:     31   256    0.001           0.8923     0.2305    0.02399     0.06\n",
      "NOTE:     32   256    0.001           0.9498     0.2734    0.02399     0.06\n",
      "NOTE:     33   256    0.001           0.9138     0.2617    0.02399     0.06\n",
      "NOTE:     34   256    0.001           0.9702     0.2891    0.02399     0.06\n",
      "NOTE:     35   256    0.001           0.9277     0.2656    0.02399     0.06\n",
      "NOTE:     36   256    0.001           0.8739     0.2188      0.024     0.06\n",
      "NOTE:     37   256    0.001           0.9244     0.2656      0.024     0.06\n",
      "NOTE:     38   256    0.001           0.9585     0.2695      0.024     0.06\n",
      "NOTE:     39   256    0.001           0.9105     0.2461      0.024     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  10        0.001          0.9648     0.2709     2.41\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001           0.9984     0.3164      0.024     0.06\n",
      "NOTE:      1   256    0.001           0.9535     0.3047      0.024     0.06\n",
      "NOTE:      2   256    0.001           0.8827     0.2461      0.024     0.06\n",
      "NOTE:      3   256    0.001           0.9541      0.293      0.024     0.06\n",
      "NOTE:      4   256    0.001           0.8922     0.2539      0.024     0.06\n",
      "NOTE:      5   256    0.001           0.9431     0.2813      0.024     0.06\n",
      "NOTE:      6   256    0.001           0.9731     0.3125      0.024     0.06\n",
      "NOTE:      7   256    0.001           0.9496     0.2617      0.024     0.06\n",
      "NOTE:      8   256    0.001           0.9056     0.2695      0.024     0.06\n",
      "NOTE:      9   256    0.001           0.9607     0.2656      0.024     0.06\n",
      "NOTE:     10   256    0.001           0.9969     0.3047      0.024     0.06\n",
      "NOTE:     11   256    0.001           0.8861     0.2461      0.024     0.06\n",
      "NOTE:     12   256    0.001           0.9636       0.25      0.024     0.06\n",
      "NOTE:     13   256    0.001           0.9044     0.3047      0.024     0.06\n",
      "NOTE:     14   256    0.001           0.9938       0.25      0.024     0.06\n",
      "NOTE:     15   256    0.001           0.9439     0.2461      0.024     0.06\n",
      "NOTE:     16   256    0.001           0.9468     0.2813      0.024     0.06\n",
      "NOTE:     17   256    0.001           0.8982     0.2578      0.024     0.06\n",
      "NOTE:     18   256    0.001           0.9165     0.2891      0.024     0.06\n",
      "NOTE:     19   256    0.001           0.8548     0.2383      0.024     0.06\n",
      "NOTE:     20   256    0.001           0.9133     0.2617      0.024     0.06\n",
      "NOTE:     21   256    0.001           0.9607     0.3125      0.024     0.06\n",
      "NOTE:     22   256    0.001           0.8837     0.2539      0.024     0.06\n",
      "NOTE:     23   256    0.001           0.8831     0.2539      0.024     0.06\n",
      "NOTE:     24   256    0.001           0.9521     0.2969    0.02401     0.06\n",
      "NOTE:     25   256    0.001           0.8988     0.2891    0.02401     0.06\n",
      "NOTE:     26   256    0.001           0.8864     0.2344    0.02401     0.06\n",
      "NOTE:     27   256    0.001           0.8112     0.1836    0.02401     0.06\n",
      "NOTE:     28   256    0.001           0.8403     0.2422    0.02401     0.06\n",
      "NOTE:     29   256    0.001           0.8465     0.2344    0.02401     0.06\n",
      "NOTE:     30   256    0.001           0.8847     0.2383    0.02401     0.06\n",
      "NOTE:     31   256    0.001           0.9122     0.2617    0.02401     0.06\n",
      "NOTE:     32   256    0.001           0.9773     0.3281    0.02401     0.06\n",
      "NOTE:     33   256    0.001            1.004     0.3281    0.02401     0.06\n",
      "NOTE:     34   256    0.001           0.9026     0.2852    0.02401     0.06\n",
      "NOTE:     35   256    0.001           0.9173     0.2383    0.02401     0.06\n",
      "NOTE:     36   256    0.001           0.8706     0.2461    0.02401     0.06\n",
      "NOTE:     37   256    0.001             0.95     0.2852    0.02401     0.06\n",
      "NOTE:     38   256    0.001           0.8594       0.25    0.02401     0.06\n",
      "NOTE:     39   256    0.001           0.9162     0.2617    0.02401     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  11        0.001          0.9197     0.2689     2.41\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001           0.8459     0.2227    0.02401     0.06\n",
      "NOTE:      1   256    0.001           0.8378     0.2188    0.02401     0.06\n",
      "NOTE:      2   256    0.001           0.8928     0.2852    0.02401     0.06\n",
      "NOTE:      3   256    0.001           0.9254     0.2695    0.02401     0.06\n",
      "NOTE:      4   256    0.001           0.7849     0.2148    0.02401     0.06\n",
      "NOTE:      5   256    0.001           0.8504     0.2344    0.02401     0.06\n",
      "NOTE:      6   256    0.001           0.8619     0.2266    0.02401     0.06\n",
      "NOTE:      7   256    0.001            0.909     0.2734    0.02401     0.06\n",
      "NOTE:      8   256    0.001           0.8309     0.2266    0.02401     0.06\n",
      "NOTE:      9   256    0.001           0.8332       0.25    0.02401     0.06\n",
      "NOTE:     10   256    0.001           0.8887     0.2578    0.02401     0.06\n",
      "NOTE:     11   256    0.001           0.9114     0.2773    0.02401     0.06\n",
      "NOTE:     12   256    0.001           0.9465     0.3086    0.02401     0.06\n",
      "NOTE:     13   256    0.001           0.9067     0.2578    0.02402     0.06\n",
      "NOTE:     14   256    0.001           0.8362     0.2617    0.02402     0.06\n",
      "NOTE:     15   256    0.001           0.8771     0.2539    0.02402     0.06\n",
      "NOTE:     16   256    0.001           0.8741     0.2813    0.02402     0.06\n",
      "NOTE:     17   256    0.001           0.8548     0.2109    0.02402     0.06\n",
      "NOTE:     18   256    0.001           0.8541     0.2188    0.02402     0.06\n",
      "NOTE:     19   256    0.001           0.9222     0.2969    0.02402     0.06\n",
      "NOTE:     20   256    0.001           0.9154     0.2813    0.02402     0.06\n",
      "NOTE:     21   256    0.001           0.8881     0.2617    0.02402     0.06\n",
      "NOTE:     22   256    0.001           0.8631     0.2344    0.02402     0.06\n",
      "NOTE:     23   256    0.001            0.876     0.2852    0.02402     0.06\n",
      "NOTE:     24   256    0.001           0.8025     0.2305    0.02402     0.06\n",
      "NOTE:     25   256    0.001           0.9675      0.293    0.02402     0.06\n",
      "NOTE:     26   256    0.001           0.8127     0.2227    0.02402     0.06\n",
      "NOTE:     27   256    0.001           0.8228       0.25    0.02402     0.06\n",
      "NOTE:     28   256    0.001           0.9202     0.3047    0.02402     0.06\n",
      "NOTE:     29   256    0.001           0.8332       0.25    0.02402     0.06\n",
      "NOTE:     30   256    0.001           0.8522     0.2617    0.02402     0.06\n",
      "NOTE:     31   256    0.001           0.9225       0.25    0.02402     0.06\n",
      "NOTE:     32   256    0.001           0.9375     0.3125    0.02402     0.06\n",
      "NOTE:     33   256    0.001           0.8351     0.2773    0.02402     0.06\n",
      "NOTE:     34   256    0.001            0.839     0.2383    0.02402     0.06\n",
      "NOTE:     35   256    0.001           0.9274     0.3008    0.02402     0.06\n",
      "NOTE:     36   256    0.001           0.8855     0.2852    0.02402     0.06\n",
      "NOTE:     37   256    0.001           0.8895     0.2656    0.02402     0.06\n",
      "NOTE:     38   256    0.001           0.8932     0.2891    0.02402     0.06\n",
      "NOTE:     39   256    0.001           0.8608     0.2188    0.02402     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  12        0.001          0.8747      0.259     2.41\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001           0.8533     0.2227    0.02402     0.06\n",
      "NOTE:      1   256    0.001           0.8629     0.2539    0.02402     0.06\n",
      "NOTE:      2   256    0.001           0.8242       0.25    0.02402     0.06\n",
      "NOTE:      3   256    0.001           0.8591     0.2578    0.02402     0.06\n",
      "NOTE:      4   256    0.001           0.9129     0.3047    0.02403     0.06\n",
      "NOTE:      5   256    0.001           0.8645     0.2344    0.02403     0.06\n",
      "NOTE:      6   256    0.001           0.9069     0.3047    0.02403     0.06\n",
      "NOTE:      7   256    0.001           0.9146     0.2578    0.02403     0.06\n",
      "NOTE:      8   256    0.001           0.8508     0.2578    0.02403     0.06\n",
      "NOTE:      9   256    0.001           0.8553     0.2578    0.02403     0.06\n",
      "NOTE:     10   256    0.001           0.8775     0.2578    0.02403     0.06\n",
      "NOTE:     11   256    0.001           0.8519       0.25    0.02403     0.06\n",
      "NOTE:     12   256    0.001           0.8439     0.2266    0.02403     0.06\n",
      "NOTE:     13   256    0.001           0.8225     0.2539    0.02403     0.06\n",
      "NOTE:     14   256    0.001           0.8739     0.2734    0.02403     0.06\n",
      "NOTE:     15   256    0.001           0.8364     0.2656    0.02403     0.06\n",
      "NOTE:     16   256    0.001           0.7388      0.207    0.02403     0.06\n",
      "NOTE:     17   256    0.001           0.9148     0.2617    0.02403     0.06\n",
      "NOTE:     18   256    0.001           0.8522     0.2578    0.02403     0.06\n",
      "NOTE:     19   256    0.001           0.9181     0.2695    0.02403     0.06\n",
      "NOTE:     20   256    0.001           0.8383     0.2734    0.02403     0.06\n",
      "NOTE:     21   256    0.001           0.8655       0.25    0.02403     0.06\n",
      "NOTE:     22   256    0.001           0.8191     0.2539    0.02403     0.06\n",
      "NOTE:     23   256    0.001           0.9094     0.2383    0.02403     0.06\n",
      "NOTE:     24   256    0.001            0.881     0.2734    0.02403     0.06\n",
      "NOTE:     25   256    0.001           0.7906     0.2344    0.02403     0.06\n",
      "NOTE:     26   256    0.001            0.764     0.2188    0.02403     0.06\n",
      "NOTE:     27   256    0.001           0.8359     0.2539    0.02403     0.06\n",
      "NOTE:     28   256    0.001           0.8381     0.2578    0.02403     0.06\n",
      "NOTE:     29   256    0.001           0.8368     0.2734    0.02403     0.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:     30   256    0.001           0.8263     0.2617    0.02403     0.06\n",
      "NOTE:     31   256    0.001           0.8105     0.2227    0.02403     0.06\n",
      "NOTE:     32   256    0.001           0.8678     0.2656    0.02403     0.06\n",
      "NOTE:     33   256    0.001           0.7547     0.2031    0.02403     0.06\n",
      "NOTE:     34   256    0.001           0.8463     0.2578    0.02403     0.06\n",
      "NOTE:     35   256    0.001           0.8351     0.2656    0.02403     0.06\n",
      "NOTE:     36   256    0.001           0.7539     0.2188    0.02404     0.06\n",
      "NOTE:     37   256    0.001           0.7923     0.2266    0.02404     0.06\n",
      "NOTE:     38   256    0.001           0.8568     0.2617    0.02404     0.06\n",
      "NOTE:     39   256    0.001           0.8144     0.2813    0.02404     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  13        0.001          0.8443     0.2529     2.41\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001           0.8556     0.2617    0.02404     0.06\n",
      "NOTE:      1   256    0.001           0.8348       0.25    0.02404     0.06\n",
      "NOTE:      2   256    0.001            0.751     0.2109    0.02404     0.06\n",
      "NOTE:      3   256    0.001           0.7839     0.2734    0.02404     0.06\n",
      "NOTE:      4   256    0.001           0.8377     0.2578    0.02404     0.06\n",
      "NOTE:      5   256    0.001           0.8055     0.2734    0.02404     0.06\n",
      "NOTE:      6   256    0.001           0.8192     0.2383    0.02404     0.06\n",
      "NOTE:      7   256    0.001           0.8355     0.2617    0.02404     0.06\n",
      "NOTE:      8   256    0.001           0.8497     0.2461    0.02404     0.06\n",
      "NOTE:      9   256    0.001           0.8584     0.2461    0.02404     0.06\n",
      "NOTE:     10   256    0.001           0.7946     0.2383    0.02404     0.06\n",
      "NOTE:     11   256    0.001            0.833     0.2695    0.02404     0.06\n",
      "NOTE:     12   256    0.001           0.8593     0.2852    0.02404     0.06\n",
      "NOTE:     13   256    0.001           0.8246       0.25    0.02404     0.06\n",
      "NOTE:     14   256    0.001           0.8346     0.2734    0.02404     0.06\n",
      "NOTE:     15   256    0.001           0.8441     0.2578    0.02404     0.06\n",
      "NOTE:     16   256    0.001           0.8582     0.2734    0.02404     0.06\n",
      "NOTE:     17   256    0.001           0.8028     0.2578    0.02404     0.06\n",
      "NOTE:     18   256    0.001            0.849     0.2422    0.02404     0.06\n",
      "NOTE:     19   256    0.001            0.757     0.2148    0.02404     0.06\n",
      "NOTE:     20   256    0.001           0.8261     0.2734    0.02404     0.06\n",
      "NOTE:     21   256    0.001           0.7936     0.2305    0.02404     0.06\n",
      "NOTE:     22   256    0.001           0.8541       0.25    0.02404     0.06\n",
      "NOTE:     23   256    0.001           0.8483     0.2773    0.02404     0.06\n",
      "NOTE:     24   256    0.001            0.863     0.2852    0.02404     0.06\n",
      "NOTE:     25   256    0.001            0.845     0.2617    0.02404     0.06\n",
      "NOTE:     26   256    0.001           0.7561     0.2266    0.02404     0.06\n",
      "NOTE:     27   256    0.001           0.8131     0.2656    0.02404     0.06\n",
      "NOTE:     28   256    0.001           0.7578     0.2266    0.02404     0.06\n",
      "NOTE:     29   256    0.001            0.727     0.2031    0.02404     0.06\n",
      "NOTE:     30   256    0.001            0.801     0.2539    0.02405     0.06\n",
      "NOTE:     31   256    0.001           0.9233     0.3086    0.02405     0.06\n",
      "NOTE:     32   256    0.001           0.8597     0.2969    0.02405     0.06\n",
      "NOTE:     33   256    0.001           0.8053     0.2656    0.02405     0.06\n",
      "NOTE:     34   256    0.001           0.7907     0.2266    0.02405     0.06\n",
      "NOTE:     35   256    0.001           0.8312     0.2813    0.02405     0.06\n",
      "NOTE:     36   256    0.001           0.8212     0.2578    0.02405     0.06\n",
      "NOTE:     37   256    0.001           0.8058       0.25    0.02405     0.06\n",
      "NOTE:     38   256    0.001             0.82     0.2578    0.02405     0.06\n",
      "NOTE:     39   256    0.001           0.7269     0.2109    0.02405     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  14        0.001          0.8189     0.2548     2.41\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is      36.29 (s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Name</td>\n",
       "      <td>lenet5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Type</td>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number of Layers</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number of Input Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number of Output Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Number of Convolutional Layers</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Number of Pooling Layers</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Number of Fully Connected Layers</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Number of FCMP Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Number of Weight Parameters</td>\n",
       "      <td>107454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Number of Bias Parameters</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Total Number of Model Parameters</td>\n",
       "      <td>107690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Approximate Memory Cost for Training (MB)</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OptIterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch\">Epoch</th>\n",
       "      <th title=\"LearningRate\">LearningRate</th>\n",
       "      <th title=\"Loss\">Loss</th>\n",
       "      <th title=\"FitError\">FitError</th>\n",
       "      <th title=\"L2Norm\">L2Norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.328266</td>\n",
       "      <td>0.844434</td>\n",
       "      <td>0.023856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.036806</td>\n",
       "      <td>0.678223</td>\n",
       "      <td>0.023857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.831030</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.023865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.652424</td>\n",
       "      <td>0.466992</td>\n",
       "      <td>0.023877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.525587</td>\n",
       "      <td>0.422461</td>\n",
       "      <td>0.023892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.395286</td>\n",
       "      <td>0.375977</td>\n",
       "      <td>0.023908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.275378</td>\n",
       "      <td>0.341602</td>\n",
       "      <td>0.023925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.167726</td>\n",
       "      <td>0.315137</td>\n",
       "      <td>0.023942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.092559</td>\n",
       "      <td>0.295996</td>\n",
       "      <td>0.023958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.033617</td>\n",
       "      <td>0.291504</td>\n",
       "      <td>0.023974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.964783</td>\n",
       "      <td>0.270898</td>\n",
       "      <td>0.023989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.919708</td>\n",
       "      <td>0.268945</td>\n",
       "      <td>0.024003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.874705</td>\n",
       "      <td>0.258984</td>\n",
       "      <td>0.024017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.844282</td>\n",
       "      <td>0.252930</td>\n",
       "      <td>0.024030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.818950</td>\n",
       "      <td>0.254785</td>\n",
       "      <td>0.024042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(weshiz)</td>\n",
       "      <td>lenet5_weights</td>\n",
       "      <td>107690</td>\n",
       "      <td>3</td>\n",
       "      <td>CASTable('lenet5_weights', caslib='CASUSER(wes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 36.3s</span> &#183; <span class=\"cas-user\">user 283s</span> &#183; <span class=\"cas-sys\">sys 0.699s</span> &#183; <span class=\"cas-memory\">mem 60.6MB</span></small></p>"
      ],
      "text/plain": [
       "[ModelInfo]\n",
       "\n",
       "                                         Descr                         Value\n",
       " 0                                  Model Name                        lenet5\n",
       " 1                                  Model Type  Convolutional Neural Network\n",
       " 2                            Number of Layers                             9\n",
       " 3                      Number of Input Layers                             1\n",
       " 4                     Number of Output Layers                             1\n",
       " 5              Number of Convolutional Layers                             2\n",
       " 6                    Number of Pooling Layers                             2\n",
       " 7            Number of Fully Connected Layers                             2\n",
       " 8                       Number of FCMP Layers                             1\n",
       " 9                 Number of Weight Parameters                        107454\n",
       " 10                  Number of Bias Parameters                           236\n",
       " 11           Total Number of Model Parameters                        107690\n",
       " 12  Approximate Memory Cost for Training (MB)                            43\n",
       "\n",
       "[OptIterHistory]\n",
       "\n",
       "     Epoch  LearningRate      Loss  FitError    L2Norm\n",
       " 0       1         0.001  2.328266  0.844434  0.023856\n",
       " 1       2         0.001  2.036806  0.678223  0.023857\n",
       " 2       3         0.001  1.831030  0.554688  0.023865\n",
       " 3       4         0.001  1.652424  0.466992  0.023877\n",
       " 4       5         0.001  1.525587  0.422461  0.023892\n",
       " 5       6         0.001  1.395286  0.375977  0.023908\n",
       " 6       7         0.001  1.275378  0.341602  0.023925\n",
       " 7       8         0.001  1.167726  0.315137  0.023942\n",
       " 8       9         0.001  1.092559  0.295996  0.023958\n",
       " 9      10         0.001  1.033617  0.291504  0.023974\n",
       " 10     11         0.001  0.964783  0.270898  0.023989\n",
       " 11     12         0.001  0.919708  0.268945  0.024003\n",
       " 12     13         0.001  0.874705  0.258984  0.024017\n",
       " 13     14         0.001  0.844282  0.252930  0.024030\n",
       " 14     15         0.001  0.818950  0.254785  0.024042\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib            Name    Rows  Columns  \\\n",
       " 0  CASUSER(weshiz)  lenet5_weights  107690        3   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('lenet5_weights', caslib='CASUSER(wes...  \n",
       "\n",
       "+ Elapsed: 36.3s, user: 283s, sys: 0.699s, mem: 60.6mb"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data=test, optimizer=optimizer, n_threads=8, target='_label_', inputs='_image_', nominals='_label_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x283141f17b8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAAFACAYAAAC/abrtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3Xd8XOWd7/HPM0W9S5ZsSW6427ItG+EKxhgIJWDaZpPAhpBNQkhCSd16793c7CabbHIhIYSQUFMIpGFiwEASgwFjmox7wwUXFVuWbPU+89w/zqhasmRb0pmRvu/X67zmzJkzM78RRvrq0e95jrHWIiIiIiIy0nncLkBEREREJBwoGIuIiIiIoGAsIiIiIgIoGIuIiIiIAArGIiIiIiKAgrGIiIiICKBgLCIiIiICKBiLiIiIiAAKxiIiIiIiAPjceuOMjAw7YcIEt95eREREREaIjRs3lltrR/V1nmvBeMKECRQWFrr19iIiIiIyQhhjDvXnPLVSiIiIiIigYCwiIiIiAigYi4iIiIgALvYYi4iIiMiZaWlpoaioiMbGRrdLCUsxMTHk5ubi9/vP6vkKxiIiIiIRoqioiMTERCZMmIAxxu1ywoq1loqKCoqKipg4ceJZvYZaKUREREQiRGNjI+np6QrFPTDGkJ6efk6j6QrGIiIiIhFEobh35/q1UTAWEREREUHBWERERETOQEJCgtslDJoRFYyttTzyxgGqGlrcLkVEREREwsyICsa7j9bw/Zd2c90D69lztMbtckRERESGhUOHDnHppZcyZ84cLr30Ug4fPgzAH/7wB/Ly8pg7dy7Lli0DYMeOHSxYsID8/HzmzJnD3r173Sy9ixG1XNuMMUk89flF3PGb97nhwTf5fx+by1Wzx7hdloiIiMgZ+7/P7WBnSfWAvubM7CT+49pZZ/y8O++8k1tvvZVPf/rTPPbYY9x99908++yzfPvb3+bll18mJyeHyspKAB566CHuuecebrnlFpqbmwkEAgP6Gc7FiBoxBiiYkMbzd13ItNGJfPHJ9/mfl3YTCFq3yxIRERGJWG+99RY333wzAJ/61KdYv349AEuXLuW2227j4Ycfbg/Aixcv5rvf/S7f//73OXToELGxsa7V3d2IGjFuMzo5hqdvX8S3Vu/gwXX72VFSzf2fmEdy3NldJUVERERkqJ3NyO5QaVs27aGHHuKdd97hhRdeID8/n82bN3PzzTezcOFCXnjhBa644goeeeQRVqxY4XLFjhE3Ytwm2uflv2+cw3dvmM2G/eVc+8B6dh8d2D9HiIiIiIwES5Ys4emnnwbgySef5MILLwRg//79LFy4kG9/+9tkZGRw5MgRDhw4wHnnncfdd9/NypUr2bp1q5uldzFig3GbmxeO4+nbF9PYEuCGn27g+a0lbpckIiIiErbq6+vJzc1t3+69917uv/9+Hn/8cebMmcOvf/1rfvzjHwPwzW9+k9mzZ5OXl8eyZcuYO3cuv/vd78jLyyM/P5/du3dz6623uvyJOhhr3emvLSgosIWFha68d0/Kqhv54pPvs/HQSe64eBLfvGIaXo+uLCMiIiLhY9euXcyYMcPtMsJaT18jY8xGa21BX88d8SPGbTKTYnjq84u4ZeE4HnptP7c9/i6V9c1ulyUiIiIiQ0TBuJMon4fv3DCb7904m3cOnODaB9YP+DIoIiIiIhKeFIx78IkF4/jdFxbR3Brkxp+9yeot6jsWERERGe4UjHsxb1wqz911IXnZydz91Ca+u2YXrYGg22WJiIiIyCBRMD6NzMQYfvv5RXxq0Xh+8foBbnv8PU7Wqe9YREREZDhSMO5DlM/Df16fx//cNId3P3T6jneUVLldloiIiIgMMAXjfvr7C8by+zsW0xqw3PSzDfx5c7HbJYmIiIgMOa/XS35+fvt28OBBCgsLufvuuwFYt24dGzZsaD//W9/6Fjk5OV2eU1lZ6Vb5pzUiLwl9tvLHpvDcXRfy5d++zz1Pb2ZbURX/ctV0fF79fiEiIiIjQ2xsLJs3b+5ybMKECRQUOMsEr1u3joSEBJYsWdL++Fe/+lW+8Y1v9Pqara2t+HwdsTQQCOD1evusxVqLtRaPZ2CymBLdGRqVGM2Tn1vIbUsm8Mj6D7n1sXepqG1yuywRERER16xbt45rrrmGgwcP8tBDD3HfffeRn5/PG2+80etznnjiCT72sY9x7bXX8pGPfIR169ZxySWXcPPNNzN79mwA7r33XvLy8sjLy+NHP/oRAAcPHmTGjBl86UtfYv78+Rw5cmTAPodGjM+C3+vhWytnkZeTzL+t2sbKB97k5586n7ycZLdLExERkZHixX+Bo9sG9jVHz4arvnfaUxoaGsjPzwdg4sSJrFq1qv2xCRMmcMcdd5CQkNA+Qrx27Vruu+8+fvOb3wCQmprKq6++CsBbb73F1q1bSUtLY926dbz77rts376diRMnsnHjRh5//HHeeecdrLUsXLiQiy++mNTUVPbs2cPjjz/Ogw8+OKAfX8H4HPzd+blMzUrgjl9v5KafbeB7N83mhnm5bpclIiIiMmh6aqXoS2+tFJdffjlpaWnt9xcsWMDEiRMBWL9+PTfccAPx8fEA3HjjjbzxxhusXLmS8ePHs2jRonP4FD1TMD5Hc3JTWH3XhXz5yff56u+2sK2omn+9ejp+9R2LiIjIYOpjZDcStIXenu5ba/v9vIGi9DYAMhKi+c3nFvKZpRN47M0P+dSj71CuvmMREREZgRITE6mpqTnn11m2bBnPPvss9fX11NXVsWrVKi666KIBqLB3CsYDxO/18B/XzuLev5/LpsOVrPzJerYWhedSJCIiIiKD5dprr2XVqlVdJt+1TcbrvMRbX+bPn89tt93GggULWLhwIZ/73OeYN2/eoNZuTjdMPZgKCgpsYWGhK+892LYXV/GFX2/keG0T/33DbG46X33HIiIicu527drFjBkz3C4jrPX0NTLGbLTWFvT1XI0YD4K8nGRW37mUgvGpfP0PW/jW6h20BIJulyUiIiIip6FgPEjSE6L51T8u4LMXTuSJDQe55ZF3OF6jvmMRERGRcKVgPIh8Xg//+5qZ/Ojj+WwtqmTlA+vZckR9xyIiInL23GqDjQTn+rVRMB4C18/L4Y93LMFjDB/7+Vv8vnDgrtAiIiIiI0dMTAwVFRUKxz2w1lJRUUFMTMxZv4Ym3w2hE3XN3PXU+7y5r4JbF4/nf310JlE+/W4iIiIi/dPS0kJRURGNjY1ulxKWYmJiyM3Nxe/3dzne38l3usDHEEqLj+KXn1nAD17ew89fP8Cu0mp+est8MhPP/jcbERERGTn8fn/7leFk4Gm4coj5vB7+9eoZ3P/JeWwrruLan6xn0+GTbpclIiIiMuIpGLtk5dxsnvniUqJ8Hj7+87f53XuH3S5JREREZERTMHbRzOwkVn/5Qhael8Y//2kb/75qG82tWu9YRERExA0Kxi5LjY/iic8s4I6LJ/HkO4f55MNvU1athnoRERGRoaZgHAa8HsO/XDWdB26ex86Saq75yXo2HlLfsYiIiMhQUjAOI9fMyWbVl5cQ4/fyiV+8xW/fUd+xiIiIyFBRMA4z00cnsfrOpSyelMG/rdrG13+/hfJaXUpaREREZLApGIehlLgoHr/tAu5aMZlnNxez/Afr+Nm6/TS2BNwuTURERGTY6jMYG2PGGmNeNcbsMsbsMMbc08M5xhhzvzFmnzFmqzFm/uCUO3J4PYavf2QaL39lGYvOS+P7L+3msntf4/mtJboMpIiIiMgg6M+IcSvwdWvtDGAR8GVjzMxu51wFTAlttwM/G9AqR7DJmQk88ukLePJzC0mI9nHnbzdx08826KIgIiIiIgOsz2BsrS211r4f2q8BdgE53U67DviVdbwNpBhjxgx4tSPY0skZvHD3RXz/ptkcPtHADQ9u4J6nN1Fc2eB2aSIiIiLDwhn1GBtjJgDzgHe6PZQDHOl0v4hTwzPGmNuNMYXGmMLjx4+fWaWC12P4+AXjWPfN5dx5yWRe2n6UFT9cxw9e3k1tU6vb5YmIiIhEtH4HY2NMAvAn4CvW2uruD/fwlFMaYa21v7DWFlhrC0aNGnVmlUq7hGgf37hiGq98YzlX5Y3mp6/uZ/kP1vH0u4cJBNV/LCIiInI2+hWMjTF+nFD8pLX2mR5OKQLGdrqfC5Sce3lyOjkpsfzoE/N49stLmZAex788s42P3v8G6/eWu12aiIiISMTpz6oUBngU2GWtvbeX01YDt4ZWp1gEVFlrSwewTjmN/LEp/OGOxTx4y3zqmlv5h0ff4bNPvMe+slq3SxMRERGJGKavpb+MMRcCbwDbgGDo8L8B4wCstQ+FwvMDwJVAPfAZa23h6V63oKDAFhae9hQ5C40tAX654SAPvLKP+pYA/7BwHPdcNpW0+Ci3SxMRERFxhTFmo7W2oM/z3FoTV8F4cFXUNnHf3z7gt+8cJj7ax90rpnDrkvFE+7xulyYiIiIypPobjHXlu2EqPSGa/7p+Ni99ZRnnj0/lO2t28ZH7Xuel7aW6QIiIiIhIDxSMh7mpWYk88ZkF/PIfFxDt83DHb97n4z9/m61FlW6XJiIiIhJWFIxHiIunjmLN3RfxnRvy2H+8lpUPvMnXfr+Z0ipdIEREREQEFIxHFJ/Xwy0Lx7Pum8v54vJJPL+1lEt+uI57//oBdbpAiIiIiIxwCsYjUGKMn3++cjprv3Yxl83I4v61e7nkh+v4feERgrpAiIiIiIxQCsYj2Ni0OB64eT5/+uISslNi+ac/buXaB9bz1v4Kt0sTERERGXIKxsL541NZ9aUl3P/JeVTWt/DJh9/m9l8V8mF5nduliYiIiAwZBWMBwBjDyrnZrP36xXzzimm8ua+cy+99jW8/t5PK+ma3yxMREREZdArG0kWM38uXL5nMum9ewscKxvLEhg+5+AfreGz9h7QEgn2/gIiIiEiEUjCWHo1KjOa/b5zNmnsuYk5uMt9+fidX3Pc6f915TBcIERERkWFJwVhOa/roJH71jwt4/LYLMAY+/6tCbn74HXaUVLldmoiIiMiAUjCWPhljuGR6Ji99ZRn/ed0sdh+t5pqfrOef/riFsupGt8sTERERGRAKxtJvfq+HTy2ewLpvXsLnLzqPVZuKWf7Dddy/di8NzQG3yxMRERE5JwrGcsaSY/3829Uz+NvXLmb5tFHc+9cPuOSH63jm/SJdIEREREQiloKxnLXx6fE8eMv5/OGOxWQmRfO132/hih+9zp83FxNQQBYREZEIo2As5+yCCWk8+6Wl/OST8zAG7nl6M5fd+xp/KDyiJd5EREQkYhi3lt4qKCiwhYWFrry3DJ5g0PKXnUe5f+0+dpZWMzYtli8tn8xN83OJ8un3MBERERl6xpiN1tqCPs9TMJbBYK3lld1l3L92L1uKqshOjuGO5ZP4+4KxxPi9bpcnIiIiI4iCsYQFay2v7y3nJ2v3UnjoJJmJ0Xzh4kncvGAcsVEKyCIiIjL4FIwlrFhreetABT9Zu4+3DlSQkRDF5y46j08tGk98tM/t8kRERGQYUzCWsPXewRPcv3Yvb+wtJyXOz2eXTuTTSyeQFON3uzQREREZhhSMJextOnySB17Zx9rdZSTG+PjM0on849IJpMRFuV2aiIiIDCMKxhIxthdX8ZNX9vLyjmMkRPu4dfF4PnvhRNITot0uTURERIYBBWOJOLuPVvPAK/t4YVspMT4v/7BoHJ9fdh6ZiTFulyYiIiIRTMFYIta+shp++up+/ry5GL/XwycXjOMLF5/HmORYt0sTERGRCKRgLBHvYHkdD67bxzPvF+Mxho8V5PLF5ZPITY1zuzQRERGJIArGMmwcOVHPz17bzx8Kj2At3Dg/hy8tn8yEjHi3SxMREZEIoGAsw05pVQM/f+0AT717mJZAkOvzc/jSJZOZnJngdmkiIiISxhSMZdgqq2nk4dcP8Ju3D9PYGuCjs8dw14opTBud6HZpIiIiEoYUjGXYq6ht4pH1H/KrDQepaw5wxaws7loxhbycZLdLExERkTCiYCwjRmV9M4+9eZDH3/yQmsZWLp2eyV2XTiF/bIrbpYmIiEgYUDCWEae6sYVfvnmQR9/8kMr6Fi6aksHdl07hgglpbpcmIiIiLlIwlhGrtqmV37x9iIdfP0BFXTOLz0vnrksns/i8dIwxbpcnIiIiQ0zBWEa8huYAv333MD9/bT9lNU0UjE/lrkunsGxKhgKyiIjICKJgLBLS2BLg94VH+Nm6/ZRWNTJ3bAp3r5jMiumZCsgiIiIjgIKxSDdNrQH+tLGYB9fto+hkA5MzE7g+P5tr52YzPl0XCxERERmuFIxFetESCPLnzSX8/r0jvHvwBABzx6awcm4218wZQ1ZSjMsVioiIyEBSMBbph5LKBp7fWsLqLSVsL67GGFg4MY2Vc3O4Km80qfFRbpcoIiIi50jBWOQM7T9ey3NbnJB84HgdPo9h2dRRrJybzeUzs4iP9rldooiIiJwFBWORs2StZUdJNc9tKeG5LSWUVDUS4/dw6YwsVs7NZvm0UUT7vG6XKSIiIv2kYCwyAIJBy8bDJ1m9uYQ120qpqGsmMcbHlbNGc+3cbJZMSsfn9bhdpoiIiJyGgrHIAGsNBNmwv4LVW0p4eftRappayUiI4urZY1g5N5v541LxeLT8m4iISLhRMBYZRI0tAdbtOc5zW0r4265jNLUGyUmJ5Zq5TkieOSZJaySLiIiECQVjkSFS29TKX3ceZfXmEt7YW05r0DJpVDwr5+awMj+biRlaI1lERMRNCsYiLjhR18yL20tZvbmEdw+ewFrIy0kKrZGcTXZKrNslioiIjDgKxiIuO1rVyPNbnZUtthRVAbBgQhrX5mdzdd5o0hOiXa5QRERkZFAwFgkjB8vr2tdI3ltWi9djuHByBivnZvORWVkkxvjdLlFERGTYUjAWCUPWWnYfrWF1aI3kopMNRPk8rJiWycr8bFZMzyTGrzWSRUREBpKCsUiYs9by/uFKnttSwvNbSymvbSIh2sdHZmZxbX42F07OwK81kkVERM6ZgrFIBGkNBHn7wAlWbynmxe1HqWlsJS0+iqvyRrNybjYXTEjTGskiIiJnScFYJEI1tQZ4/YNyVm8p4W87j9HQEmBMcgzXzs3m+vwcZoxJ1BrJIiIiZ2DAgrEx5jHgGqDMWpvXw+PLgT8DH4YOPWOt/XZfb6xgLNK3uqZW/rbrGKs3l/DaB8dpDVqmZiVw/bwcrsvPIUfLv4mIiPRpIIPxMqAW+NVpgvE3rLXXnEmBCsYiZ+ZEXTMvbCvl2U3FbDx0EnCWf7t+Xg5Xzx5NSlyUyxWKiIiEpwFtpTDGTACeVzAWCQ9HTtTz583FrNpUzP7jdfi9huXTMrlhXo5WthAREelmqIPxn4AioAQnJO/o5XVuB24HGDdu3PmHDh3q871FpHfWWnaUVPPspmJWbymhrKaJxGgfV+aN5oZ5OSw8Lx2vJu2JiMgIN5TBOAkIWmtrjTFXAz+21k7p6zU1YiwysAJBy1v7K3h2czEvbT9KbVMrWUnRrJybzfXzcpg5JkmT9kREZEQasmDcw7kHgQJrbfnpzlMwFhk8jS0B/rbrGM9uKmHdnjJag5Ypmc6kvZVzsxmbFud2iSIiIkOmv8HYNwBvNBo4Zq21xpgFgAeoONfXFZGzF+P3cs2cbK6Zk83J0KS9P28u5gcv7+EHL+/hggmpXJefw0dnjyE1XpP2REREoH+rUjwFLAcygGPAfwB+AGvtQ8aYO4EvAq1AA/A1a+2Gvt5YI8YiQ+/IiXpWbynh2U3F7C2rxe81XDzVmbR36QxN2hMRkeFJF/gQkV5Za9lZ2jFp71i1cznqK/NGc31+DosnadKeiIgMHwrGItIvgaDl7QMVPLvJmbRX09RKZmLHpL1Z2Zq0JyIikU3BWETOWGNLgFd2l7FqUzHr9pTRErBMzkzg+vxsrsvP0aQ9ERGJSArGInJOKutDk/Y2lfDuwRMAFIxP5bp5OVyjSXsiIhJBFIxFZMAUneyYtPfBsVp8HsPyaaO4Lj+Hy2ZkERulSXsiIhK+FIxFZMBZa9lVWsOzm4tZvbmEo9WNJET7uGLWaK6fl82SSRmatCciImFHwVhEBlUgaHnnQ2fS3ovbOibtXTs3m5Vzs5mTm6xJeyIiEhYUjEVkyDS2BHi1fdLecZoDQXJSYrl69miumj2G/NwUPBpJFhERlygYi4grqupb+MvOo7y4/Shv7D1OS8AyJjmGK/NGc/XsMZw/LlUhWUREhpSCsYi4rqqhhVd2H2PNtqO89sFxmluDZCZGc2XeaK7KG8OCiWnqSRYRkUGnYCwiYaW2qZVXdpfx4rZSXt1TRmNLkIyEKK6Y5YwkL5yYhs/rcbtMEREZhhSMRSRs1Te38uru46zZXsqru8uobw6QGufnillOT/KSSen4FZJFRGSAKBiLSERoaA7w2gfHeXF7KWt3lVHb1EpyrJ/LZ2Zx9ezRLJ2cQbRP6ySLiMjZUzAWkYjT2BJg/d5y1mwv5a87j1HT2EpitI/LZmZxVd5olk0dRYxfIVlERM5Mf4OxbyiKERHpjxi/l8tmZnHZzCyaW4O8ub+cF7eV8pedx1i1qZj4KC8rZmRxdd5olk/L1BX3RERkQGnEWETCXksgyNsHKliz7Sgv7zjKibpmYv1eLpk+iqtnj+GSaZnER+v3fBER6ZlaKURkWGoNBHn34AnWbCvlpe3HKK9tItrnYfk0JySvmJ5JYozf7TJFRCSMKBiLyLAXCFoKD57gxe1HeXF7Kceqm4jyelg2NYOr8sZw2cwskmMVkkVERjoFYxEZUYJBy6YjJ1mz7SgvbiulpKoRv9ewdHIGV+eN4fKZWaTGR7ldpoiIuEDBWERGLGstW4qqeHFbKS9sK6XoZANej2HJpHSuyhvDR2ZlkZEQ7XaZIiIyRBSMRURwQvKOkmrWbCtlzbZSDlbU4zGwcGI6V88ezRWzRpOZFON2mSIiMogUjEVEurHWsvtoTftI8v7jdRgDc3NTWDE9kxXTM5mVnYQxxu1SRURkACkYi4j04YNjNby0/Shrd5extagSayErKZpLpjkheenkDC0DJyIyDCgYi4icgeM1TazbU8are8p4/YNyaptaifJ6WDQpnRXTRrFiehbj0uPcLlNERM6CgrGIyFlqbg1SePAEr+wu45XdZRworwNgcmYCl07P5JLpmZw/PhW/1+NypSIi0h8KxiIiA+TD8jpe2V3Gq7vLeOfDCloClqQYH8umjuLSGZlcPDWTNC0FJyISthSMRUQGQU1jC2/uK2ftrjJe3XOc8tomjIF5Y1O4dEYWl0zLZMaYRE3gExEJIwrGIiKDLBi0bCuuam+52FZcBcCY5BgumZ7JpdMzWTIpg9gor8uVioiMbArGIiJDrKy6kXV7jrN29zHW7y2nrjlAtM/DkknprAj1JuemagKfiMhQUzAWEXFRU2uAdz/smMB3qKIegGlZic5o8oxM5o1NwacJfCIig07BWEQkTFhrOVBexyu7nJD83sETtAYtybF+lk8bxYrpmVw8dRQpcZrAJyIyGBSMRUTCVHVjC298UM4ru8tYt6eMirpmPAbOH58a6k3OYmpWgibwiYgMEAVjEZEIEAhathZVtrdc7CipBiAnJbb9MtWLJ6UT49cEPhGRs6VgLCISgY5WNfLqnjLW7irjzX3lNLQEiPF7WDopgxUzMrlwcgbj0uI0miwicgYUjEVEIlxjS4C3D1Tw6u4y1u4uo+hkAwDJsX7m5CaHthTm5CYzOilGYVlEpBcKxiIiw4i1ln1ltbx38CRbiyrZWlTFnmM1BILO9/BRidHMDQXl2bnJzM1N0dX4RERC+huMfUNRjIiInBtjDFOyEpmSlcjNC8cBzojyjpJqtoWC8paiStbuLqNtvCM3NbbLqHJeTjJJMX4XP4WISHhTMBYRiVAxfi/nj0/l/PGp7cdqGlvYXlztjCoXV7G1qJI12462P37eqHjmhoLynNxkZmUna2KfiEiIgrGIyDCSGONn8aR0Fk9Kbz92oq6ZbcVVbD1SyZaiKt7cV86qTcUAeD2GqVmJzMlJZs5YpwVjalYiUT5deERERh71GIuIjEDHqhvZcsRpwWgbWa6sbwEgyudhxpik9p7lObnJTBqVgNejyX0iEpk0+U5ERPrNWsuREw1sLQ71Kx+pZHtxFXXNAQDio7zMykkOjSynMDc3WcvGiUjEUDAWEZFzEgxaDpTXsuVIFduKncl9O0qqaW4NAlo2TkQih4KxiIgMuJZAkA+O1TgtGG3Lxh2tobXbsnGzc5ygPHV0ItnJCssi4i4t1yYiIgPO7/UwK9tZzeKTCzqWjdtZWs220JJxW4uquiwbFxflZdKoBKZkJjApM4HJmc7+uLQ4fF5N8hOR8KFgLCIi5yTG72X+uFTmj+tYNq62qZWdJdXsLathX1kt+8pqeetABc+EVsMAiPJ6mJARx5TMxPbAPHlUAueNitcSciLiCgVjEREZcAnRPhZMTGPBxLQux2ubWtlfVsveUFjeV1bLjpIqXtxeSqgbA4+BsWlxTB6VwOQsJyxPDgXnRF2gREQGkYKxiIgMmYRoH3PHpjB3bEqX440tAQ5W1LH3WCgwH69lf1ktb+wtpzkQbD9vdFJMe0ie3KktIz0heqg/iogMQwrGIiLiuhi/l+mjk5g+OqnL8dZAkCMnG9hXVtvelrG/rJY/FB5pX0oOIDXO3yksJ7bva+KfiJwJBWMREQlbPq+HiRnxTMyI5/KZWe3HrbWUVjWGAnNte2B+aftRTtYfaT8vPsrr9C93a8vQxD8R6YmCsYiIRBxjDNkpsWSnxLJs6qguj1XUNnUNzMdr2bD/1Il/EzPiu7RkTBudyHkZ8QrMIiOYgrGIiAwr6QnRpCdEs/C89C7Haxpb2H+8jr3Hatp7mLd3m/gX7fMwfXQiM7OTmDkmiZnZTntHfLR+XIqMBH3+n26MeQy4Biiz1ub18LgBfgxcDdQDt1lr3x/oQkVERM5FYoyf/LEp5Pcw8e/A8Tr2HKtmZ0k1O0ureXH7UZ5612nJMAYmpsczIxSWZ2U7gTkzMcaNjyEig6g/vwI/ATw2sfwTAAAeMklEQVQA/KqXx68CpoS2hcDPQrciIiJhL8bvdUaIs5O4YZ5zrK2HeWdJNTtKqtlZ6lzp74Wtpe3Py0iIZmZ2KCiHRpcnpMfj9Wiyn0ik6jMYW2tfN8ZMOM0p1wG/ss61pd82xqQYY8ZYa0tP8xwREZGw1bmH+bJOk/6qGlrYVdoxsryzpJpH3jhAS8DpxYiL8nZqxUgOtWIk6oIlIhFiIJqmcoAjne4XhY6dEoyNMbcDtwOMGzduAN5aRERk6CTH+ll0XjqLOvUvN7cG2VtW0x6Wd5RU8+dNJfzm7cOAc8GSSaMS2vuWZ2U7gTktPsqtjyEivRiIYNzT34xsTydaa38B/AKgoKCgx3NEREQiSZTPw6zsZGZlJ7cfs9ZSdLLBacMoqWJnaTXvfXiCP28uaT9ndFLMKa0YY1Pj8KgVQ8Q1AxGMi4Cxne7nAiW9nCsiIjLsGWMYmxbH2LQ4rswb3X78ZF1zewuGM7pcxWsfHCcQWhYjIdrXHpLbbqdkJRDtUyuGyFAYiGC8GrjTGPM0zqS7KvUXi4iInCo1PoqlkzNYOjmj/VhjS4APjtV0muhXze8Lj1AfurKfz2OYnJnQJSzPHJNESpxaMUQGWn+Wa3sKWA5kGGOKgP8A/ADW2oeANThLte3DWa7tM4NVrIiIyHAT4/cyJzeFObkdy8gFg5ZDJ+rZUVLVPrq8fm85z7zfcZGSxBgfualx5KTEkpvaeXOOpcT5dTlskTNknMUkhl5BQYEtLCx05b1FREQi0fGaJnaWVrPnaDVFJxsoPtlA0ckGik7WUxcaYW4TH+UlJxSUc1NjQwE6LnQslvT4KAVnGTGMMRuttQV9nadL+YiIiESIUYnRXJw4iou7XQbbWktVQ0soJDtBuehkA8WVzv3Cgyeobmzt8pwYv+eUsNw22jw2NZaMhGhNBJQRR8FYREQkwhljSImLIiUuiryc5B7PqW5s6TLC3LZfXNnA1qJKTta3dDk/yutpD8xt7RqdR6AzE2N0MRMZdhSMRURERoCkGD9JY/zMGJPU4+N1Ta2hEeaO0FwUGnHetesY5bXNXc73eZyLoHT0OMd1CdJjkmPweT1D8dFEBoyCsYiIiBAf7WNqViJTsxJ7fLyhOUBxZcOp4flkPa/vPc6x6qYu53s9htFJMR1tGt3aNsYkxxLlU3CW8KJgLCIiIn2KjfIyOTOByZkJPT7e1BqgtLIx1J5R36Xf+e39FRytbiTYab6/MZCVGNOpRSOWnJS49vs5KbG6lLYMOQVjEREROWfRPi8TMuKZkBHf4+MtgSBHqxo50q2/uehkPe8fPsnzW0vbL3TSZlRi9Kn9zZ3ux0UpxsjA0r8oERERGXR+r6f9aoA9aQ0EOVbTFArNXcPz9uIqXt5xlJZA1+CcFh/VdXJgqF0jN83ZT4zxD8VHk2FEwVhERERc5/M6y8flpMSyYGLaKY8Hg5bjtU3tS9F1Xo5uz7EaXtldRlNrsMtzkmP9p4w4t90fmxpHUqxPazlLFwrGIiIiEvY8HkNWUgxZSTGcP/7Ux621lNc2nzI5sLiygYMVdazfV95+me02CdG+U5ajy0mJIz0hivT4KNITokmJ9Ws95xFEwVhEREQinjGGUYnRjEqMJn9syimPW2uprG/pYXKgE6Tf/fAENU2tpzzPYyA1Loq0+KhQYI7utB9FWnx0p31nLWmt7xy5FIxFRERk2DPGkBofRWp8FLNze74ISlVDC6VVDVTUNlNR10xFbRMn6pz9E7XNVNQ1setoNSfqmqnsdkGUNp2DdFp8FBkJ0Z32nSDdsa8gHW4UjEVERERwepKTY/s3Ya8lEORkfbMTnGvbwnOTE6jPMEinxHWMOJ86Kt11hFpBenCNrGDcXA+/WglzPg75t0BUzzNjRURERE7H7/WQmRhDZmJMv87vHKRP1DZTHgrSJ+ra9p3Hdh+t4URdRa9B2nRu7YiPIjUuipQ4P8lxflJio0iO9ZMS5yclNnQszjkWH+XVRMN+GFnBuPYoWAtrvgGvfhcWfB4W3A7xGW5XJiIiIsPYmQbp1kCQk/UtVNQ1hUafu7Z2VISC9P7jtVQ1tFBZ30JzINjr6/k8xgnQoVHxlLiojvAcG0VyrM8J0W2hOnROUoxvRF3a21hr+z5rEBQUFNjCwsKhf2Nr4fDbsOF+2LMGfDEw95Ow5C5InzT09YiIiIicI2stjS1BKhua24NyZX0LVQ3NodsWKhtaqKpv6XJOVX1Lj5MOO0uM8XUaie4enjuOdb6fEucPqysXGmM2WmsL+jxvxAXjzsr3woafwJanIdAM0z8KS+6GcQvdrUtERERkiLQEglQ39BCe6537bcHaCdPNnc5pOeVqhZ1F+zynhOfv3JDX71HzgdTfYDyyWim6y5gCK++HFf8L3v0FvPcI7H4echfA0rth2tXgCZ/fdkREREQGmt/rIT0hmvSE6DN6nrWWuuaAE5a7hOeOYN35/pET9fg84d2WMbJHjLtrroNNT8JbD0DlIUibBIu/DPk3gz/W7epERERE5Cz0d8Q4vGP7UIuKh4W3w92b4GNPQEwyvPA1uG8WrPse1FW4XaGIiIiIDBIF4554vDDrBvj8K3DbGqe1Yt1/OwH5+a9BxX63KxQRERGRATaye4z7YgxMWOpsx/c4E/U2/RoKH4MZ18CSe2DsBW5XKSIiIiIDQCPG/TVqGlz3AHxlO1z0NfjwDXj0Mnj0Ctj9AgR7XztQRERERMKfgvGZSsyCS/8PfHUHXPl9qCmBp2+Gn14AhY9DS4PbFYqIiIjIWVAwPlvRCbDoDrhrE/zdYxCVAM9/Be7Lg9f+B+pPuF2hiIiIiJwBBeNz5fVB3k1w+zr49POQcz68+h24dya88A04ccDtCkVERESkHzT5bqAYAxMvcrayXc5ayO//EgofhRnXOlfUy+1z+TwRERERcYlGjAdD5gy47qfwlW2w9B7Yvw4euRQeuwp2r9FEPREREZEwpGA8mBJHw2Xfgq/tgCv+G6qOwNOfhJ8ugI1PQEujywWKiIiISBsF46EQnQiLvwR3b4abHnUuL/3cPfCj2fD6DzRRT0RERCQMKBgPJa8PZv8dfOF1uHU1jJkLr/yXc0W9Nf8EJw+6XaGIiIjIiKXJd24wBs672NmO7XQm6hU+Bu89DDOvgyV3OatbiIiIiMiQ0Yix27JmwvUPwle2OitX7HsFHl4Bj38U9rykiXoiIiIiQ8RYa11544KCAltYWOjKe4e1xmrY9Gt460GoLoLUic46yXk3QuZMZ7RZRERERPrNGLPRWtvnurkKxuEq0AI7noXNv4EPXwcbhIypMOtGJySPmuZ2hSIiIiIRQcF4OKk9DrtWw45VcHA9YJ3R41k3wqwbIGOy2xWKiIiIhC0F4+Gq5hjs/DPseAYOv+UcGz3bCcizboS0ie7WJyIiIhJmFIxHguoSp91ixyooetc5lj0vFJJvgJRx7tYnIiIiEgYUjEeaysPOSPL2Z6DkfedYToHTjzzzekjOcbc+EREREZcoGI9kJz6Enc86IfnoVufY2EWhkHydc6lqERERkRFCwVgc5ftg5yrYvgrKdgAGxi+FvBtgxnWQMMrtCkVEREQGlYKxnOr4HqcfefszUL4HjAcmXOT0I89YCfHpblcoIiIiMuAUjKV31kLZLmdli+3PwIn9YLxw3vJQSL4GYlPdrlJERERkQCgYS/9YC0e3OSPJO56BkwfB44dJlzjLv02/GmKS3a5SRERE5KwpGMuZsxZKNoVC8rNQdRi8UTD5MickT7sSohPdrlJERETkjPQ3GPuGohiJEMZAznxnu/zbULzRabXYsQr2rAFfDEy53Gm3mHolRMW7XbGIiIjIgNGIsfQtGHQuILL9GWcZuNpj4I+DqVc4IXnKR8Af63aVIiIiIj1SK4UMjmDAuRT19mecC4rUl0NUgjOCnHcjTFymdgsREREJKwrGMvgCrXBovdNqsXM1NJxwloAbPQfGL3G2cYshPsPtSkVERGQEUzCWoRVogUMb4NCbzm3Re9Da6DyWMTUUkkNhOWWsu7WKiIjIiKLJdzK0vH4472JnA2htdla4OLzBCcrbV8HGJ5zHksd2jCaPXwoZU5yJfyIiIiIu0oixDI1gAI7tcPqTD70Jh96CujLnsbgMGL+4Y0R59GzweN2tV0RERIaNAR0xNsZcCfwY8AKPWGu/1+3x24AfAMWhQw9Yax85o4plePN4YcwcZ1v4BWfN5BMHOkLyoTdh13POuVGJMHZBR59y9nzwx7hbv4iIiAx7fQZjY4wX+ClwOVAEvGeMWW2t3dnt1N9Za+8chBplODIG0ic52/xbnWNVxaER5VD7xSv/6Rz3RkPO+aGgvBjGLtTKFyIiIjLg+jNivADYZ609AGCMeRq4DugejEXOTXIOzP47ZwOoP9E1KK+/D974YaeVL5aGWjC08oWIiIicu/4E4xzgSKf7RcDCHs67yRizDPgA+Kq19kj3E4wxtwO3A4wbN+7Mq5WRJS4Npn/U2QCaap0LjRwKheXCR+HtnzqPZUxzQvL4pU5Q1soXIiIicob6E4x7Wi6g+4y954CnrLVNxpg7gF8CK055krW/AH4BzuS7M6xVRrroBJi0wtkAWpuclS8Obei46Ej7yhfjQkE5tEycVr4QERGRPvQnGBcBnYffcoGSzidYays63X0Y+P65lybSB180jFvkbNCx8sWhDc4ycftfga2/cx5rW/mibURZK1+IiIhIN/0Jxu8BU4wxE3FWnfgEcHPnE4wxY6y1paG7K4FdA1qlSH90Xvli0R3OyhcV+50VLw53W/kiOsnpU04ZCynjnLWVU8Y6t8m5TugWERGREaXPYGytbTXG3Am8jLNc22PW2h3GmG8Dhdba1cDdxpiVQCtwArhtEGsW6R9jIGOys53/aedYVZHTo3x4AxzbCR++DtUldO0OMpA4umtYThnXNUBHxbvxiURERGQQ6QIfIq3NUF0MVUeg8kjHbeUhZ7+qGIItXZ8TmxYKy2OdfuYuAXosxKSop1lERCRM6JLQIv3li4K0ic7Wk2AAao+FwvJhqDrcEaCP74G9f4PWhq7PiUrsuU2jbeQ5fpSCs4iISJhRMBbpi8cLSdnONq6HlQqthfqKUGgOhefOI8+H34LGqq7P8cU4vcydR5k7jzwnZWtyoIiIyBBTMBY5V8Y4FxiJz4Cc+T2f01jVNSxXHe4I0HvWQN3xrud7fE44bgvLbSPNKeMgZTwk5YBX//uKiIgMJP1kFRkKMckwOhlG5/X8eHO9MzGwqttoc+VhZ4JgTSnYYMf5xutcKTBlvLOlhm5Txjn7CaPB4xmazyYiIjJMKBiLhIOoOBg11dl60toM1UVOUD55KDTaHLrd9zeoPdr1fG+U05KROr5jlDllHKROcPbjM9TjLCIi0o2CsUgk8EVB2nnO1pOWBmfE+eShUGA+1BGgS7c4PdCd+eO6tmZ0D9CxqQrOIiIy4igYiwwH/ljnstcZU3p+vKk2NMp8uFNoDm1H3jl1cmB0UtfWjO4BOjpx8D+TiIjIEFMwFhkJohMga6az9aShsqM1o3OrxskP4cA6aKnren7bOs5depsndIxC+2MH+xOJiIgMOAVjEYHYFGcbM/fUx9qXozt0an/zsZ2w5yUINHV9Tly6MwEwIRMSsjrdZnU9ppYNEREJIwrGInJ6XZajO//Ux4NBqCvrFJoPOpfZri1zLoxyeD/UHDs1PIMzSbB7WO4tSGsUWkREBpmCsYicG48HEkc7W08XQAFn1Lmp2gnLNUedwNwWnNtuK49AUWFoTeceLlUfndy/AB2foYujiIjIWVEwFpHBZ4yzlnNMcu8TBNsEWqG+vFt47hakj251RqGba3p4Lw/EZUBiVs/tG52PRSeplUNERNopGItIePH6Okag+9JcFwrMPQXo0Fa2y7kNtp76fF9MqNc5DeLSnJ7n2NBtXFq3/VRni0nWiLSIyDClYCwikSsqHtImOtvpBIPQWNlLeC6D+hPQcAJOHnT2G6vosZ0DABOarNiPEN35vkanRUTCnoKxiAx/Ho8TUOPSIHNG3+cHA044bjjZEZrb9092vV93HMr3QP3Jnls72hhvt+Cc1nuI7hyy/XEK1CIiQ0TBWESkO4+3I0inT+r/8wItoeDcS4juvF9V5PRKN5yElvreX9Mb3TU0xyQ7F1iJSnBuoxMgKrFjPzoxdD+h63lq/xAR6ZOCsYjIQPH6QxP8Ms/seS2NHSG6x0DdKWxXHnZW+GiqhaYaCLb07z38cZ2CcoLT2tElXIeOdQ/UPYVwr350iMjwpO9uIiJu88eAfwwkjTnz57Y2hUJyNTTXdgTm5hrntqk2dLymY2s7r6qo63k9rTXdE19stwCd1ClcJ3YK3vHg8TkrhXh8zqi18Xa69YRufT0c83Z7zNP1+e2v2/01vT0fa3ttEZHTUDAWEYlkvmhni08/99dqbe4aotuDdnWn47Vdw3TbedUlnZ5TA62N517PYPD4eg/n/phQyE/qGC2PadsPHe9yv+2cUHuLL9rtTyci50jBWEREHL4o8IV6q89VoMVZTi8YABtwlstr3w+ADfbwWLDbOW2PBbsdC/RwXttt0HlO92P9qaGlMRT4q6H2KJR/0HE/0Nz3Z/ZG9xymTznWKUx3D+FqVRFxlf7vExGRgef1O8vaDRetTU5IbqzqCMtNNdAYum2q6hhpbz9WDZWHQvdDmw32/V7++J7DdOcR66iE0IRK46xaYjyd9k1o33PqfpfzzuU5dNrv4Tndn+/xOr84+KKdS8F3uY1Wm4uEDQVjERGRvrS3rGSc/WtY66xA0lOg7hym2247H6sp7bh/umUBI5XHFwrOUd1uewrSnQJ1v84/0/OiwONXWB+hFIxFRESGgjHOhMSo+P5d2bE3wSC01DmjzzboBG7o2LdBwJ5m/2yfQ7fn2348P3Q/2OpM7mxtDt02Oe0pPd52Pq/T+a2Nzoh9oDl0bg+vZQNn/3U9hQkFdn/HJFCPLxSaO933+js91svmPc1jba/V/j5tr+0/zeOdNtr+u3T72nf+b3DKf7fT/Dcb7Odd+FWnlShMKRiLiIhEEo/HaaeQUwUDpw/X/Q3hNuj0yQdbO3rTg93u9/V4c10/n9/W/x663592m4jSrcXmgs8pGIuIiIgMOo8XouKAOLcrOXttk0W7BO9uW6C1I0wDp/Z399Qbzul7wQf6eRF6xU4FYxEREZFw4fEAHqd1QoacOstFRERERFAwFhEREREBFIxFRERERAAFYxERERERQMFYRERERARQMBYRERERARSMRUREREQABWMREREREUDBWEREREQEUDAWEREREQHAWGvdeWNjjgOHXHlzyADKXXrvMxVJtUJk1RtJtUJk1RtJtUJk1ataB08k1RtJtUJk1RtJtUJk1etmreOttaP6Osm1YOwmY0yhtbbA7Tr6I5JqhciqN5JqhciqN5JqhciqV7UOnkiqN5JqhciqN5JqhciqNxJqVSuFiIiIiAgKxiIiIiIiwMgNxr9wu4AzEEm1QmTVG0m1QmTVG0m1QmTVq1oHTyTVG0m1QmTVG0m1QmTVG/a1jsgeYxERERGR7kbqiLGIiIiISBcKxiIiIiIijLBgbIx5zBhTZozZ7nYtfTHGjDXGvGqM2WWM2WGMucftmnpjjIkxxrxrjNkSqvX/ul1TX4wxXmPMJmPM827X0hdjzEFjzDZjzGZjTKHb9fTFGJNijPmjMWZ36N/vYrdr6okxZlroa9q2VRtjvuJ2XadjjPlq6P+x7caYp4wxMW7X1BtjzD2hOneE49e1p58Hxpg0Y8xfjTF7Q7epbtbYppdaPxb62gaNMWG1/FUv9f4g9D1hqzFmlTEmxc0a2/RS63+G6txsjPmLMSbbzRo7O12OMcZ8wxhjjTEZbtTWXS9f228ZY4o7fd+92s0aezKigjHwBHCl20X0UyvwdWvtDGAR8GVjzEyXa+pNE7DCWjsXyAeuNMYscrmmvtwD7HK7iDNwibU2P9zXfwz5MfCStXY6MJcw/Tpba/eEvqb5wPlAPbDK5bJ6ZYzJAe4GCqy1eYAX+IS7VfXMGJMHfB5YgPNv4BpjzBR3qzrFE5z68+BfgLXW2inA2tD9cPAEp9a6HbgReH3Iq+nbE5xa71+BPGvtHOAD4F+HuqhePMGptf7AWjsn9L3heeD/DHlVvXuCHnKMMWYscDlweKgLOo0n6Dlz3df2vddau2aIa+rTiArG1trXgRNu19Ef1tpSa+37of0anHCR425VPbOO2tBdf2gL21mdxphc4KPAI27XMtwYY5KAZcCjANbaZmttpbtV9culwH5rrVtX4+wvHxBrjPEBcUCJy/X0ZgbwtrW23lrbCrwG3OByTV308vPgOuCXof1fAtcPaVG96KlWa+0ua+0el0o6rV7q/Uvo3wLA20DukBfWg15qre50N54w+nl2mhxzH/BPREatYW1EBeNIZYyZAMwD3nG3kt6FWhM2A2XAX621YVsr8COcbyBBtwvpJwv8xRiz0Rhzu9vF9OE84DjweKhV5RFjTLzbRfXDJ4Cn3C7idKy1xcAPcUaESoEqa+1f3K2qV9uBZcaYdGNMHHA1MNblmvojy1pbCs7gBJDpcj3D1T8CL7pdxOkYY75jjDkC3EJ4jRifwhizEii21m5xu5Z+ujPUqvJYuLQrdaZgHOaMMQnAn4CvdPstNqxYawOhPzvlAgtCf0oNO8aYa4Aya+1Gt2s5A0uttfOBq3Baapa5XdBp+ID5wM+stfOAOsLnz9E9MsZEASuBP7hdy+mEfoBcB0wEsoF4Y8w/uFtVz6y1u4Dv4/z5/CVgC057mIxwxph/x/m38KTbtZyOtfbfrbVjceq80+16ehP6xfPfCfPw3snPgEk4bZelwP9zt5xTKRiHMWOMHycUP2mtfcbtevoj9GfzdYRvL/dSYKUx5iDwNLDCGPMbd0s6PWttSei2DKcHdoG7FZ1WEVDU6S8Gf8QJyuHsKuB9a+0xtwvpw2XAh9ba49baFuAZYInLNfXKWvuotXa+tXYZzp9T97pdUz8cM8aMAQjdlrlcz7BijPk0cA1wi42ciyj8FrjJ7SJOYxLOL8tbQj/XcoH3jTGjXa2qF9baY6GBtCDwMGH480zBOEwZYwxOn+Yua+29btdzOsaYUW0zjI0xsTg/wHe7W1XPrLX/aq3NtdZOwPnz+SvW2rAcdQMwxsQbYxLb9oGP4PyZOixZa48CR4wx00KHLgV2ulhSf3ySMG+jCDkMLDLGxIW+P1xKmE5sBDDGZIZux+FMEouEr/Fq4NOh/U8Df3axlmHFGHMl8M/ASmttvdv1nE63iaIrCdOfZwDW2m3W2kxr7YTQz7UiYH7oe3HYafvFM+QGwvDnmc/tAoaSMeYpYDmQYYwpAv7DWvuou1X1ainwKWBbqHcX4N/CcQYnMAb4pTHGi/PL1u+ttWG/DFqEyAJWOTkIH/Bba+1L7pbUp7uAJ0MtCgeAz7hcT69Cf4a8HPiC27X0xVr7jjHmj8D7OH+K3kR4X171T8aYdKAF+LK19qTbBXXW088D4HvA740xn8X5ReRj7lXYoZdaTwA/AUYBLxhjNltrr3Cvyg691PuvQDTw19D3s7ettXe4VmRIL7VeHfrlPggcAlyvs00k5ZhevrbLjTH5OHNnDhKG33t1SWgREREREdRKISIiIiICKBiLiIiIiAAKxiIiIiIigIKxiIiIiAigYCwiIiIiAigYi4i4whgTMMZs7rQN2BUCjTETjDFhtz6oiEi4G1HrGIuIhJGG0GXURUQkTGjEWEQkjBhjDhpjvm+MeTe0TQ4dH2+MWWuM2Rq6HRc6nmWMWWWM2RLa2i4T7TXGPGyM2WGM+UvoqpQiInIaCsYiIu6I7dZK8fFOj1VbaxcADwA/Ch17APiVtXYO8CRwf+j4/cBr1tq5wHxgR+j4FOCn1tpZQCVw0yB/HhGRiKcr34mIuMAYU2utTejh+EFghbX2gDHGDxy11qYbY8qBMdbaltDxUmtthjHmOJBrrW3q9BoTgL9aa6eE7v8z4LfW/tfgfzIRkcilEWMRkfBje9nv7ZyeNHXaD6A5JSIifVIwFhEJPx/vdPtWaH8D8InQ/i3A+tD+WuCLAMYYrzEmaaiKFBEZbjSCICLijlhjzOZO91+y1rYt2RZtjHkHZ/Dik6FjdwOPGWO+CRwHPhM6fg/wC2PMZ3FGhr8IlA569SIiw5B6jEVEwkiox7jAWlvudi0iIiONWilERERERNCIsYiIiIgIoBFjERERERFAwVhEREREBFAwFhEREREBFIxFRERERAAFYxERERERAP4/XrznLQxv3LUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.plot_training_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s.endsession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
