{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swat import *\n",
    "import swat as sw\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import sys\n",
    "\n",
    "# sys.path.append(r'\\\\sashq\\root\\dept\\cas\\weshiz\\github_vb025_dev\\python-dlpy')\n",
    "sys.path.append('C:\\\\Users\\\\weshiz\\\\Documents\\\\GitHub\\\\modify\\\\python-dlpy')\n",
    "sys.path.append('C:\\\\Users\\\\weshiz\\\\Documents\\\\GitHub\\\\python-fcmp')\n",
    "from dlpy.layers import * \n",
    "from dlpy.applications import *\n",
    "from dlpy import Model, Sequential\n",
    "from dlpy.utils import *\n",
    "from dlpy.splitting import two_way_split\n",
    "from dlpy.images import *\n",
    "from dlpy.model import *\n",
    "from src.parser import *\n",
    "from src.decorator import *\n",
    "from dlpy.lr_scheduler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sw.CAS('dlgrd009', 13300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path='/disk/linux/dlpy/Giraffe_Dolphin'\n",
    "img_path='/cas/DeepLearn/data/Giraffe_Dolphin'\n",
    "my_images = ImageTable.load_files(s, path=img_path)\n",
    "my_images.resize(112)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cyclic Learning Rate Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "factor = 2\n",
    "num_batch_per_epoch = math.ceil(s.numrows(my_images).numrows / batch_size)\n",
    "step_size = int(num_batch_per_epoch * factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batch_per_epoch, step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_lr(rate, iterNum, batch, initRate):\n",
    "    num_batch_per_epoch = 10\n",
    "    step_size = 10\n",
    "    max_lr = 0.01\n",
    "    batch_cum = num_batch_per_epoch * iterNum + batch\n",
    "    cycle = int(batch_cum / (2 * step_size) + 1)\n",
    "    x = abs(batch_cum / step_size - 2 * cycle + 1)\n",
    "    rate = initRate + (max_lr - initRate) * max(0, 1-x)\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('function cyclic_lr(rate, iterNum, batch, initRate);\\n'\n",
      " '    num_batch_per_epoch = 10;\\n'\n",
      " '    step_size = 10;\\n'\n",
      " '    max_lr = 0.01;\\n'\n",
      " '    batch_cum = (num_batch_per_epoch * iterNum + batch);\\n'\n",
      " '    cycle = int((batch_cum / 2 * step_size + 1));\\n'\n",
      " '    x = abs(((batch_cum / step_size - 2 * cycle) + 1));\\n'\n",
      " '    rate = (initRate + (max_lr - initRate) * max(0, (1 - x)));\\n'\n",
      " '    return (rate);\\n'\n",
      " 'endsub;\\n')\n"
     ]
    }
   ],
   "source": [
    "fcmp_code = python_to_fcmp(func=cyclic_lr, print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services saved the file CYCLIC_LR.sashdat in caslib CASUSER(weshiz).\n"
     ]
    }
   ],
   "source": [
    "register_fcmp_routines(conn=s, routine_code=fcmp_code, function_tbl_name='cyclic_lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following argument(s) learning_rate, gamma, step_size, power are overwritten by the according arguments specified in lr_scheduler.\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = FCMPLR(conn=s, fcmp_learning_rate='cyclic_lr', learning_rate=0.0001)\n",
    "solver = MomentumSolver(lr_scheduler=lr_scheduler,\n",
    "                        clip_grad_max=100, clip_grad_min=-100)\n",
    "optimizer = Optimizer(algorithm=solver, mini_batch_size=16, log_level=3, max_epochs=50, reg_l2=0.0005)\n",
    "gpu = Gpu(devices=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filters, size, stride=1, mode='same', act=True):\n",
    "    x = Conv2d(filters, size, size, act='identity', include_bias=False, stride=stride)(x)\n",
    "    x = BN(act='relu' if act else 'identity')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_block(ip, nf=64):\n",
    "    x = conv_block(ip, nf, 3, 2)\n",
    "    x = conv_block(x, nf, 3, 1, act=False)\n",
    "    return Res()([x, ip])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Model compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "inp_resnet= Input(3, 112, 112, scale = 1.0 / 255, name='InputLayer_1')\n",
    "x=conv_block(inp_resnet, 64, 9, 1)\n",
    "for i in range(4): x=res_block(x)\n",
    "x=Conv2d(20, 9, 9, act='tanh')(x)\n",
    "x=Pooling(7, 7)(x)\n",
    "output = OutputLayer(n=2)(x)\n",
    "resnet_like_model = Model(s, inputs = inp_resnet, outputs = output)\n",
    "resnet_like_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Inputs=_image_ is used\n",
      "NOTE: Training from scratch.\n",
      "NOTE: Using dlgrd009.unx.sas.com: 1 out of 4 available GPU devices.\n",
      "NOTE:  Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 415358.\n",
      "NOTE:  The approximate memory cost is 115.00 MB.\n",
      "NOTE:  Loading weights cost       0.00 (s).\n",
      "NOTE:  Initializing each layer cost       2.17 (s).\n",
      "NOTE:  The total number of threads on each worker is 4.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 16.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 64.\n",
      "NOTE:  Target variable: _label_\n",
      "NOTE:  Number of levels for the target variable:      2\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: Dolphin\n",
      "NOTE:  Level      1: Giraffe\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of numeric input variables:      1\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.8488        0.5     0.2999     0.54\n",
      "NOTE:      1    64   0.0001           0.8329     0.4844     0.2999     0.03\n",
      "NOTE:      2    64   0.0001           0.8676     0.5156     0.2999     0.03\n",
      "NOTE:      3    64   0.0001           0.8468        0.5     0.2999     0.03\n",
      "NOTE:      4    64   0.0001           0.7764     0.4375     0.2999     0.03\n",
      "NOTE:      5    64   0.0001           0.7924     0.4531     0.2999     0.03\n",
      "NOTE:      6    64   0.0001           0.6342     0.3125     0.2999     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  0        0.0001          0.7999     0.4576     0.70\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.7568     0.4219     0.2999     0.03\n",
      "NOTE:      1    64   0.0001           0.5611       0.25     0.2999     0.02\n",
      "NOTE:      2    64   0.0001           0.7529     0.4219     0.2999     0.02\n",
      "NOTE:      3    64   0.0001           0.6824     0.3594     0.2999     0.02\n",
      "NOTE:      4    64   0.0001            0.854     0.5156     0.2999     0.02\n",
      "NOTE:      5    64   0.0001           0.7817     0.4531     0.2999     0.02\n",
      "NOTE:      6    64   0.0001           0.8021     0.4688     0.2999     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  1        0.0001          0.7416     0.4129     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.8008     0.4688     0.2999     0.03\n",
      "NOTE:      1    64   0.0001           0.6276     0.3125     0.2999     0.02\n",
      "NOTE:      2    64   0.0001           0.6965      0.375     0.2999     0.02\n",
      "NOTE:      3    64   0.0001           0.7966     0.4688     0.2999     0.02\n",
      "NOTE:      4    64   0.0001           0.7598     0.4375     0.2999     0.02\n",
      "NOTE:      5    64   0.0001           0.7267     0.4063     0.2999     0.02\n",
      "NOTE:      6    64   0.0001           0.7602     0.4375     0.2999     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  2        0.0001          0.7383     0.4152     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.8389     0.5156     0.2999     0.03\n",
      "NOTE:      1    64   0.0001           0.7873     0.4688     0.2999     0.02\n",
      "NOTE:      2    64   0.0001           0.7892     0.4688     0.2999     0.02\n",
      "NOTE:      3    64   0.0001           0.7029     0.3906     0.2999     0.02\n",
      "NOTE:      4    64   0.0001           0.7204     0.4063     0.2999     0.02\n",
      "NOTE:      5    64   0.0001           0.7345     0.4219     0.2999     0.02\n",
      "NOTE:      6    64   0.0001           0.7027     0.3906     0.2999     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  3        0.0001          0.7537     0.4375     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.7525     0.4375     0.2999     0.03\n",
      "NOTE:      1    64   0.0001           0.7019     0.3906     0.2999     0.02\n",
      "NOTE:      2    64   0.0001           0.7947     0.4844     0.2999     0.02\n",
      "NOTE:      3    64   0.0001           0.6358     0.3281     0.2999     0.02\n",
      "NOTE:      4    64   0.0001           0.6402     0.3281     0.2999     0.02\n",
      "NOTE:      5    64   0.0001           0.7577     0.4531     0.2999     0.02\n",
      "NOTE:      6    64   0.0001           0.7266     0.4219     0.2999     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  4        0.0001          0.7156     0.4063     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.8457     0.5469     0.2999     0.03\n",
      "NOTE:      1    64   0.0001           0.6645     0.3594     0.2999     0.02\n",
      "NOTE:      2    64   0.0001           0.7352     0.4375     0.2999     0.02\n",
      "NOTE:      3    64   0.0001           0.8395     0.5469     0.2999     0.02\n",
      "NOTE:      4    64   0.0001           0.6802      0.375     0.2999     0.02\n",
      "NOTE:      5    64   0.0001            0.646     0.3438     0.2999     0.02\n",
      "NOTE:      6    64   0.0001           0.6912     0.3906     0.2999     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  5        0.0001          0.7289     0.4286     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.7197     0.4219     0.2999     0.03\n",
      "NOTE:      1    64   0.0001            0.789        0.5     0.2999     0.02\n",
      "NOTE:      2    64   0.0001           0.6898     0.3906     0.2999     0.02\n",
      "NOTE:      3    64   0.0001           0.8862     0.6094     0.2999     0.02\n",
      "NOTE:      4    64   0.0001           0.7008     0.4063     0.2999     0.02\n",
      "NOTE:      5    64   0.0001             0.78        0.5     0.2999     0.02\n",
      "NOTE:      6    64   0.0001            0.755     0.4688     0.2999     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  6        0.0001          0.7601      0.471     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.7256     0.4375     0.2999     0.03\n",
      "NOTE:      1    64   0.0001           0.6559     0.3594     0.2999     0.02\n",
      "NOTE:      2    64   0.0001            0.603     0.2969     0.2998     0.02\n",
      "NOTE:      3    64   0.0001           0.7073     0.4219     0.2998     0.02\n",
      "NOTE:      4    64   0.0001           0.6692      0.375     0.2998     0.02\n",
      "NOTE:      5    64   0.0001           0.7341     0.4531     0.2998     0.02\n",
      "NOTE:      6    64   0.0001           0.6568     0.3594     0.2998     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  7        0.0001          0.6788     0.3862     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001            0.618     0.3125     0.2998     0.03\n",
      "NOTE:      1    64   0.0001           0.6537     0.3594     0.2998     0.02\n",
      "NOTE:      2    64   0.0001           0.7068     0.4219     0.2998     0.02\n",
      "NOTE:      3    64   0.0001           0.7524     0.4844     0.2998     0.02\n",
      "NOTE:      4    64   0.0001            0.806     0.5469     0.2998     0.02\n",
      "NOTE:      5    64   0.0001           0.6776     0.3906     0.2998     0.02\n",
      "NOTE:      6    64   0.0001           0.7639        0.5     0.2998     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  8        0.0001          0.7112     0.4308     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.6897     0.4063     0.2998     0.03\n",
      "NOTE:      1    64   0.0001           0.7366     0.4688     0.2998     0.02\n",
      "NOTE:      2    64   0.0001           0.7377     0.4688     0.2998     0.02\n",
      "NOTE:      3    64   0.0001           0.6987     0.4219     0.2998     0.02\n",
      "NOTE:      4    64   0.0001           0.6647      0.375     0.2998     0.02\n",
      "NOTE:      5    64   0.0001            0.722     0.4531     0.2998     0.02\n",
      "NOTE:      6    64   0.0001           0.7107     0.4375     0.2998     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  9        0.0001          0.7086      0.433     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.7318     0.4688     0.2998     0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      1    64   0.0001           0.6359     0.3438     0.2998     0.02\n",
      "NOTE:      2    64   0.0001           0.6733     0.3906     0.2998     0.02\n",
      "NOTE:      3    64   0.0001           0.6614      0.375     0.2998     0.02\n",
      "NOTE:      4    64   0.0001           0.6829     0.4063     0.2998     0.02\n",
      "NOTE:      5    64   0.0001           0.6844     0.4063     0.2998     0.02\n",
      "NOTE:      6    64   0.0001           0.6134     0.3125     0.2998     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  10       0.0001           0.669     0.3862     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.6912     0.4219     0.2998     0.03\n",
      "NOTE:      1    64   0.0001           0.6904     0.4219     0.2998     0.02\n",
      "NOTE:      2    64   0.0001           0.7589     0.5156     0.2998     0.02\n",
      "NOTE:      3    64   0.0001           0.7016     0.4375     0.2998     0.02\n",
      "NOTE:      4    64   0.0001           0.6752     0.4063     0.2998     0.02\n",
      "NOTE:      5    64   0.0001           0.6547      0.375     0.2998     0.02\n",
      "NOTE:      6    64   0.0001           0.7222     0.4688     0.2998     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  11       0.0001          0.6992     0.4353     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.6716     0.3906     0.2998     0.03\n",
      "NOTE:      1    64   0.0001           0.7208     0.4688     0.2998     0.02\n",
      "NOTE:      2    64   0.0001           0.7304     0.4844     0.2998     0.02\n",
      "NOTE:      3    64   0.0001           0.6996     0.4375     0.2998     0.02\n",
      "NOTE:      4    64   0.0001           0.6571      0.375     0.2998     0.02\n",
      "NOTE:      5    64   0.0001           0.6881     0.4219     0.2998     0.02\n",
      "NOTE:      6    64   0.0001           0.6642     0.3906     0.2998     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  12       0.0001          0.6903     0.4241     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.6164     0.3125     0.2998     0.03\n",
      "NOTE:      1    64   0.0001           0.6649     0.3906     0.2998     0.02\n",
      "NOTE:      2    64   0.0001            0.702     0.4531     0.2998     0.02\n",
      "NOTE:      3    64   0.0001           0.6557     0.3906     0.2998     0.02\n",
      "NOTE:      4    64   0.0001           0.6745     0.4063     0.2998     0.02\n",
      "NOTE:      5    64   0.0001           0.6654     0.3906     0.2998     0.02\n",
      "NOTE:      6    64   0.0001           0.6512      0.375     0.2998     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  13       0.0001          0.6614     0.3884     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.6775     0.4219     0.2998     0.03\n",
      "NOTE:      1    64   0.0001           0.7498     0.5313     0.2998     0.02\n",
      "NOTE:      2    64   0.0001           0.6385     0.3594     0.2998     0.02\n",
      "NOTE:      3    64   0.0001           0.6682     0.4063     0.2998     0.02\n",
      "NOTE:      4    64   0.0001           0.6396     0.3594     0.2998     0.02\n",
      "NOTE:      5    64   0.0001           0.6487      0.375     0.2998     0.02\n",
      "NOTE:      6    64   0.0001           0.6899     0.4375     0.2998     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  14       0.0001          0.6732     0.4129     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.7063     0.4688     0.2998     0.03\n",
      "NOTE:      1    64   0.0001           0.7458     0.5313     0.2998     0.02\n",
      "NOTE:      2    64   0.0001           0.6535      0.375     0.2998     0.02\n",
      "NOTE:      3    64   0.0001           0.7198     0.4844     0.2998     0.02\n",
      "NOTE:      4    64   0.0001           0.7444     0.5313     0.2998     0.02\n",
      "NOTE:      5    64   0.0001           0.6643     0.4063     0.2998     0.02\n",
      "NOTE:      6    64   0.0001           0.7219        0.5     0.2998     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  15       0.0001           0.708      0.471     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.6607     0.3906     0.2998     0.03\n",
      "NOTE:      1    64   0.0001           0.7269     0.5156     0.2998     0.02\n",
      "NOTE:      2    64   0.0001            0.704     0.4688     0.2998     0.02\n",
      "NOTE:      3    64   0.0001           0.6334     0.3594     0.2998     0.02\n",
      "NOTE:      4    64   0.0001           0.7288     0.5156     0.2998     0.02\n",
      "NOTE:      5    64   0.0001           0.7163        0.5     0.2998     0.02\n",
      "NOTE:      6    64   0.0001           0.6834     0.4375     0.2998     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  16       0.0001          0.6934     0.4554     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.6968     0.4844     0.2998     0.03\n",
      "NOTE:      1    64   0.0001           0.7053     0.4844     0.2998     0.02\n",
      "NOTE:      2    64   0.0001           0.6281     0.3438     0.2998     0.02\n",
      "NOTE:      3    64   0.0001           0.6726     0.4219     0.2998     0.02\n",
      "NOTE:      4    64   0.0001           0.6868     0.4531     0.2998     0.02\n",
      "NOTE:      5    64   0.0001            0.693     0.4688     0.2998     0.02\n",
      "NOTE:      6    64   0.0001           0.7165     0.5156     0.2998     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  17       0.0001          0.6856     0.4531     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.5871     0.2813     0.2998     0.03\n",
      "NOTE:      1    64   0.0001           0.6677     0.4219     0.2998     0.02\n",
      "NOTE:      2    64   0.0001           0.7363     0.5469     0.2998     0.02\n",
      "NOTE:      3    64   0.0001           0.6265     0.3281     0.2998     0.02\n",
      "NOTE:      4    64   0.0001           0.6849     0.4531     0.2998     0.02\n",
      "NOTE:      5    64   0.0001            0.635     0.3594     0.2998     0.02\n",
      "NOTE:      6    64   0.0001           0.6403     0.3906     0.2998     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  18       0.0001           0.654     0.3973     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.6715     0.4219     0.2998     0.03\n",
      "NOTE:      1    64   0.0001           0.7093        0.5     0.2998     0.02\n",
      "NOTE:      2    64   0.0001           0.6582     0.4063     0.2998     0.02\n",
      "NOTE:      3    64   0.0001            0.708        0.5     0.2998     0.02\n",
      "NOTE:      4    64   0.0001           0.6463     0.3906     0.2998     0.02\n",
      "NOTE:      5    64   0.0001           0.6468     0.3906     0.2998     0.02\n",
      "NOTE:      6    64   0.0001           0.6746     0.4531     0.2998     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  19       0.0001          0.6735     0.4375     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.6986        0.5     0.2998     0.03\n",
      "NOTE:      1    64   0.0001            0.645     0.3906     0.2998     0.02\n",
      "NOTE:      2    64   0.0001           0.6936     0.4844     0.2998     0.02\n",
      "NOTE:      3    64   0.0001           0.6448      0.375     0.2998     0.02\n",
      "NOTE:      4    64   0.0001           0.6656     0.4531     0.2998     0.02\n",
      "NOTE:      5    64   0.0001           0.6392      0.375     0.2998     0.02\n",
      "NOTE:      6    64   0.0001           0.6592     0.4219     0.2998     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  20       0.0001          0.6637     0.4286     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.6921        0.5     0.2998     0.03\n",
      "NOTE:      1    64   0.0001           0.6869     0.4844     0.2998     0.02\n",
      "NOTE:      2    64   0.0001           0.6601     0.4063     0.2998     0.02\n",
      "NOTE:      3    64   0.0001           0.7036     0.5156     0.2998     0.02\n",
      "NOTE:      4    64   0.0001           0.6224     0.3438     0.2998     0.02\n",
      "NOTE:      5    64   0.0001           0.6751     0.4688     0.2998     0.02\n",
      "NOTE:      6    64   0.0001            0.649     0.4063     0.2998     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  21       0.0001          0.6699     0.4464     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.6609     0.4375     0.2998     0.03\n",
      "NOTE:      1    64   0.0001           0.6201     0.3281     0.2998     0.02\n",
      "NOTE:      2    64   0.0001           0.6417     0.4063     0.2998     0.02\n",
      "NOTE:      3    64   0.0001           0.6989     0.5313     0.2998     0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      4    64   0.0001           0.6818        0.5     0.2998     0.02\n",
      "NOTE:      5    64   0.0001           0.6573     0.4531     0.2998     0.02\n",
      "NOTE:      6    64   0.0001             0.63     0.3906     0.2998     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  22       0.0001          0.6558     0.4353     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.6839     0.5156     0.2998     0.03\n",
      "NOTE:      1    64   0.0001           0.6289     0.3906     0.2998     0.02\n",
      "NOTE:      2    64   0.0001           0.6693     0.4688     0.2998     0.02\n",
      "NOTE:      3    64   0.0001           0.6965     0.5156     0.2998     0.02\n",
      "NOTE:      4    64   0.0001           0.6743     0.4688     0.2998     0.02\n",
      "NOTE:      5    64   0.0001           0.6594     0.4531     0.2998     0.02\n",
      "NOTE:      6    64   0.0001           0.6336     0.4063     0.2998     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  23       0.0001          0.6637     0.4598     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.6492     0.4531     0.2998     0.03\n",
      "NOTE:      1    64   0.0001           0.6283     0.3906     0.2998     0.02\n",
      "NOTE:      2    64   0.0001            0.626     0.3906     0.2998     0.02\n",
      "NOTE:      3    64   0.0001           0.6339     0.4219     0.2998     0.02\n",
      "NOTE:      4    64   0.0001           0.6006     0.3281     0.2998     0.02\n",
      "NOTE:      5    64   0.0001           0.6192      0.375     0.2998     0.02\n",
      "NOTE:      6    64   0.0001           0.6788        0.5     0.2998     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  24       0.0001          0.6337     0.4085     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.6906     0.5469     0.2998     0.03\n",
      "NOTE:      1    64   0.0001           0.6421     0.4375     0.2998     0.02\n",
      "NOTE:      2    64   0.0001            0.682     0.5313     0.2998     0.02\n",
      "NOTE:      3    64   0.0001           0.6256      0.375     0.2998     0.02\n",
      "NOTE:      4    64   0.0001           0.6461     0.4219     0.2998     0.02\n",
      "NOTE:      5    64   0.0001           0.6715        0.5     0.2998     0.02\n",
      "NOTE:      6    64   0.0001            0.632     0.4063     0.2998     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  25       0.0001          0.6557     0.4598     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.6378     0.4375     0.2998     0.03\n",
      "NOTE:      1    64   0.0001           0.6228     0.3906     0.2998     0.02\n",
      "NOTE:      2    64   0.0001            0.623     0.4219     0.2998     0.02\n",
      "NOTE:      3    64   0.0001            0.639     0.4063     0.2998     0.02\n",
      "NOTE:      4    64   0.0001           0.5973      0.375     0.2998     0.02\n",
      "NOTE:      5    64   0.0001           0.6104     0.3594     0.2998     0.02\n",
      "NOTE:      6    64   0.0001           0.5924     0.3438     0.2998     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  26       0.0001          0.6175     0.3906     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.6809     0.5938     0.2998     0.03\n",
      "NOTE:      1    64   0.0001           0.6377     0.4688     0.2998     0.02\n",
      "NOTE:      2    64   0.0001           0.6427     0.4375     0.2998     0.02\n",
      "NOTE:      3    64   0.0001           0.6239      0.375     0.2998     0.02\n",
      "NOTE:      4    64   0.0001           0.6106     0.4063     0.2998     0.02\n",
      "NOTE:      5    64   0.0001           0.6193     0.3906     0.2998     0.02\n",
      "NOTE:      6    64   0.0001           0.5929     0.3281     0.2998     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  27       0.0001          0.6297     0.4286     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.6362     0.4531     0.2998     0.03\n",
      "NOTE:      1    64   0.0001           0.6134      0.375     0.2998     0.02\n",
      "NOTE:      2    64   0.0001            0.619     0.4375     0.2998     0.02\n",
      "NOTE:      3    64   0.0001           0.6276     0.4375     0.2998     0.02\n",
      "NOTE:      4    64   0.0001           0.6194      0.375     0.2998     0.02\n",
      "NOTE:      5    64   0.0001           0.6016      0.375     0.2998     0.02\n",
      "NOTE:      6    64   0.0001           0.6546     0.5156     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  28       0.0001          0.6245     0.4241     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.6491     0.5156     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.5768     0.2813     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.5814     0.3594     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.6264     0.4531     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.5708     0.3125     0.2997     0.02\n",
      "NOTE:      5    64   0.0001            0.597     0.3594     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.6043     0.4219     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  29       0.0001          0.6008     0.3862     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.5751     0.3125     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.5915     0.3906     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.6289     0.4844     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.5931     0.3594     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.5726     0.3281     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.5536     0.2813     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.6224     0.4688     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  30       0.0001           0.591      0.375     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.5844     0.3438     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.5837     0.3438     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.5715     0.2813     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.5864     0.4219     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.5755     0.3281     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.6025     0.4688     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.5758      0.375     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  31       0.0001          0.5828     0.3661     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.6028     0.4375     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.5619     0.3281     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.5771     0.3438     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.5699     0.3906     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.5586     0.3594     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.5227     0.2188     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.6034        0.5     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  32       0.0001          0.5709     0.3683     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.5409     0.3125     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.5377       0.25     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.5339     0.2813     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.5891     0.3906     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.5798     0.4219     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.5752     0.4063     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.5225     0.2969     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  33       0.0001          0.5541     0.3371     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.5364     0.2656     0.2997     0.03\n",
      "NOTE:      1    64   0.0001             0.53     0.3281     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.5797     0.4219     0.2997     0.02\n",
      "NOTE:      3    64   0.0001            0.533     0.2344     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.5413     0.3281     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.5134     0.2813     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.5198     0.3281     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  34       0.0001          0.5362     0.3125     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.5887     0.4531     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.5188     0.2969     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.5373     0.4063     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.5452     0.2813     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.5594     0.3594     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.5171     0.2969     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.4995     0.2344     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  35       0.0001           0.538     0.3326     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.5401     0.3438     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.5359     0.3281     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.5074     0.2344     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.5125     0.2656     0.2997     0.02\n",
      "NOTE:      4    64   0.0001            0.507     0.2656     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.5147     0.2344     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.5217     0.2969     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  36       0.0001          0.5199     0.2813     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.5046     0.2344     0.2997     0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      1    64   0.0001           0.4976       0.25     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.4788     0.1875     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.4897     0.2031     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.5567     0.3438     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.4893     0.1719     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.4751      0.125     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  37       0.0001          0.4988     0.2165     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.4702     0.1719     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.4984     0.2188     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.4909     0.2344     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.4605     0.1094     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.4773     0.1563     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.4395     0.1094     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.4806     0.1406     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  38       0.0001          0.4739     0.1629     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001             0.47     0.2344     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.5153     0.2656     0.2997     0.02\n",
      "NOTE:      2    64   0.0001            0.434     0.1094     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.4444     0.1563     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.4426     0.1563     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.4375     0.1094     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.4355      0.125     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  39       0.0001          0.4542     0.1652     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.4463    0.09375     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.4495     0.1406     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.4475     0.1563     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.4225    0.07813     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.3985    0.07813     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.3998     0.1094     0.2997     0.02\n",
      "NOTE:      6    64   0.0001            0.462     0.1719     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  40       0.0001          0.4323     0.1183     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.4249     0.1563     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.4617      0.125     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.4319     0.1563     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.4153    0.07813     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.4136     0.1563     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.4009      0.125     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.4258     0.1563     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  41       0.0001          0.4249     0.1362     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.3917      0.125     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.4106     0.1406     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.4416     0.1406     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.4079     0.0625     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.3677    0.01563     0.2997     0.02\n",
      "NOTE:      5    64   0.0001            0.398    0.07813     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.3923     0.0625     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  42       0.0001          0.4014    0.08929     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.3932      0.125     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.4248     0.1406     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.4042    0.07813     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.3809      0.125     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.3727    0.07813     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.4241    0.09375     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.4221     0.1719     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  43       0.0001          0.4031     0.1161     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.3804    0.04688     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.3938     0.1406     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.3903      0.125     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.3641    0.03125     0.2997     0.02\n",
      "NOTE:      4    64   0.0001            0.445     0.1563     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.3748    0.04688     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.3792     0.1094     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  44       0.0001          0.3897    0.09375     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.3882     0.0625     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.3812    0.09375     0.2997     0.02\n",
      "NOTE:      2    64   0.0001            0.349     0.0625     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.3958    0.07813     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.3734    0.07813     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.3738     0.1094     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.3796    0.09375     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  45       0.0001          0.3773    0.08259     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.3505    0.07813     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.3685     0.0625     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.3538    0.09375     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.3671     0.0625     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.3247    0.04688     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.3458    0.04688     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.3735    0.07813     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  46       0.0001          0.3548    0.06696     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.3776     0.0625     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.3104    0.04688     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.3446    0.04688     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.3422    0.03125     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.3659     0.0625     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.3657    0.07813     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.3516    0.07813     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  47       0.0001          0.3511    0.05804     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.3541    0.09375     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.3168    0.01563     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.3232    0.03125     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.3662    0.07813     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.3517     0.0625     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.3249    0.04688     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.3269    0.07813     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  48       0.0001          0.3377    0.05804     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.3409    0.09375     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.3082    0.01563     0.2997     0.02\n",
      "NOTE:      2    64   0.0001            0.365     0.0625     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.3102    0.04688     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.2816    0.01563     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.3234    0.03125     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.3309    0.01563     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  49       0.0001          0.3229    0.04018     0.17\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is       8.78 (s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Name</td>\n",
       "      <td>model_hisnor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Type</td>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number of Layers</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number of Input Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number of Output Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Number of Convolutional Layers</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Number of Pooling Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Number of Fully Connected Layers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Number of Batch Normalization Layers</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Number of Residual Layers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Number of Weight Parameters</td>\n",
       "      <td>414184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Number of Bias Parameters</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Total Number of Model Parameters</td>\n",
       "      <td>415358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Approximate Memory Cost for Training (MB)</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OptIterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch\">Epoch</th>\n",
       "      <th title=\"LearningRate\">LearningRate</th>\n",
       "      <th title=\"Loss\">Loss</th>\n",
       "      <th title=\"FitError\">FitError</th>\n",
       "      <th title=\"L2Norm\">L2Norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.799876</td>\n",
       "      <td>0.457589</td>\n",
       "      <td>0.299882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.741579</td>\n",
       "      <td>0.412946</td>\n",
       "      <td>0.299879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.738321</td>\n",
       "      <td>0.415179</td>\n",
       "      <td>0.299874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.753692</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.299869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.715629</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.299864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.728887</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.299859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.760095</td>\n",
       "      <td>0.470982</td>\n",
       "      <td>0.299854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.678833</td>\n",
       "      <td>0.386161</td>\n",
       "      <td>0.299849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.711193</td>\n",
       "      <td>0.430804</td>\n",
       "      <td>0.299844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.708570</td>\n",
       "      <td>0.433036</td>\n",
       "      <td>0.299839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.669012</td>\n",
       "      <td>0.386161</td>\n",
       "      <td>0.299834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.699159</td>\n",
       "      <td>0.435268</td>\n",
       "      <td>0.299829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.690255</td>\n",
       "      <td>0.424107</td>\n",
       "      <td>0.299824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.661443</td>\n",
       "      <td>0.388393</td>\n",
       "      <td>0.299819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.673182</td>\n",
       "      <td>0.412946</td>\n",
       "      <td>0.299815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.708005</td>\n",
       "      <td>0.470982</td>\n",
       "      <td>0.299810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.693352</td>\n",
       "      <td>0.455357</td>\n",
       "      <td>0.299805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.685568</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.299801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.653981</td>\n",
       "      <td>0.397321</td>\n",
       "      <td>0.299796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.673532</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.299791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.663726</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.299787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.669895</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>0.299782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.655795</td>\n",
       "      <td>0.435268</td>\n",
       "      <td>0.299778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.663711</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.299773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.633732</td>\n",
       "      <td>0.408482</td>\n",
       "      <td>0.299769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.655691</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.299764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.617529</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.299760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.629710</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.299756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.624539</td>\n",
       "      <td>0.424107</td>\n",
       "      <td>0.299751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.600829</td>\n",
       "      <td>0.386161</td>\n",
       "      <td>0.299747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.591019</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.299743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.582832</td>\n",
       "      <td>0.366071</td>\n",
       "      <td>0.299739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.570922</td>\n",
       "      <td>0.368304</td>\n",
       "      <td>0.299735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.554143</td>\n",
       "      <td>0.337054</td>\n",
       "      <td>0.299731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.536245</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.299727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.538009</td>\n",
       "      <td>0.332589</td>\n",
       "      <td>0.299723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.519913</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.299719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.498818</td>\n",
       "      <td>0.216518</td>\n",
       "      <td>0.299716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.473928</td>\n",
       "      <td>0.162946</td>\n",
       "      <td>0.299712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.454188</td>\n",
       "      <td>0.165179</td>\n",
       "      <td>0.299709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.432284</td>\n",
       "      <td>0.118304</td>\n",
       "      <td>0.299705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.424877</td>\n",
       "      <td>0.136161</td>\n",
       "      <td>0.299702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.401410</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.299699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.403132</td>\n",
       "      <td>0.116071</td>\n",
       "      <td>0.299696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.389659</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.299693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.377284</td>\n",
       "      <td>0.082589</td>\n",
       "      <td>0.299691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.354815</td>\n",
       "      <td>0.066964</td>\n",
       "      <td>0.299688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.351135</td>\n",
       "      <td>0.058036</td>\n",
       "      <td>0.299685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.337695</td>\n",
       "      <td>0.058036</td>\n",
       "      <td>0.299682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.322887</td>\n",
       "      <td>0.040179</td>\n",
       "      <td>0.299680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(weshiz)</td>\n",
       "      <td>Model_HIsnOr_weights</td>\n",
       "      <td>416510</td>\n",
       "      <td>3</td>\n",
       "      <td>CASTable('Model_HIsnOr_weights', caslib='CASUS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 11.3s</span> &#183; <span class=\"cas-user\">user 12.2s</span> &#183; <span class=\"cas-sys\">sys 4.43s</span> &#183; <span class=\"cas-memory\">mem 132MB</span></small></p>"
      ],
      "text/plain": [
       "[ModelInfo]\n",
       "\n",
       "                                         Descr                         Value\n",
       " 0                                  Model Name                  model_hisnor\n",
       " 1                                  Model Type  Convolutional Neural Network\n",
       " 2                            Number of Layers                            26\n",
       " 3                      Number of Input Layers                             1\n",
       " 4                     Number of Output Layers                             1\n",
       " 5              Number of Convolutional Layers                            10\n",
       " 6                    Number of Pooling Layers                             1\n",
       " 7            Number of Fully Connected Layers                             0\n",
       " 8        Number of Batch Normalization Layers                             9\n",
       " 9                   Number of Residual Layers                             4\n",
       " 10                Number of Weight Parameters                        414184\n",
       " 11                  Number of Bias Parameters                          1174\n",
       " 12           Total Number of Model Parameters                        415358\n",
       " 13  Approximate Memory Cost for Training (MB)                           115\n",
       "\n",
       "[OptIterHistory]\n",
       "\n",
       "     Epoch  LearningRate      Loss  FitError    L2Norm\n",
       " 0       1        0.0001  0.799876  0.457589  0.299882\n",
       " 1       2        0.0001  0.741579  0.412946  0.299879\n",
       " 2       3        0.0001  0.738321  0.415179  0.299874\n",
       " 3       4        0.0001  0.753692  0.437500  0.299869\n",
       " 4       5        0.0001  0.715629  0.406250  0.299864\n",
       " 5       6        0.0001  0.728887  0.428571  0.299859\n",
       " 6       7        0.0001  0.760095  0.470982  0.299854\n",
       " 7       8        0.0001  0.678833  0.386161  0.299849\n",
       " 8       9        0.0001  0.711193  0.430804  0.299844\n",
       " 9      10        0.0001  0.708570  0.433036  0.299839\n",
       " 10     11        0.0001  0.669012  0.386161  0.299834\n",
       " 11     12        0.0001  0.699159  0.435268  0.299829\n",
       " 12     13        0.0001  0.690255  0.424107  0.299824\n",
       " 13     14        0.0001  0.661443  0.388393  0.299819\n",
       " 14     15        0.0001  0.673182  0.412946  0.299815\n",
       " 15     16        0.0001  0.708005  0.470982  0.299810\n",
       " 16     17        0.0001  0.693352  0.455357  0.299805\n",
       " 17     18        0.0001  0.685568  0.453125  0.299801\n",
       " 18     19        0.0001  0.653981  0.397321  0.299796\n",
       " 19     20        0.0001  0.673532  0.437500  0.299791\n",
       " 20     21        0.0001  0.663726  0.428571  0.299787\n",
       " 21     22        0.0001  0.669895  0.446429  0.299782\n",
       " 22     23        0.0001  0.655795  0.435268  0.299778\n",
       " 23     24        0.0001  0.663711  0.459821  0.299773\n",
       " 24     25        0.0001  0.633732  0.408482  0.299769\n",
       " 25     26        0.0001  0.655691  0.459821  0.299764\n",
       " 26     27        0.0001  0.617529  0.390625  0.299760\n",
       " 27     28        0.0001  0.629710  0.428571  0.299756\n",
       " 28     29        0.0001  0.624539  0.424107  0.299751\n",
       " 29     30        0.0001  0.600829  0.386161  0.299747\n",
       " 30     31        0.0001  0.591019  0.375000  0.299743\n",
       " 31     32        0.0001  0.582832  0.366071  0.299739\n",
       " 32     33        0.0001  0.570922  0.368304  0.299735\n",
       " 33     34        0.0001  0.554143  0.337054  0.299731\n",
       " 34     35        0.0001  0.536245  0.312500  0.299727\n",
       " 35     36        0.0001  0.538009  0.332589  0.299723\n",
       " 36     37        0.0001  0.519913  0.281250  0.299719\n",
       " 37     38        0.0001  0.498818  0.216518  0.299716\n",
       " 38     39        0.0001  0.473928  0.162946  0.299712\n",
       " 39     40        0.0001  0.454188  0.165179  0.299709\n",
       " 40     41        0.0001  0.432284  0.118304  0.299705\n",
       " 41     42        0.0001  0.424877  0.136161  0.299702\n",
       " 42     43        0.0001  0.401410  0.089286  0.299699\n",
       " 43     44        0.0001  0.403132  0.116071  0.299696\n",
       " 44     45        0.0001  0.389659  0.093750  0.299693\n",
       " 45     46        0.0001  0.377284  0.082589  0.299691\n",
       " 46     47        0.0001  0.354815  0.066964  0.299688\n",
       " 47     48        0.0001  0.351135  0.058036  0.299685\n",
       " 48     49        0.0001  0.337695  0.058036  0.299682\n",
       " 49     50        0.0001  0.322887  0.040179  0.299680\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib                  Name    Rows  Columns  \\\n",
       " 0  CASUSER(weshiz)  Model_HIsnOr_weights  416510        3   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('Model_HIsnOr_weights', caslib='CASUS...  \n",
       "\n",
       "+ Elapsed: 11.3s, user: 12.2s, sys: 4.43s, mem: 132mb"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_like_model.fit(data=my_images, n_threads=4, record_seed=13309, optimizer=optimizer,\n",
    "                      gpu=gpu, log_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce_lr_on_plateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments, loss, are casted to array type\n"
     ]
    }
   ],
   "source": [
    "@cast_array('loss')\n",
    "def reduce_lr_on_plateau(rate, initRate, gamma, loss):\n",
    "    cool_down_iters = 5\n",
    "    patience = 1\n",
    "    len_ = len(loss)\n",
    "    temp_rate = initRate\n",
    "    cool_down_counter = cool_down_iters\n",
    "    best = loss[0]\n",
    "    for i in range(len_):\n",
    "        if loss[i] < best:\n",
    "            best = loss[i]\n",
    "            bad_epoch = 0\n",
    "        else:\n",
    "            bad_epoch = bad_epoch + 1;\n",
    "\n",
    "        if cool_down_counter > 0:\n",
    "            cool_down_counter = cool_down_counter - 1\n",
    "            bad_epoch = 0\n",
    "\n",
    "        if bad_epoch > patience:\n",
    "            temp_rate = temp_rate * gamma\n",
    "            cool_down_counter = cool_down_iters\n",
    "            bad_epoch = 0\n",
    "    rate = temp_rate\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('function reduce_lr_on_plateau(rate, initRate, gamma, loss[*]);\\n'\n",
      " '    cool_down_iters = 5;\\n'\n",
      " '    patience = 1;\\n'\n",
      " '    len_ = dim(loss);\\n'\n",
      " '    temp_rate = initRate;\\n'\n",
      " '    cool_down_counter = cool_down_iters;\\n'\n",
      " '    best = loss[1];\\n'\n",
      " '    do i = 0 to len_ by 1;\\n'\n",
      " '        if loss[i + 1] < best then do;\\n'\n",
      " '            best = loss[i + 1];\\n'\n",
      " '            bad_epoch = 0;\\n'\n",
      " '        end;\\n'\n",
      " '        else do;\\n'\n",
      " '            bad_epoch = (bad_epoch + 1);\\n'\n",
      " '        end;\\n'\n",
      " '        if cool_down_counter > 0 then do;\\n'\n",
      " '            cool_down_counter = (cool_down_counter - 1);\\n'\n",
      " '            bad_epoch = 0;\\n'\n",
      " '        end;\\n'\n",
      " '        if bad_epoch > patience then do;\\n'\n",
      " '            temp_rate = temp_rate * gamma;\\n'\n",
      " '            cool_down_counter = cool_down_iters;\\n'\n",
      " '            bad_epoch = 0;\\n'\n",
      " '        end;\\n'\n",
      " '    end;\\n'\n",
      " '    rate = temp_rate;\\n'\n",
      " '    return (rate);\\n'\n",
      " 'endsub;\\n')\n"
     ]
    }
   ],
   "source": [
    "rlr_fcmp_code = python_to_fcmp(func=reduce_lr_on_plateau, print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services saved the file REDUCE_LR_ON_PLATEAU.sashdat in caslib CASUSER(weshiz).\n"
     ]
    }
   ],
   "source": [
    "register_fcmp_routines(conn=s, routine_code=rlr_fcmp_code, function_tbl_name='reduce_lr_on_plateau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following argument(s) learning_rate, gamma, step_size, power are overwritten by the according arguments specified in lr_scheduler.\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = FCMPLR(conn=s, fcmp_learning_rate='reduce_lr_on_plateau', learning_rate=0.0001)\n",
    "solver = MomentumSolver(lr_scheduler=lr_scheduler,\n",
    "                        clip_grad_max=100, clip_grad_min=-100)\n",
    "optimizer = Optimizer(algorithm=solver, mini_batch_size=16, log_level=3, max_epochs=50, reg_l2=0.0005)\n",
    "gpu = Gpu(devices=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Inputs=_image_ is used\n",
      "NOTE: Training based on existing weights.\n",
      "NOTE: Using dlgrd009.unx.sas.com: 1 out of 4 available GPU devices.\n",
      "NOTE:  Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 415358.\n",
      "NOTE:  The approximate memory cost is 115.00 MB.\n",
      "NOTE:  Loading weights cost       0.00 (s).\n",
      "NOTE:  Initializing each layer cost       1.68 (s).\n",
      "NOTE:  The total number of threads on each worker is 4.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 16.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 64.\n",
      "NOTE:  Target variable: _label_\n",
      "NOTE:  Number of levels for the target variable:      2\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: Dolphin\n",
      "NOTE:  Level      1: Giraffe\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of numeric input variables:      1\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.3391     0.0625     0.2997     0.54\n",
      "NOTE:      1    64   0.0001           0.3371    0.04688     0.2997     0.03\n",
      "NOTE:      2    64   0.0001            0.313    0.01563     0.2997     0.03\n",
      "NOTE:      3    64   0.0001            0.356    0.01563     0.2997     0.03\n",
      "NOTE:      4    64   0.0001           0.3062    0.01563     0.2997     0.03\n",
      "NOTE:      5    64   0.0001           0.3358    0.04688     0.2997     0.03\n",
      "NOTE:      6    64   0.0001           0.3529      0.125     0.2997     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  0        0.0001          0.3343    0.04688     0.70\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.3166     0.0625     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.3419     0.1406     0.2997     0.03\n",
      "NOTE:      2    64   0.0001           0.2918    0.04688     0.2997     0.02\n",
      "NOTE:      3    64   0.0001            0.309    0.03125     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.2967    0.03125     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.2624    0.01563     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.2957    0.01563     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  1        0.0001           0.302    0.04911     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.3253    0.07813     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.3183    0.07813     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.3225    0.03125     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.3084    0.04688     0.2997     0.02\n",
      "NOTE:      4    64   0.0001            0.285    0.01563     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.2861          0     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.2906          0     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  2        0.0001          0.3052    0.03571     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.2942    0.04688     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.2957    0.03125     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.3121    0.03125     0.2997     0.02\n",
      "NOTE:      3    64   0.0001             0.32    0.04688     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.3088    0.01563     0.2997     0.02\n",
      "NOTE:      5    64   0.0001            0.284    0.03125     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.2972    0.01563     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  3        0.0001          0.3017    0.03125     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001              0.3    0.01563     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.3083    0.03125     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.3107          0     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.2769    0.03125     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.3102    0.03125     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.3274     0.0625     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.2707    0.01563     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  4        0.0001          0.3006    0.02679     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.3309    0.03125     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.2911    0.01563     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.2708          0     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.3225    0.03125     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.2868     0.0625     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.2858    0.01563     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.2723          0     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  5        0.0001          0.2943    0.02232     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.2764    0.01563     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.3199     0.0625     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.2699    0.01563     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.2824          0     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.2896    0.01563     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.3145    0.04688     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.2741    0.03125     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  6        0.0001          0.2895    0.02679     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.2466    0.03125     0.2997     0.03\n",
      "NOTE:      1    64   0.0001            0.322    0.07813     0.2997     0.02\n",
      "NOTE:      2    64   0.0001            0.323     0.1094     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.2992     0.0625     0.2997     0.02\n",
      "NOTE:      4    64   0.0001            0.306     0.0625     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.2856    0.07813     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.2702    0.03125     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  7        0.0001          0.2932    0.06473     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001           0.3107     0.0625     0.2997     0.03\n",
      "NOTE:      1    64  0.00001           0.2714    0.01563     0.2997     0.02\n",
      "NOTE:      2    64  0.00001           0.2639    0.01563     0.2997     0.02\n",
      "NOTE:      3    64  0.00001           0.2587          0     0.2997     0.02\n",
      "NOTE:      4    64  0.00001           0.3076    0.04688     0.2997     0.02\n",
      "NOTE:      5    64  0.00001             0.29    0.03125     0.2997     0.02\n",
      "NOTE:      6    64  0.00001           0.2622    0.01563     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  8          1E-5          0.2806    0.02679     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.2714    0.01563     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.2703    0.03125     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.2523    0.01563     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.2542    0.03125     0.2997     0.03\n",
      "NOTE:      4    64   0.0001           0.2975    0.07813     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.2784    0.04688     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.2791    0.04688     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  9        0.0001          0.2719    0.03795     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.2672          0     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.3012    0.07813     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.2647     0.0625     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.2859    0.03125     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.2389    0.01563     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.2627    0.03125     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.2765    0.04688     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  10       0.0001           0.271    0.03795     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.2407    0.04688     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.2537          0     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.3096    0.04688     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.2567    0.01563     0.2997     0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      4    64   0.0001            0.258    0.03125     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.2609    0.03125     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.2743    0.03125     0.2997     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  11       0.0001          0.2648    0.02902     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.2355          0     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.2651    0.03125     0.2997     0.02\n",
      "NOTE:      2    64   0.0001           0.2666    0.03125     0.2997     0.02\n",
      "NOTE:      3    64   0.0001           0.2767    0.04688     0.2997     0.02\n",
      "NOTE:      4    64   0.0001           0.2561    0.01563     0.2997     0.02\n",
      "NOTE:      5    64   0.0001           0.2329    0.03125     0.2997     0.02\n",
      "NOTE:      6    64   0.0001           0.2528    0.03125     0.2997     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  12       0.0001          0.2551    0.02679     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.2753    0.04688     0.2997     0.03\n",
      "NOTE:      1    64   0.0001           0.2435    0.04688     0.2996     0.02\n",
      "NOTE:      2    64   0.0001           0.2366    0.01563     0.2996     0.02\n",
      "NOTE:      3    64   0.0001           0.2425    0.03125     0.2996     0.02\n",
      "NOTE:      4    64   0.0001           0.2908    0.03125     0.2996     0.02\n",
      "NOTE:      5    64   0.0001            0.252     0.0625     0.2996     0.02\n",
      "NOTE:      6    64   0.0001           0.2308    0.01563     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  13       0.0001          0.2531    0.03571     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.2289    0.01563     0.2996     0.03\n",
      "NOTE:      1    64   0.0001           0.2749    0.04688     0.2996     0.02\n",
      "NOTE:      2    64   0.0001           0.2358          0     0.2996     0.02\n",
      "NOTE:      3    64   0.0001           0.2209    0.01563     0.2996     0.02\n",
      "NOTE:      4    64   0.0001           0.2349    0.01563     0.2996     0.02\n",
      "NOTE:      5    64   0.0001           0.2511    0.03125     0.2996     0.02\n",
      "NOTE:      6    64   0.0001           0.2293    0.04688     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  14       0.0001          0.2394    0.02455     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001            0.234    0.01563     0.2996     0.03\n",
      "NOTE:      1    64   0.0001           0.2519    0.01563     0.2996     0.02\n",
      "NOTE:      2    64   0.0001           0.2369    0.03125     0.2996     0.02\n",
      "NOTE:      3    64   0.0001           0.2818    0.04688     0.2996     0.02\n",
      "NOTE:      4    64   0.0001           0.2776    0.03125     0.2996     0.02\n",
      "NOTE:      5    64   0.0001           0.2327    0.01563     0.2996     0.02\n",
      "NOTE:      6    64   0.0001            0.257    0.03125     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  15       0.0001          0.2531    0.02679     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001           0.2391    0.01563     0.2996     0.03\n",
      "NOTE:      1    64  0.00001           0.2272          0     0.2996     0.02\n",
      "NOTE:      2    64  0.00001           0.2474    0.04688     0.2996     0.02\n",
      "NOTE:      3    64  0.00001           0.2532    0.07813     0.2996     0.02\n",
      "NOTE:      4    64  0.00001           0.2317    0.01563     0.2996     0.02\n",
      "NOTE:      5    64  0.00001            0.234    0.01563     0.2996     0.02\n",
      "NOTE:      6    64  0.00001           0.2216          0     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  16         1E-5          0.2363    0.02455     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.2277    0.04688     0.2996     0.03\n",
      "NOTE:      1    64   0.0001           0.2313    0.01563     0.2996     0.02\n",
      "NOTE:      2    64   0.0001           0.2377    0.03125     0.2996     0.02\n",
      "NOTE:      3    64   0.0001           0.2464    0.01563     0.2996     0.02\n",
      "NOTE:      4    64   0.0001           0.2417    0.01563     0.2996     0.02\n",
      "NOTE:      5    64   0.0001           0.2331          0     0.2996     0.02\n",
      "NOTE:      6    64   0.0001           0.2371    0.04688     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  17       0.0001          0.2364    0.02455     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001           0.2975      0.125     0.2996     0.03\n",
      "NOTE:      1    64  0.00001           0.2438    0.04688     0.2996     0.02\n",
      "NOTE:      2    64  0.00001            0.218    0.01563     0.2996     0.02\n",
      "NOTE:      3    64  0.00001           0.2302    0.01563     0.2996     0.02\n",
      "NOTE:      4    64  0.00001           0.2187    0.01563     0.2996     0.02\n",
      "NOTE:      5    64  0.00001           0.2472    0.03125     0.2996     0.02\n",
      "NOTE:      6    64  0.00001           0.1955          0     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  18         1E-5          0.2358    0.03571     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.2262    0.03125     0.2996     0.03\n",
      "NOTE:      1    64   0.0001           0.2477    0.03125     0.2996     0.02\n",
      "NOTE:      2    64   0.0001            0.217          0     0.2996     0.02\n",
      "NOTE:      3    64   0.0001           0.2363    0.01563     0.2996     0.02\n",
      "NOTE:      4    64   0.0001           0.2475    0.04688     0.2996     0.02\n",
      "NOTE:      5    64   0.0001           0.2418    0.03125     0.2996     0.02\n",
      "NOTE:      6    64   0.0001           0.2497    0.01563     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  19       0.0001           0.238    0.02455     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001           0.2485    0.01563     0.2996     0.03\n",
      "NOTE:      1    64  0.00001            0.172          0     0.2996     0.02\n",
      "NOTE:      2    64  0.00001           0.1939          0     0.2996     0.02\n",
      "NOTE:      3    64  0.00001           0.1899          0     0.2996     0.02\n",
      "NOTE:      4    64  0.00001           0.2108    0.01563     0.2996     0.02\n",
      "NOTE:      5    64  0.00001           0.2111    0.03125     0.2996     0.02\n",
      "NOTE:      6    64  0.00001            0.208    0.01563     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  20         1E-5          0.2049    0.01116     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.2466    0.04688     0.2996     0.03\n",
      "NOTE:      1    64   0.0001           0.2271    0.03125     0.2996     0.02\n",
      "NOTE:      2    64   0.0001           0.2105    0.01563     0.2996     0.02\n",
      "NOTE:      3    64   0.0001           0.2174          0     0.2996     0.02\n",
      "NOTE:      4    64   0.0001           0.2067    0.01563     0.2996     0.02\n",
      "NOTE:      5    64   0.0001           0.2052    0.01563     0.2996     0.02\n",
      "NOTE:      6    64   0.0001           0.2094    0.03125     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  21       0.0001          0.2175    0.02232     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001           0.2331    0.03125     0.2996     0.03\n",
      "NOTE:      1    64  0.00001           0.2108          0     0.2996     0.02\n",
      "NOTE:      2    64  0.00001           0.1832          0     0.2996     0.02\n",
      "NOTE:      3    64  0.00001           0.2343    0.01563     0.2996     0.02\n",
      "NOTE:      4    64  0.00001             0.23    0.01563     0.2996     0.02\n",
      "NOTE:      5    64  0.00001           0.2006    0.01563     0.2996     0.02\n",
      "NOTE:      6    64  0.00001           0.1932          0     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  22         1E-5          0.2122    0.01116     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001            0.228          0     0.2996     0.03\n",
      "NOTE:      1    64  0.00001           0.2099    0.01563     0.2996     0.02\n",
      "NOTE:      2    64  0.00001           0.2255    0.01563     0.2996     0.02\n",
      "NOTE:      3    64  0.00001           0.2382          0     0.2996     0.02\n",
      "NOTE:      4    64  0.00001           0.2441    0.04688     0.2996     0.02\n",
      "NOTE:      5    64  0.00001           0.1958    0.01563     0.2996     0.02\n",
      "NOTE:      6    64  0.00001           0.2053    0.01563     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  23         1E-5           0.221    0.01563     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001           0.2021    0.03125     0.2996     0.03\n",
      "NOTE:      1    64  0.00001           0.2017    0.01563     0.2996     0.02\n",
      "NOTE:      2    64  0.00001           0.2404     0.0625     0.2996     0.02\n",
      "NOTE:      3    64  0.00001            0.194    0.03125     0.2996     0.02\n",
      "NOTE:      4    64  0.00001           0.2568    0.07813     0.2996     0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      5    64  0.00001           0.2008    0.04688     0.2996     0.02\n",
      "NOTE:      6    64  0.00001           0.2311    0.01563     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  24         1E-5          0.2181    0.04018     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001           0.2355    0.01563     0.2996     0.03\n",
      "NOTE:      1    64  0.00001           0.1971          0     0.2996     0.02\n",
      "NOTE:      2    64  0.00001           0.2343    0.01563     0.2996     0.02\n",
      "NOTE:      3    64  0.00001           0.1906    0.01563     0.2996     0.02\n",
      "NOTE:      4    64  0.00001           0.1971          0     0.2996     0.02\n",
      "NOTE:      5    64  0.00001           0.2436    0.03125     0.2996     0.02\n",
      "NOTE:      6    64  0.00001           0.2249    0.01563     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  25         1E-5          0.2176    0.01339     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001           0.2328    0.04688     0.2996     0.03\n",
      "NOTE:      1    64  0.00001           0.2101    0.01563     0.2996     0.02\n",
      "NOTE:      2    64  0.00001           0.2137    0.03125     0.2996     0.02\n",
      "NOTE:      3    64  0.00001           0.2257          0     0.2996     0.02\n",
      "NOTE:      4    64  0.00001           0.2022          0     0.2996     0.02\n",
      "NOTE:      5    64  0.00001           0.2152    0.04688     0.2996     0.02\n",
      "NOTE:      6    64  0.00001           0.2095    0.01563     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  26         1E-5          0.2156    0.02232     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001           0.2698    0.04688     0.2996     0.03\n",
      "NOTE:      1    64  0.00001           0.2497    0.03125     0.2996     0.02\n",
      "NOTE:      2    64  0.00001           0.2112    0.01563     0.2996     0.02\n",
      "NOTE:      3    64  0.00001           0.2322    0.01563     0.2996     0.02\n",
      "NOTE:      4    64  0.00001            0.211          0     0.2996     0.02\n",
      "NOTE:      5    64  0.00001           0.2125    0.03125     0.2996     0.02\n",
      "NOTE:      6    64  0.00001           0.2366     0.0625     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  27         1E-5          0.2319    0.02902     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001           0.2044          0     0.2996     0.03\n",
      "NOTE:      1    64  0.00001           0.2116    0.03125     0.2996     0.02\n",
      "NOTE:      2    64  0.00001           0.1988          0     0.2996     0.02\n",
      "NOTE:      3    64  0.00001            0.236    0.01563     0.2996     0.02\n",
      "NOTE:      4    64  0.00001            0.219    0.03125     0.2996     0.02\n",
      "NOTE:      5    64  0.00001            0.196          0     0.2996     0.02\n",
      "NOTE:      6    64  0.00001           0.2423    0.01563     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  28         1E-5          0.2154    0.01339     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6           0.2313    0.01563     0.2996     0.03\n",
      "NOTE:      1    64     1E-6           0.2666    0.07813     0.2996     0.02\n",
      "NOTE:      2    64     1E-6           0.2321    0.03125     0.2996     0.02\n",
      "NOTE:      3    64     1E-6           0.2075          0     0.2996     0.02\n",
      "NOTE:      4    64     1E-6           0.2187    0.01563     0.2996     0.02\n",
      "NOTE:      5    64     1E-6           0.2647    0.07813     0.2996     0.02\n",
      "NOTE:      6    64     1E-6           0.2128    0.03125     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  29         1E-6          0.2334    0.03571     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6           0.2314    0.03125     0.2996     0.03\n",
      "NOTE:      1    64     1E-6           0.2197    0.03125     0.2996     0.02\n",
      "NOTE:      2    64     1E-6           0.2252    0.01563     0.2996     0.02\n",
      "NOTE:      3    64     1E-6           0.2429     0.0625     0.2996     0.02\n",
      "NOTE:      4    64     1E-6           0.2116    0.01563     0.2996     0.02\n",
      "NOTE:      5    64     1E-6           0.2151    0.03125     0.2996     0.02\n",
      "NOTE:      6    64     1E-6           0.2155    0.01563     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  30         1E-6          0.2231    0.02902     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6           0.2098    0.04688     0.2996     0.03\n",
      "NOTE:      1    64     1E-6           0.1957          0     0.2996     0.02\n",
      "NOTE:      2    64     1E-6           0.2245    0.03125     0.2996     0.02\n",
      "NOTE:      3    64     1E-6           0.2169    0.01563     0.2996     0.02\n",
      "NOTE:      4    64     1E-6           0.2284    0.04688     0.2996     0.02\n",
      "NOTE:      5    64     1E-6            0.187          0     0.2996     0.02\n",
      "NOTE:      6    64     1E-6           0.2611    0.04688     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  31         1E-6          0.2176    0.02679     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6           0.2158          0     0.2996     0.03\n",
      "NOTE:      1    64     1E-6           0.2338     0.0625     0.2996     0.02\n",
      "NOTE:      2    64     1E-6           0.2256    0.03125     0.2996     0.02\n",
      "NOTE:      3    64     1E-6           0.2258    0.04688     0.2996     0.02\n",
      "NOTE:      4    64     1E-6           0.1874          0     0.2996     0.02\n",
      "NOTE:      5    64     1E-6           0.2056    0.03125     0.2996     0.02\n",
      "NOTE:      6    64     1E-6           0.2147          0     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  32         1E-6          0.2155    0.02455     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6           0.2014    0.04688     0.2996     0.03\n",
      "NOTE:      1    64     1E-6            0.214    0.01563     0.2996     0.02\n",
      "NOTE:      2    64     1E-6           0.2299    0.03125     0.2996     0.02\n",
      "NOTE:      3    64     1E-6           0.1963          0     0.2996     0.02\n",
      "NOTE:      4    64     1E-6           0.2392    0.01563     0.2996     0.02\n",
      "NOTE:      5    64     1E-6           0.2154          0     0.2996     0.02\n",
      "NOTE:      6    64     1E-6           0.1769    0.01563     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  33         1E-6          0.2104    0.01786     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6           0.2431    0.04688     0.2996     0.03\n",
      "NOTE:      1    64     1E-6           0.1888    0.01563     0.2996     0.02\n",
      "NOTE:      2    64     1E-6           0.2195          0     0.2996     0.02\n",
      "NOTE:      3    64     1E-6           0.2528     0.0625     0.2996     0.02\n",
      "NOTE:      4    64     1E-6           0.2009          0     0.2996     0.02\n",
      "NOTE:      5    64     1E-6           0.2008          0     0.2996     0.02\n",
      "NOTE:      6    64     1E-6           0.2089    0.01563     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  34         1E-6          0.2164    0.02009     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6           0.2383    0.01563     0.2996     0.03\n",
      "NOTE:      1    64     1E-6           0.1961          0     0.2996     0.02\n",
      "NOTE:      2    64     1E-6            0.169          0     0.2996     0.02\n",
      "NOTE:      3    64     1E-6           0.2344          0     0.2996     0.02\n",
      "NOTE:      4    64     1E-6           0.2312    0.01563     0.2996     0.02\n",
      "NOTE:      5    64     1E-6           0.1963    0.01563     0.2996     0.02\n",
      "NOTE:      6    64     1E-6           0.1867    0.03125     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  35         1E-6          0.2074    0.01116     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7           0.2144    0.03125     0.2996     0.03\n",
      "NOTE:      1    64     1E-7           0.2308    0.04688     0.2996     0.02\n",
      "NOTE:      2    64     1E-7           0.2076          0     0.2996     0.02\n",
      "NOTE:      3    64     1E-7           0.2113          0     0.2996     0.02\n",
      "NOTE:      4    64     1E-7              0.2          0     0.2996     0.02\n",
      "NOTE:      5    64     1E-7           0.2054    0.01563     0.2996     0.02\n",
      "NOTE:      6    64     1E-7            0.221    0.03125     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  36         1E-7           0.213    0.01786     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7           0.1978    0.01563     0.2996     0.03\n",
      "NOTE:      1    64     1E-7           0.2081    0.03125     0.2996     0.02\n",
      "NOTE:      2    64     1E-7           0.2125    0.01563     0.2996     0.02\n",
      "NOTE:      3    64     1E-7           0.2052          0     0.2996     0.02\n",
      "NOTE:      4    64     1E-7           0.2376    0.03125     0.2996     0.02\n",
      "NOTE:      5    64     1E-7           0.2355    0.04688     0.2996     0.02\n",
      "NOTE:      6    64     1E-7             0.21    0.01563     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  37         1E-7          0.2152    0.02232     0.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7           0.2075    0.01563     0.2996     0.03\n",
      "NOTE:      1    64     1E-7           0.2463     0.0625     0.2996     0.02\n",
      "NOTE:      2    64     1E-7           0.2241    0.01563     0.2996     0.02\n",
      "NOTE:      3    64     1E-7           0.1992          0     0.2996     0.02\n",
      "NOTE:      4    64     1E-7           0.2448     0.0625     0.2996     0.02\n",
      "NOTE:      5    64     1E-7           0.1865          0     0.2996     0.02\n",
      "NOTE:      6    64     1E-7           0.2164    0.01563     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  38         1E-7          0.2178    0.02455     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7           0.2307    0.04688     0.2996     0.03\n",
      "NOTE:      1    64     1E-7           0.2485    0.04688     0.2996     0.02\n",
      "NOTE:      2    64     1E-7           0.1978          0     0.2996     0.02\n",
      "NOTE:      3    64     1E-7           0.1917          0     0.2996     0.02\n",
      "NOTE:      4    64     1E-7           0.2182    0.03125     0.2996     0.02\n",
      "NOTE:      5    64     1E-7           0.2323    0.04688     0.2996     0.02\n",
      "NOTE:      6    64     1E-7           0.2127    0.03125     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  39         1E-7          0.2188    0.02902     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7           0.2554     0.0625     0.2996     0.03\n",
      "NOTE:      1    64     1E-7           0.2006    0.03125     0.2996     0.02\n",
      "NOTE:      2    64     1E-7           0.1955    0.01563     0.2996     0.02\n",
      "NOTE:      3    64     1E-7           0.2241    0.03125     0.2996     0.02\n",
      "NOTE:      4    64     1E-7           0.2392    0.07813     0.2996     0.02\n",
      "NOTE:      5    64     1E-7           0.2019          0     0.2996     0.02\n",
      "NOTE:      6    64     1E-7           0.2982    0.09375     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  40         1E-7          0.2307    0.04464     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7           0.2167    0.01563     0.2996     0.03\n",
      "NOTE:      1    64     1E-7           0.2269    0.01563     0.2996     0.02\n",
      "NOTE:      2    64     1E-7            0.223    0.01563     0.2996     0.02\n",
      "NOTE:      3    64     1E-7           0.2011    0.03125     0.2996     0.02\n",
      "NOTE:      4    64     1E-7           0.2088          0     0.2996     0.02\n",
      "NOTE:      5    64     1E-7           0.1881    0.01563     0.2996     0.02\n",
      "NOTE:      6    64     1E-7           0.2248    0.01563     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  41         1E-7          0.2128    0.01563     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7           0.1862          0     0.2996     0.03\n",
      "NOTE:      1    64     1E-7           0.2049    0.01563     0.2996     0.02\n",
      "NOTE:      2    64     1E-7           0.2239    0.01563     0.2996     0.02\n",
      "NOTE:      3    64     1E-7            0.203          0     0.2996     0.02\n",
      "NOTE:      4    64     1E-7           0.1784          0     0.2996     0.02\n",
      "NOTE:      5    64     1E-7           0.2386          0     0.2996     0.02\n",
      "NOTE:      6    64     1E-7           0.2235    0.03125     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  42         1E-7          0.2084   0.008929     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8           0.2019    0.01563     0.2996     0.03\n",
      "NOTE:      1    64     1E-8           0.2016    0.01563     0.2996     0.02\n",
      "NOTE:      2    64     1E-8           0.2199    0.01563     0.2996     0.02\n",
      "NOTE:      3    64     1E-8           0.2124    0.03125     0.2996     0.02\n",
      "NOTE:      4    64     1E-8           0.2131    0.01563     0.2996     0.02\n",
      "NOTE:      5    64     1E-8           0.2494     0.0625     0.2996     0.02\n",
      "NOTE:      6    64     1E-8           0.2421    0.03125     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  43         1E-8          0.2201    0.02679     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8           0.2124    0.03125     0.2996     0.03\n",
      "NOTE:      1    64     1E-8           0.2102    0.03125     0.2996     0.02\n",
      "NOTE:      2    64     1E-8           0.2188    0.03125     0.2996     0.02\n",
      "NOTE:      3    64     1E-8           0.2324    0.03125     0.2996     0.02\n",
      "NOTE:      4    64     1E-8           0.2904     0.0625     0.2996     0.02\n",
      "NOTE:      5    64     1E-8           0.2098    0.01563     0.2996     0.02\n",
      "NOTE:      6    64     1E-8            0.226    0.03125     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  44         1E-8          0.2286    0.03348     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8           0.2345    0.01563     0.2996     0.03\n",
      "NOTE:      1    64     1E-8           0.2211    0.03125     0.2996     0.02\n",
      "NOTE:      2    64     1E-8           0.2062    0.01563     0.2996     0.02\n",
      "NOTE:      3    64     1E-8           0.2394    0.01563     0.2996     0.02\n",
      "NOTE:      4    64     1E-8           0.2325    0.03125     0.2996     0.02\n",
      "NOTE:      5    64     1E-8           0.2134    0.01563     0.2996     0.02\n",
      "NOTE:      6    64     1E-8           0.2433    0.04688     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  45         1E-8          0.2272    0.02455     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8           0.2161    0.01563     0.2996     0.03\n",
      "NOTE:      1    64     1E-8           0.2237    0.03125     0.2996     0.02\n",
      "NOTE:      2    64     1E-8           0.2249     0.0625     0.2996     0.02\n",
      "NOTE:      3    64     1E-8           0.2235    0.04688     0.2996     0.02\n",
      "NOTE:      4    64     1E-8           0.2014    0.04688     0.2996     0.02\n",
      "NOTE:      5    64     1E-8           0.2058    0.03125     0.2996     0.02\n",
      "NOTE:      6    64     1E-8           0.2138          0     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  46         1E-8          0.2156    0.03348     0.17\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8           0.2375    0.03125     0.2996     0.03\n",
      "NOTE:      1    64     1E-8             0.18          0     0.2996     0.02\n",
      "NOTE:      2    64     1E-8           0.2134    0.03125     0.2996     0.02\n",
      "NOTE:      3    64     1E-8            0.212    0.03125     0.2996     0.02\n",
      "NOTE:      4    64     1E-8           0.2342    0.01563     0.2996     0.02\n",
      "NOTE:      5    64     1E-8           0.2436    0.04688     0.2996     0.02\n",
      "NOTE:      6    64     1E-8           0.2268    0.03125     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  47         1E-8          0.2211    0.02679     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8           0.2255    0.04688     0.2996     0.03\n",
      "NOTE:      1    64     1E-8           0.1971          0     0.2996     0.02\n",
      "NOTE:      2    64     1E-8            0.198    0.01563     0.2996     0.02\n",
      "NOTE:      3    64     1E-8           0.2382    0.04688     0.2996     0.02\n",
      "NOTE:      4    64     1E-8           0.2334     0.0625     0.2996     0.02\n",
      "NOTE:      5    64     1E-8           0.2088    0.01563     0.2996     0.02\n",
      "NOTE:      6    64     1E-8           0.2263    0.04688     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  48         1E-8          0.2182    0.03348     0.16\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8           0.2229    0.04688     0.2996     0.03\n",
      "NOTE:      1    64     1E-8            0.206    0.01563     0.2996     0.02\n",
      "NOTE:      2    64     1E-8           0.2416    0.01563     0.2996     0.02\n",
      "NOTE:      3    64     1E-8           0.1912    0.01563     0.2996     0.02\n",
      "NOTE:      4    64     1E-8           0.1926    0.01563     0.2996     0.02\n",
      "NOTE:      5    64     1E-8           0.2006          0     0.2996     0.02\n",
      "NOTE:      6    64     1E-8           0.2179          0     0.2996     0.02\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  49         1E-8          0.2104    0.01563     0.17\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is       8.79 (s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Name</td>\n",
       "      <td>model_hisnor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Type</td>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number of Layers</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number of Input Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number of Output Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Number of Convolutional Layers</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Number of Pooling Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Number of Fully Connected Layers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Number of Batch Normalization Layers</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Number of Residual Layers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Number of Weight Parameters</td>\n",
       "      <td>414184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Number of Bias Parameters</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Total Number of Model Parameters</td>\n",
       "      <td>415358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Approximate Memory Cost for Training (MB)</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OptIterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch\">Epoch</th>\n",
       "      <th title=\"LearningRate\">LearningRate</th>\n",
       "      <th title=\"Loss\">Loss</th>\n",
       "      <th title=\"FitError\">FitError</th>\n",
       "      <th title=\"L2Norm\">L2Norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.334300</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.299678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.302017</td>\n",
       "      <td>0.049107</td>\n",
       "      <td>0.299677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.305171</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.299675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.301721</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.299672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.300604</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.299670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.294296</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.299667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>57</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.289547</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.299665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>58</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.293226</td>\n",
       "      <td>0.064732</td>\n",
       "      <td>0.299662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>59</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.280648</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.299660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>60</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.271888</td>\n",
       "      <td>0.037946</td>\n",
       "      <td>0.299658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>61</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.271024</td>\n",
       "      <td>0.037946</td>\n",
       "      <td>0.299656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>62</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.264836</td>\n",
       "      <td>0.029018</td>\n",
       "      <td>0.299654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>63</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.255107</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.299652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>64</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.253085</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.299649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>65</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.239416</td>\n",
       "      <td>0.024554</td>\n",
       "      <td>0.299647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>66</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.253098</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.299644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>67</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.236312</td>\n",
       "      <td>0.024554</td>\n",
       "      <td>0.299642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>68</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.236430</td>\n",
       "      <td>0.024554</td>\n",
       "      <td>0.299640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>69</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.235839</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.299639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>70</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.238029</td>\n",
       "      <td>0.024554</td>\n",
       "      <td>0.299637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>71</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.204884</td>\n",
       "      <td>0.011161</td>\n",
       "      <td>0.299636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>72</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.217545</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.299635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>73</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.212182</td>\n",
       "      <td>0.011161</td>\n",
       "      <td>0.299633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>74</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.220962</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.299632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>75</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.218110</td>\n",
       "      <td>0.040179</td>\n",
       "      <td>0.299631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>76</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.217566</td>\n",
       "      <td>0.013393</td>\n",
       "      <td>0.299631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>77</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.215619</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.299631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>78</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.231856</td>\n",
       "      <td>0.029018</td>\n",
       "      <td>0.299630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>79</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.215443</td>\n",
       "      <td>0.013393</td>\n",
       "      <td>0.299630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>80</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.233373</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.299630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>81</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.223056</td>\n",
       "      <td>0.029018</td>\n",
       "      <td>0.299630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>82</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.217625</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.299630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>83</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.215525</td>\n",
       "      <td>0.024554</td>\n",
       "      <td>0.299630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>84</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.210445</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.299630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>85</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.216385</td>\n",
       "      <td>0.020089</td>\n",
       "      <td>0.299630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>86</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.207431</td>\n",
       "      <td>0.011161</td>\n",
       "      <td>0.299630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>87</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.212957</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.299629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>88</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.215245</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.299629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>89</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.217833</td>\n",
       "      <td>0.024554</td>\n",
       "      <td>0.299629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>90</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.218830</td>\n",
       "      <td>0.029018</td>\n",
       "      <td>0.299629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>91</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.230704</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>0.299629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>92</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.212775</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.299629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>93</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.208354</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.299629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>94</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.220056</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.299629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>95</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.228560</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>0.299629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>96</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.227208</td>\n",
       "      <td>0.024554</td>\n",
       "      <td>0.299629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>97</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.215595</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>0.299629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>98</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.221062</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.299629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>99</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.218188</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>0.299629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>100</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.210408</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.299629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(weshiz)</td>\n",
       "      <td>Model_HIsnOr_weights</td>\n",
       "      <td>416510</td>\n",
       "      <td>3</td>\n",
       "      <td>CASTable('Model_HIsnOr_weights', caslib='CASUS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 10.8s</span> &#183; <span class=\"cas-user\">user 12.4s</span> &#183; <span class=\"cas-sys\">sys 3.83s</span> &#183; <span class=\"cas-memory\">mem 132MB</span></small></p>"
      ],
      "text/plain": [
       "[ModelInfo]\n",
       "\n",
       "                                         Descr                         Value\n",
       " 0                                  Model Name                  model_hisnor\n",
       " 1                                  Model Type  Convolutional Neural Network\n",
       " 2                            Number of Layers                            26\n",
       " 3                      Number of Input Layers                             1\n",
       " 4                     Number of Output Layers                             1\n",
       " 5              Number of Convolutional Layers                            10\n",
       " 6                    Number of Pooling Layers                             1\n",
       " 7            Number of Fully Connected Layers                             0\n",
       " 8        Number of Batch Normalization Layers                             9\n",
       " 9                   Number of Residual Layers                             4\n",
       " 10                Number of Weight Parameters                        414184\n",
       " 11                  Number of Bias Parameters                          1174\n",
       " 12           Total Number of Model Parameters                        415358\n",
       " 13  Approximate Memory Cost for Training (MB)                           115\n",
       "\n",
       "[OptIterHistory]\n",
       "\n",
       "     Epoch  LearningRate      Loss  FitError    L2Norm\n",
       " 0      51  1.000000e-04  0.334300  0.046875  0.299678\n",
       " 1      52  1.000000e-04  0.302017  0.049107  0.299677\n",
       " 2      53  1.000000e-04  0.305171  0.035714  0.299675\n",
       " 3      54  1.000000e-04  0.301721  0.031250  0.299672\n",
       " 4      55  1.000000e-04  0.300604  0.026786  0.299670\n",
       " 5      56  1.000000e-04  0.294296  0.022321  0.299667\n",
       " 6      57  1.000000e-04  0.289547  0.026786  0.299665\n",
       " 7      58  1.000000e-04  0.293226  0.064732  0.299662\n",
       " 8      59  1.000000e-05  0.280648  0.026786  0.299660\n",
       " 9      60  1.000000e-04  0.271888  0.037946  0.299658\n",
       " 10     61  1.000000e-04  0.271024  0.037946  0.299656\n",
       " 11     62  1.000000e-04  0.264836  0.029018  0.299654\n",
       " 12     63  1.000000e-04  0.255107  0.026786  0.299652\n",
       " 13     64  1.000000e-04  0.253085  0.035714  0.299649\n",
       " 14     65  1.000000e-04  0.239416  0.024554  0.299647\n",
       " 15     66  1.000000e-04  0.253098  0.026786  0.299644\n",
       " 16     67  1.000000e-05  0.236312  0.024554  0.299642\n",
       " 17     68  1.000000e-04  0.236430  0.024554  0.299640\n",
       " 18     69  1.000000e-05  0.235839  0.035714  0.299639\n",
       " 19     70  1.000000e-04  0.238029  0.024554  0.299637\n",
       " 20     71  1.000000e-05  0.204884  0.011161  0.299636\n",
       " 21     72  1.000000e-04  0.217545  0.022321  0.299635\n",
       " 22     73  1.000000e-05  0.212182  0.011161  0.299633\n",
       " 23     74  1.000000e-05  0.220962  0.015625  0.299632\n",
       " 24     75  1.000000e-05  0.218110  0.040179  0.299631\n",
       " 25     76  1.000000e-05  0.217566  0.013393  0.299631\n",
       " 26     77  1.000000e-05  0.215619  0.022321  0.299631\n",
       " 27     78  1.000000e-05  0.231856  0.029018  0.299630\n",
       " 28     79  1.000000e-05  0.215443  0.013393  0.299630\n",
       " 29     80  1.000000e-06  0.233373  0.035714  0.299630\n",
       " 30     81  1.000000e-06  0.223056  0.029018  0.299630\n",
       " 31     82  1.000000e-06  0.217625  0.026786  0.299630\n",
       " 32     83  1.000000e-06  0.215525  0.024554  0.299630\n",
       " 33     84  1.000000e-06  0.210445  0.017857  0.299630\n",
       " 34     85  1.000000e-06  0.216385  0.020089  0.299630\n",
       " 35     86  1.000000e-06  0.207431  0.011161  0.299630\n",
       " 36     87  1.000000e-07  0.212957  0.017857  0.299629\n",
       " 37     88  1.000000e-07  0.215245  0.022321  0.299629\n",
       " 38     89  1.000000e-07  0.217833  0.024554  0.299629\n",
       " 39     90  1.000000e-07  0.218830  0.029018  0.299629\n",
       " 40     91  1.000000e-07  0.230704  0.044643  0.299629\n",
       " 41     92  1.000000e-07  0.212775  0.015625  0.299629\n",
       " 42     93  1.000000e-07  0.208354  0.008929  0.299629\n",
       " 43     94  1.000000e-08  0.220056  0.026786  0.299629\n",
       " 44     95  1.000000e-08  0.228560  0.033482  0.299629\n",
       " 45     96  1.000000e-08  0.227208  0.024554  0.299629\n",
       " 46     97  1.000000e-08  0.215595  0.033482  0.299629\n",
       " 47     98  1.000000e-08  0.221062  0.026786  0.299629\n",
       " 48     99  1.000000e-08  0.218188  0.033482  0.299629\n",
       " 49    100  1.000000e-08  0.210408  0.015625  0.299629\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib                  Name    Rows  Columns  \\\n",
       " 0  CASUSER(weshiz)  Model_HIsnOr_weights  416510        3   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('Model_HIsnOr_weights', caslib='CASUS...  \n",
       "\n",
       "+ Elapsed: 10.8s, user: 12.4s, sys: 3.83s, mem: 132mb"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_like_model.fit(data=my_images, n_threads=4, record_seed=13309, optimizer=optimizer,\n",
    "                      gpu=gpu, log_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.000123s</span> &#183; <span class=\"cas-user\">user 0.000101s</span> &#183; <span class=\"cas-memory\">mem 0.203MB</span></small></p>"
      ],
      "text/plain": [
       "+ Elapsed: 0.000123s, user: 0.000101s, mem: 0.203mb"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.endsession()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
