{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swat import *\n",
    "import swat as sw\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import sys\n",
    "\n",
    "sys.path.append('/cas/DeepLearn/weshiz/github_workspace/python-dlpy/')\n",
    "sys.path.append('/cas/DeepLearn/weshiz/github_workspace/python-fcmp')\n",
    "from dlpy.layers import * \n",
    "from dlpy.applications import *\n",
    "from dlpy import Model, Sequential\n",
    "from dlpy.utils import *\n",
    "from dlpy.splitting import two_way_split\n",
    "from dlpy.images import *\n",
    "from dlpy.model import *\n",
    "from python_fcmp.parser import *\n",
    "from python_fcmp.decorator import *\n",
    "from python_fcmp import fcmp\n",
    "from dlpy.lr_scheduler import *\n",
    "from dlpy.network import *\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Added action set 'deeplearn'.\n",
      "NOTE: Added action set 'clustering'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; actionset</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>clustering</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.00183s</span> &#183; <span class=\"cas-sys\">sys 0.0018s</span> &#183; <span class=\"cas-memory\">mem 0.225MB</span></small></p>"
      ],
      "text/plain": [
       "[actionset]\n",
       "\n",
       " 'clustering'\n",
       "\n",
       "+ Elapsed: 0.00183s, sys: 0.0018s, mem: 0.225mb"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = sw.CAS('dlgrd009', 23309)\n",
    "s.loadactionset('deeplearn')\n",
    "s.loadactionset('clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: 'CASUSER(weshiz)' is now the active caslib.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.000216s</span> &#183; <span class=\"cas-user\">user 0.000121s</span> &#183; <span class=\"cas-sys\">sys 7.4e-05s</span> &#183; <span class=\"cas-memory\">mem 0.256MB</span></small></p>"
      ],
      "text/plain": [
       "+ Elapsed: 0.000216s, user: 0.000121s, sys: 7.4e-05s, mem: 0.256mb"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://go.documentation.sas.com/?docsetId=lesysoptsref&docsetTarget=n16rhscxem9ljwn1kuhn5170xkbg.htm&docsetVersion=9.4&locale=en\n",
    "s.sessionProp.setSessOpt(caslib='CASUSER', cmplib=\"CASUSER.fcmpfunction\", cmpopt=\"ALL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define forward and backward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     18
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments, srcY, weights, y_out, are casted to array type\n",
      "Arguments, y_out, are declared as outargs.\n",
      "Arguments, srcY, Y, weights, deltas, gradient_out, srcDeltas_out, are casted to array type\n",
      "Arguments, gradient_out, srcDeltas_out, are declared as outargs.\n"
     ]
    }
   ],
   "source": [
    "@out_args('y_out')  # pass by reference\n",
    "@cast_array('srcY', 'weights', 'y_out')  # declare the arguments as array type\n",
    "def forward_prop(srcHeight, srcWidth, srcDepth, srcY, weights, y_out):\n",
    "    mean = numpy.zeros((6)) # initialize mean and var with length\n",
    "    var = numpy.zeros((6))\n",
    "    \n",
    "    wd = fcmp.reduce_axis((0, srcWidth)) # create an iterator which iterate from 0 to srcWidth\n",
    "    ht = fcmp.reduce_axis((0, srcHeight))\n",
    "    eps = 0.0001 # avoid deviding by 0\n",
    "    srcY = fcmp.reshape(srcY, (srcDepth, srcHeight, srcWidth)) # reshape the array and access each element via multiple subscripts\n",
    "    mean = fcmp.compute((srcDepth), lambda i: fcmp.sum(srcY[i, ht, wd] / srcHeight / srcWidth, [ht, wd])) # generate a new variable and describle the computation rule\n",
    "    var = fcmp.compute((srcDepth), lambda i: fcmp.sum((srcY[i, ht, wd] - mean[i]) ** 2 / srcHeight / srcWidth, [ht, wd]))\n",
    "    y_out = fcmp.compute((srcDepth, srcHeight, srcWidth), lambda i, j, m: (srcY[i, j, m] - mean[i]) / (var[i] + eps) ** 0.5)\n",
    "    \n",
    "    return\n",
    "\n",
    "@out_args('gradient_out', 'srcDeltas_out')\n",
    "@cast_array('srcY', 'Y', 'weights', 'deltas', 'gradient_out', 'srcDeltas_out')\n",
    "def back_prop(srcHeight, srcWidth, srcDepth, srcY, Y, weights, deltas, gradient_out, srcDeltas_out):\n",
    "    dmean = numpy.zeros((6))\n",
    "    dvar = numpy.zeros((6))\n",
    "    mean = numpy.zeros((6))\n",
    "    var = numpy.zeros((6))\n",
    "    grad_sigma = numpy.zeros((6))\n",
    "    grad_mean = numpy.zeros((6))\n",
    "    grad_mean_2 = numpy.zeros((6))\n",
    "    for i in range(srcHeight*srcWidth*srcDepth):\n",
    "        srcDeltas_out[i] = 0\n",
    "    \n",
    "    by_norm = 1 / (srcHeight * srcWidth)\n",
    "    \n",
    "    wd = fcmp.reduce_axis((0, srcWidth))\n",
    "    ht = fcmp.reduce_axis((0, srcHeight))\n",
    "    \n",
    "    srcY = fcmp.reshape(srcY, (srcDepth, srcHeight, srcWidth))\n",
    "    deltas = fcmp.reshape(deltas, (srcDepth, srcHeight, srcWidth))\n",
    "    \n",
    "    eps = 0.0001\n",
    "    \n",
    "    mean = fcmp.compute((srcDepth), lambda i: fcmp.sum(srcY[i, ht, wd] / srcHeight / srcWidth, [ht, wd]))\n",
    "    var = fcmp.compute((srcDepth), lambda i: fcmp.sum((srcY[i, ht, wd] - mean[i]) ** 2 / srcHeight / srcWidth, [ht, wd]))\n",
    "    \n",
    "    grad_sigma = fcmp.compute((srcDepth), lambda i: fcmp.sum(deltas[i, ht, wd] * (srcY[i] - mean[i]), [ht, wd]))\n",
    "    grad_sigma = fcmp.compute((srcDepth), lambda i: grad_sigma[i] * (-0.5) * (var[i] + eps) ** -1.5)\n",
    "    \n",
    "    grad_mean = fcmp.compute((srcDepth), lambda i: fcmp.sum(deltas[i, ht, wd] * (-1 * (var[i] + eps) ** 0.5), [ht, wd]))\n",
    "    grad_mean_2 = fcmp.compute((srcDepth), lambda i: fcmp.sum(srcY[i, ht, wd] - mean[i], [ht, wd]))\n",
    "    grad_mean = fcmp.compute((srcDepth), lambda i: grad_mean[i] + grad_sigma[i] * by_norm * 2 * grad_mean_2[i] * -1)\n",
    "    \n",
    "    srcDeltas_out = fcmp.compute((srcDepth), lambda i: fcmp.sum(srcY[i, ht, wd] - mean[i], [ht, wd]))\n",
    "    srcDeltas_out = fcmp.compute((srcDepth), lambda i: deltas[i] * (1 / (var[i] + eps)** 0.5) + grad_sigma[i] * by_norm * 2 + srcDeltas_out[i] + grad_mean[i] * by_norm)\n",
    "    \n",
    "    return\n",
    "\n",
    "def dummy_loss(t, target):\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate forward FCMP code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('function forward_prop(srcHeight, srcWidth, srcDepth, srcY[*], weights[*], y_out[*]);outargs y_out;\\n'\n",
      " '    array mean[6]; do i = 1 to 6 by 1; mean[i] = 0; end;\\n'\n",
      " '    array var[6]; do i = 1 to 6 by 1; var[i] = 0; end;\\n'\n",
      " '    eps = 0.0001;\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        do ht = 0 to (srcHeight - 1) by 1;\\n'\n",
      " '            do wd = 0 to (srcWidth - 1) by 1;\\n'\n",
      " '                mean[i + 1] = (mean[i + 1] + ((srcY[(((srcHeight * srcWidth) * i) + (srcWidth * ht)) + wd + 1] / srcHeight) / srcWidth));\\n'\n",
      " '            end;\\n'\n",
      " '        end;\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        do ht = 0 to (srcHeight - 1) by 1;\\n'\n",
      " '            do wd = 0 to (srcWidth - 1) by 1;\\n'\n",
      " '                var[i + 1] = (var[i + 1] + (((srcY[(((srcHeight * srcWidth) * i) + (srcWidth * ht)) + wd + 1] - mean[i + 1]) ** 2 / srcHeight) / srcWidth));\\n'\n",
      " '            end;\\n'\n",
      " '        end;\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        do j = 0 to (srcHeight - 1) by 1;\\n'\n",
      " '            do m = 0 to (srcWidth - 1) by 1;\\n'\n",
      " '                y_out[(((srcHeight * srcWidth) * i) + (srcWidth * j)) + m + 1] = ((srcY[(((srcHeight * srcWidth) * i) + (srcWidth * j)) + m + 1] - mean[i + 1]) / (var[i + 1] + eps) ** 0.5);\\n'\n",
      " '            end;\\n'\n",
      " '        end;\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    return ;\\n'\n",
      " 'endsub;\\n')\n"
     ]
    }
   ],
   "source": [
    "# pdb.set_trace()\n",
    "forward_fcmp_code = python_to_fcmp(func=forward_prop, print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate backward FCMP code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('function back_prop(srcHeight, srcWidth, srcDepth, srcY[*], Y[*], weights[*], deltas[*], gradient_out[*], srcDeltas_out[*]);outargs gradient_out, srcDeltas_out;\\n'\n",
      " '    array dmean[6]; do i = 1 to 6 by 1; dmean[i] = 0; end;\\n'\n",
      " '    array dvar[6]; do i = 1 to 6 by 1; dvar[i] = 0; end;\\n'\n",
      " '    array mean[6]; do i = 1 to 6 by 1; mean[i] = 0; end;\\n'\n",
      " '    array var[6]; do i = 1 to 6 by 1; var[i] = 0; end;\\n'\n",
      " '    array grad_sigma[6]; do i = 1 to 6 by 1; grad_sigma[i] = 0; end;\\n'\n",
      " '    array grad_mean[6]; do i = 1 to 6 by 1; grad_mean[i] = 0; end;\\n'\n",
      " '    array grad_mean_2[6]; do i = 1 to 6 by 1; grad_mean_2[i] = 0; end;\\n'\n",
      " '    do i = 0 to ((srcHeight * srcWidth) * srcDepth) - 1 by 1;\\n'\n",
      " '        srcDeltas_out[i + 1] = 0;\\n'\n",
      " '    end;\\n'\n",
      " '    by_norm = (1 / (srcHeight * srcWidth));\\n'\n",
      " '    eps = 0.0001;\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        do ht = 0 to (srcHeight - 1) by 1;\\n'\n",
      " '            do wd = 0 to (srcWidth - 1) by 1;\\n'\n",
      " '                mean[i + 1] = (mean[i + 1] + ((srcY[(((srcHeight * srcWidth) * i) + (srcWidth * ht)) + wd + 1] / srcHeight) / srcWidth));\\n'\n",
      " '            end;\\n'\n",
      " '        end;\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        do ht = 0 to (srcHeight - 1) by 1;\\n'\n",
      " '            do wd = 0 to (srcWidth - 1) by 1;\\n'\n",
      " '                var[i + 1] = (var[i + 1] + (((srcY[(((srcHeight * srcWidth) * i) + (srcWidth * ht)) + wd + 1] - mean[i + 1]) ** 2 / srcHeight) / srcWidth));\\n'\n",
      " '            end;\\n'\n",
      " '        end;\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        do ht = 0 to (srcHeight - 1) by 1;\\n'\n",
      " '            do wd = 0 to (srcWidth - 1) by 1;\\n'\n",
      " '                grad_sigma[i + 1] = (grad_sigma[i + 1] + (deltas[(((srcHeight * srcWidth) * i) + (srcWidth * ht)) + wd + 1] * (srcY[i + 1] - mean[i + 1])));\\n'\n",
      " '            end;\\n'\n",
      " '        end;\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        grad_sigma[i + 1] = ((grad_sigma[i + 1] * -0.5) * (var[i + 1] + eps) ** -1.5);\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        do ht = 0 to (srcHeight - 1) by 1;\\n'\n",
      " '            do wd = 0 to (srcWidth - 1) by 1;\\n'\n",
      " '                grad_mean[i + 1] = (grad_mean[i + 1] + (deltas[(((srcHeight * srcWidth) * i) + (srcWidth * ht)) + wd + 1] * (-1 * (var[i + 1] + eps) ** 0.5)));\\n'\n",
      " '            end;\\n'\n",
      " '        end;\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        do ht = 0 to (srcHeight - 1) by 1;\\n'\n",
      " '            do wd = 0 to (srcWidth - 1) by 1;\\n'\n",
      " '                grad_mean_2[i + 1] = (grad_mean_2[i + 1] + (srcY[(((srcHeight * srcWidth) * i) + (srcWidth * ht)) + wd + 1] - mean[i + 1]));\\n'\n",
      " '            end;\\n'\n",
      " '        end;\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        grad_mean[i + 1] = (grad_mean[i + 1] + ((((grad_sigma[i + 1] * by_norm) * 2) * grad_mean_2[i + 1]) * -1));\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        do ht = 0 to (srcHeight - 1) by 1;\\n'\n",
      " '            do wd = 0 to (srcWidth - 1) by 1;\\n'\n",
      " '                srcDeltas_out[i + 1] = (srcDeltas_out[i + 1] + (srcY[(((srcHeight * srcWidth) * i) + (srcWidth * ht)) + wd + 1] - mean[i + 1]));\\n'\n",
      " '            end;\\n'\n",
      " '        end;\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    do i = 0 to (srcDepth - 1) by 1;\\n'\n",
      " '        srcDeltas_out[i + 1] = ((((deltas[i + 1] * (1 / (var[i + 1] + eps) ** 0.5)) + ((grad_sigma[i + 1] * by_norm) * 2)) + srcDeltas_out[i + 1]) + (grad_mean[i + 1] * by_norm));\\n'\n",
      " '    end;\\n'\n",
      " '\\n'\n",
      " '    return ;\\n'\n",
      " 'endsub;\\n')\n"
     ]
    }
   ],
   "source": [
    "backward_fcmp_code = python_to_fcmp(func=back_prop, print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services saved the file FCMPFUNCTION.sashdat in caslib CASUSER(weshiz).\n"
     ]
    }
   ],
   "source": [
    "# register forward and backward function together\n",
    "register_fcmp_routines(s,\n",
    "                       routine_code= forward_fcmp_code + backward_fcmp_code,\n",
    "                       function_tbl_name='fcmpfunction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training data\n",
    "In the case, we have Fashion MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ImageTable.load_files(conn=s, path='/cas/DeepLearn/weshiz/fashion-mnist/testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ColumnInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Column\">Column</th>\n",
       "      <th title=\"Label\">Label</th>\n",
       "      <th title=\"ID\">ID</th>\n",
       "      <th title=\"Type\">Type</th>\n",
       "      <th title=\"RawLength\">RawLength</th>\n",
       "      <th title=\"FormattedLength\">FormattedLength</th>\n",
       "      <th title=\"Format\">Format</th>\n",
       "      <th title=\"NFL\">NFL</th>\n",
       "      <th title=\"NFD\">NFD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_image_</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>varbinary(image)</td>\n",
       "      <td>837</td>\n",
       "      <td>837</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_label_</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>varchar</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_filename_0</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>varchar</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_id_</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>int64</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.000525s</span> &#183; <span class=\"cas-sys\">sys 0.000505s</span> &#183; <span class=\"cas-memory\">mem 0.803MB</span></small></p>"
      ],
      "text/plain": [
       "[ColumnInfo]\n",
       "\n",
       "         Column Label  ID              Type  RawLength  FormattedLength Format  \\\n",
       " 0      _image_         1  varbinary(image)        837              837          \n",
       " 1      _label_         2           varchar          1                1          \n",
       " 2  _filename_0         3           varchar          8                8          \n",
       " 3         _id_         4             int64          8               12          \n",
       " \n",
       "    NFL  NFD  \n",
       " 0    0    0  \n",
       " 1    0    0  \n",
       " 2    0    0  \n",
       " 3    0    0  \n",
       "\n",
       "+ Elapsed: 0.000525s, sys: 0.000505s, mem: 0.803mb"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Input layer added.\n",
      "NOTE: Convolution layer added.\n",
      "NOTE: FCMP layer added.\n",
      "NOTE: Pooling layer added.\n",
      "NOTE: Convolution layer added.\n",
      "NOTE: Pooling layer added.\n",
      "NOTE: Fully-connected layer added.\n",
      "NOTE: Fully-connected layer added.\n",
      "NOTE: Output layer added.\n",
      "NOTE: Model compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(conn=s, model_table='lenet5')\n",
    "\n",
    "model.add(InputLayer(n_channels=1, width=28, height=28, scale=1.0/255, offsets=None,\n",
    "                     random_flip=None, random_crop=None))\n",
    "\n",
    "model.add(Conv2d(n_filters=6, width=3, height=3, stride=1))\n",
    "\n",
    "# instance normalizationi\n",
    "model.add(FCMPLayer(width=28, height=28, depth=6, n_weights=0, \n",
    "                    forward_func='forward_prop', backward_func='back_prop',\n",
    "                    name='FCMPLayer1'))\n",
    "\n",
    "model.add(Pooling(width=2, height=2, stride=2, pool='max'))\n",
    "\n",
    "model.add(Conv2d(n_filters=16, width=5, height=5, stride=1))\n",
    "model.add(Pooling(width=2, height=2, stride=2, pool='max'))\n",
    "\n",
    "model.add(Dense(n=120))\n",
    "model.add(Dense(n=84))\n",
    "\n",
    "model.add(OutputLayer(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = VanillaSolver(clip_grad_max=100, clip_grad_min=-100, learning_rate=0.001,\n",
    "                       learning_rate_policy='step', gamma=0.1, step_size=30)\n",
    "optimizer = Optimizer(algorithm=solver, seed=13309, max_epochs=15, log_level=3, mini_batch_size=32, reg_l2=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Training from scratch.\n",
      "NOTE:  Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 107690.\n",
      "NOTE:  The approximate memory cost is 43.00 MB.\n",
      "NOTE:  Loading weights cost       0.00 (s).\n",
      "NOTE:  Initializing each layer cost       0.03 (s).\n",
      "NOTE:  The total number of threads on each worker is 8.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 32.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 256.\n",
      "NOTE:  Target variable: _label_\n",
      "NOTE:  Number of levels for the target variable:     10\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: 0\n",
      "NOTE:  Level      1: 1\n",
      "NOTE:  Level      2: 2\n",
      "NOTE:  Level      3: 3\n",
      "NOTE:  Level      4: 4\n",
      "NOTE:  Level      5: 5\n",
      "NOTE:  Level      6: 6\n",
      "NOTE:  Level      7: 7\n",
      "NOTE:  Level      8: 8\n",
      "NOTE:  Level      9: 9\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of numeric input variables:      1\n",
      "NOTE:  Number of FCMP layers in model: 1\n",
      "NOTE:  FCMP layer 'fcmplayer1' has input tensor size: width=28, height=28, depth=6\n",
      "NOTE:  FCMP layer 'fcmplayer1' has output tensor size: width=28, height=28, depth=6\n",
      "NOTE:  FCMP layer 'fcmplayer1' has 0 weights.\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001            2.634     0.9258    0.02386     0.08\n",
      "NOTE:      1   256    0.001            2.501     0.8516    0.02386     0.07\n",
      "NOTE:      2   256    0.001            2.434     0.8477    0.02386     0.06\n",
      "NOTE:      3   256    0.001            2.515     0.8672    0.02386     0.06\n",
      "NOTE:      4   256    0.001            2.405     0.8711    0.02386     0.06\n",
      "NOTE:      5   256    0.001            2.522     0.8945    0.02386     0.07\n",
      "NOTE:      6   256    0.001            2.433     0.8672    0.02386     0.07\n",
      "NOTE:      7   256    0.001            2.491     0.9102    0.02386     0.07\n",
      "NOTE:      8   256    0.001            2.417     0.8945    0.02386     0.06\n",
      "NOTE:      9   256    0.001            2.435     0.8828    0.02386     0.06\n",
      "NOTE:     10   256    0.001            2.349     0.8906    0.02386     0.06\n",
      "NOTE:     11   256    0.001            2.396     0.8711    0.02386     0.07\n",
      "NOTE:     12   256    0.001            2.374     0.8867    0.02386     0.08\n",
      "NOTE:     13   256    0.001            2.416     0.9141    0.02386     0.06\n",
      "NOTE:     14   256    0.001            2.384     0.8555    0.02386     0.07\n",
      "NOTE:     15   256    0.001            2.341     0.8711    0.02386     0.07\n",
      "NOTE:     16   256    0.001            2.377     0.8906    0.02386     0.06\n",
      "NOTE:     17   256    0.001            2.342     0.8555    0.02386     0.07\n",
      "NOTE:     18   256    0.001            2.267     0.8594    0.02386     0.06\n",
      "NOTE:     19   256    0.001            2.324     0.8555    0.02386     0.07\n",
      "NOTE:     20   256    0.001            2.308     0.8359    0.02386     0.07\n",
      "NOTE:     21   256    0.001            2.287     0.8555    0.02386     0.07\n",
      "NOTE:     22   256    0.001            2.275     0.8398    0.02386     0.07\n",
      "NOTE:     23   256    0.001            2.276     0.8008    0.02386     0.07\n",
      "NOTE:     24   256    0.001            2.276     0.8047    0.02386     0.08\n",
      "NOTE:     25   256    0.001            2.293     0.8438    0.02386     0.07\n",
      "NOTE:     26   256    0.001            2.214      0.793    0.02385     0.06\n",
      "NOTE:     27   256    0.001             2.22     0.8242    0.02385     0.07\n",
      "NOTE:     28   256    0.001            2.258     0.8242    0.02385     0.06\n",
      "NOTE:     29   256    0.001            2.252     0.8359    0.02385     0.06\n",
      "NOTE:     30   256    0.001            2.246      0.793    0.02385     0.06\n",
      "NOTE:     31   256    0.001            2.174     0.7813    0.02385     0.06\n",
      "NOTE:     32   256    0.001            2.204     0.8242    0.02385     0.06\n",
      "NOTE:     33   256    0.001             2.17     0.8125    0.02385     0.06\n",
      "NOTE:     34   256    0.001            2.194     0.7891    0.02385     0.06\n",
      "NOTE:     35   256    0.001            2.142     0.7461    0.02385     0.06\n",
      "NOTE:     36   256    0.001            2.162     0.7539    0.02385     0.06\n",
      "NOTE:     37   256    0.001            2.183     0.7891    0.02385     0.08\n",
      "NOTE:     38   256    0.001            2.188     0.7695    0.02385     0.06\n",
      "NOTE:     39   256    0.001            2.151     0.7148    0.02385     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  0         0.001           2.321     0.8398     2.65\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001            2.149     0.7383    0.02385     0.06\n",
      "NOTE:      1   256    0.001            2.145     0.7383    0.02386     0.07\n",
      "NOTE:      2   256    0.001            2.157     0.7773    0.02386     0.07\n",
      "NOTE:      3   256    0.001            2.149     0.7539    0.02386     0.06\n",
      "NOTE:      4   256    0.001            2.134     0.7539    0.02386     0.06\n",
      "NOTE:      5   256    0.001            2.115     0.7227    0.02386     0.06\n",
      "NOTE:      6   256    0.001            2.056     0.6758    0.02386     0.06\n",
      "NOTE:      7   256    0.001            2.107     0.7227    0.02386     0.07\n",
      "NOTE:      8   256    0.001            2.118       0.75    0.02386     0.06\n",
      "NOTE:      9   256    0.001            2.078     0.7109    0.02386     0.06\n",
      "NOTE:     10   256    0.001            2.094     0.7656    0.02386     0.08\n",
      "NOTE:     11   256    0.001            2.071     0.6836    0.02386     0.06\n",
      "NOTE:     12   256    0.001            2.082     0.7422    0.02386     0.06\n",
      "NOTE:     13   256    0.001            2.084     0.7188    0.02386     0.06\n",
      "NOTE:     14   256    0.001            2.058     0.7031    0.02386     0.06\n",
      "NOTE:     15   256    0.001            2.067     0.7227    0.02386     0.06\n",
      "NOTE:     16   256    0.001            2.054     0.6602    0.02386     0.06\n",
      "NOTE:     17   256    0.001            2.053     0.7305    0.02386     0.06\n",
      "NOTE:     18   256    0.001            2.022     0.6641    0.02386     0.06\n",
      "NOTE:     19   256    0.001            2.009      0.668    0.02386     0.06\n",
      "NOTE:     20   256    0.001            2.009     0.6602    0.02386     0.07\n",
      "NOTE:     21   256    0.001            2.003      0.668    0.02386     0.07\n",
      "NOTE:     22   256    0.001            1.979     0.6445    0.02386     0.08\n",
      "NOTE:     23   256    0.001            1.972     0.6133    0.02386     0.06\n",
      "NOTE:     24   256    0.001            1.992     0.6953    0.02386     0.06\n",
      "NOTE:     25   256    0.001            1.971     0.6211    0.02386     0.06\n",
      "NOTE:     26   256    0.001            1.967     0.6484    0.02386     0.06\n",
      "NOTE:     27   256    0.001            1.964     0.6406    0.02386     0.06\n",
      "NOTE:     28   256    0.001            1.935     0.6016    0.02386     0.06\n",
      "NOTE:     29   256    0.001             1.96     0.5938    0.02386     0.06\n",
      "NOTE:     30   256    0.001            1.971     0.6289    0.02386     0.06\n",
      "NOTE:     31   256    0.001            1.978     0.6719    0.02386     0.06\n",
      "NOTE:     32   256    0.001            1.974     0.6563    0.02386     0.06\n",
      "NOTE:     33   256    0.001            1.987     0.6563    0.02386     0.06\n",
      "NOTE:     34   256    0.001            1.945     0.6094    0.02386     0.06\n",
      "NOTE:     35   256    0.001            1.939     0.6367    0.02386     0.08\n",
      "NOTE:     36   256    0.001            1.931     0.5859    0.02386     0.06\n",
      "NOTE:     37   256    0.001            1.989     0.6875    0.02386     0.06\n",
      "NOTE:     38   256    0.001            1.913     0.6055    0.02386     0.06\n",
      "NOTE:     39   256    0.001            1.938     0.6289    0.02386     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  1         0.001           2.028     0.6789     2.61\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001             1.97     0.6563    0.02386     0.06\n",
      "NOTE:      1   256    0.001            1.906     0.5859    0.02386     0.06\n",
      "NOTE:      2   256    0.001            1.915     0.6133    0.02386     0.06\n",
      "NOTE:      3   256    0.001            1.949     0.6211    0.02386     0.06\n",
      "NOTE:      4   256    0.001            1.916     0.6094    0.02386     0.06\n",
      "NOTE:      5   256    0.001             1.91     0.5586    0.02386     0.06\n",
      "NOTE:      6   256    0.001            1.923     0.6016    0.02386     0.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      7   256    0.001            1.883      0.543    0.02386     0.06\n",
      "NOTE:      8   256    0.001            1.892     0.5938    0.02386     0.08\n",
      "NOTE:      9   256    0.001            1.914     0.5938    0.02386     0.06\n",
      "NOTE:     10   256    0.001            1.895     0.5977    0.02386     0.06\n",
      "NOTE:     11   256    0.001            1.855     0.5742    0.02386     0.06\n",
      "NOTE:     12   256    0.001            1.826     0.5273    0.02386     0.06\n",
      "NOTE:     13   256    0.001            1.869     0.5586    0.02386     0.06\n",
      "NOTE:     14   256    0.001            1.881      0.625    0.02386     0.06\n",
      "NOTE:     15   256    0.001            1.818      0.543    0.02386     0.06\n",
      "NOTE:     16   256    0.001            1.885      0.582    0.02386     0.06\n",
      "NOTE:     17   256    0.001            1.872     0.5859    0.02386     0.06\n",
      "NOTE:     18   256    0.001            1.862     0.5703    0.02386     0.07\n",
      "NOTE:     19   256    0.001            1.825     0.5234    0.02386     0.07\n",
      "NOTE:     20   256    0.001            1.844     0.5508    0.02387     0.08\n",
      "NOTE:     21   256    0.001            1.868     0.5547    0.02387     0.06\n",
      "NOTE:     22   256    0.001            1.786     0.5117    0.02387     0.06\n",
      "NOTE:     23   256    0.001            1.852     0.5938    0.02387     0.06\n",
      "NOTE:     24   256    0.001            1.842     0.5742    0.02387     0.07\n",
      "NOTE:     25   256    0.001            1.813     0.5078    0.02387     0.07\n",
      "NOTE:     26   256    0.001            1.797     0.5117    0.02387     0.06\n",
      "NOTE:     27   256    0.001             1.84      0.582    0.02387     0.07\n",
      "NOTE:     28   256    0.001            1.798     0.5078    0.02387     0.06\n",
      "NOTE:     29   256    0.001            1.769     0.4883    0.02387     0.06\n",
      "NOTE:     30   256    0.001            1.729     0.4805    0.02387     0.06\n",
      "NOTE:     31   256    0.001            1.752     0.5078    0.02387     0.06\n",
      "NOTE:     32   256    0.001            1.775     0.4961    0.02387     0.07\n",
      "NOTE:     33   256    0.001             1.78     0.5586    0.02387     0.08\n",
      "NOTE:     34   256    0.001             1.76     0.4766    0.02387     0.06\n",
      "NOTE:     35   256    0.001            1.809      0.543    0.02387     0.07\n",
      "NOTE:     36   256    0.001             1.74     0.4375    0.02387     0.07\n",
      "NOTE:     37   256    0.001            1.773     0.5195    0.02387     0.06\n",
      "NOTE:     38   256    0.001             1.78     0.5117    0.02387     0.06\n",
      "NOTE:     39   256    0.001            1.762     0.4961    0.02387     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  2         0.001           1.841     0.5519     2.62\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001            1.739     0.4766    0.02387     0.06\n",
      "NOTE:      1   256    0.001             1.79      0.543    0.02387     0.06\n",
      "NOTE:      2   256    0.001             1.72     0.4688    0.02387     0.06\n",
      "NOTE:      3   256    0.001            1.682     0.4297    0.02387     0.06\n",
      "NOTE:      4   256    0.001            1.715     0.5078    0.02387     0.06\n",
      "NOTE:      5   256    0.001            1.741     0.5313    0.02387     0.07\n",
      "NOTE:      6   256    0.001            1.676     0.4609    0.02387     0.07\n",
      "NOTE:      7   256    0.001            1.724     0.5117    0.02387     0.06\n",
      "NOTE:      8   256    0.001            1.692     0.4883    0.02387     0.06\n",
      "NOTE:      9   256    0.001            1.682     0.4805    0.02387     0.06\n",
      "NOTE:     10   256    0.001            1.718     0.4648    0.02387     0.07\n",
      "NOTE:     11   256    0.001            1.665     0.4922    0.02387     0.06\n",
      "NOTE:     12   256    0.001            1.675      0.457    0.02387     0.06\n",
      "NOTE:     13   256    0.001            1.665      0.457    0.02387     0.06\n",
      "NOTE:     14   256    0.001             1.71     0.4805    0.02388     0.06\n",
      "NOTE:     15   256    0.001             1.68     0.4453    0.02388     0.07\n",
      "NOTE:     16   256    0.001            1.665     0.4844    0.02388     0.07\n",
      "NOTE:     17   256    0.001            1.635     0.4648    0.02388     0.07\n",
      "NOTE:     18   256    0.001            1.684     0.4805    0.02388     0.06\n",
      "NOTE:     19   256    0.001            1.692     0.4727    0.02388     0.06\n",
      "NOTE:     20   256    0.001            1.654     0.4453    0.02388     0.06\n",
      "NOTE:     21   256    0.001            1.671     0.4609    0.02388     0.06\n",
      "NOTE:     22   256    0.001            1.684     0.4531    0.02388     0.06\n",
      "NOTE:     23   256    0.001            1.613     0.4414    0.02388     0.06\n",
      "NOTE:     24   256    0.001            1.623     0.4453    0.02388     0.06\n",
      "NOTE:     25   256    0.001            1.668     0.5078    0.02388     0.06\n",
      "NOTE:     26   256    0.001            1.574     0.4063    0.02388     0.06\n",
      "NOTE:     27   256    0.001            1.671     0.5313    0.02388     0.06\n",
      "NOTE:     28   256    0.001            1.597     0.4141    0.02388     0.06\n",
      "NOTE:     29   256    0.001            1.633     0.4688    0.02388     0.06\n",
      "NOTE:     30   256    0.001            1.623     0.4375    0.02388     0.06\n",
      "NOTE:     31   256    0.001            1.593     0.4141    0.02388     0.06\n",
      "NOTE:     32   256    0.001            1.597     0.3945    0.02388     0.06\n",
      "NOTE:     33   256    0.001            1.547     0.3945    0.02388     0.06\n",
      "NOTE:     34   256    0.001            1.679     0.4844    0.02388     0.07\n",
      "NOTE:     35   256    0.001            1.621     0.4531    0.02388     0.07\n",
      "NOTE:     36   256    0.001            1.584     0.4883    0.02388     0.06\n",
      "NOTE:     37   256    0.001            1.596     0.4102    0.02388     0.06\n",
      "NOTE:     38   256    0.001            1.536     0.4023    0.02388     0.06\n",
      "NOTE:     39   256    0.001             1.57     0.4297    0.02388     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  3         0.001           1.657      0.462     2.57\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001            1.537     0.4102    0.02388     0.06\n",
      "NOTE:      1   256    0.001            1.626     0.4766    0.02389     0.06\n",
      "NOTE:      2   256    0.001            1.601     0.4297    0.02389     0.06\n",
      "NOTE:      3   256    0.001            1.524     0.4219    0.02389     0.06\n",
      "NOTE:      4   256    0.001            1.537     0.3828    0.02389     0.06\n",
      "NOTE:      5   256    0.001            1.571      0.457    0.02389     0.06\n",
      "NOTE:      6   256    0.001            1.598     0.4688    0.02389     0.06\n",
      "NOTE:      7   256    0.001            1.538     0.4102    0.02389     0.06\n",
      "NOTE:      8   256    0.001            1.535     0.4336    0.02389     0.06\n",
      "NOTE:      9   256    0.001            1.504     0.3906    0.02389     0.06\n",
      "NOTE:     10   256    0.001            1.515     0.4219    0.02389     0.06\n",
      "NOTE:     11   256    0.001            1.472     0.3867    0.02389     0.06\n",
      "NOTE:     12   256    0.001            1.486      0.418    0.02389     0.06\n",
      "NOTE:     13   256    0.001            1.548     0.4297    0.02389     0.07\n",
      "NOTE:     14   256    0.001             1.51     0.4102    0.02389     0.07\n",
      "NOTE:     15   256    0.001              1.5     0.3906    0.02389     0.06\n",
      "NOTE:     16   256    0.001            1.591     0.4258    0.02389     0.06\n",
      "NOTE:     17   256    0.001            1.508     0.4297    0.02389     0.06\n",
      "NOTE:     18   256    0.001            1.491     0.3594    0.02389     0.06\n",
      "NOTE:     19   256    0.001            1.459     0.3711    0.02389     0.06\n",
      "NOTE:     20   256    0.001            1.522     0.4063    0.02389     0.06\n",
      "NOTE:     21   256    0.001            1.532      0.418    0.02389     0.06\n",
      "NOTE:     22   256    0.001            1.502     0.3711    0.02389     0.06\n",
      "NOTE:     23   256    0.001            1.464      0.418    0.02389     0.06\n",
      "NOTE:     24   256    0.001            1.489     0.3789    0.02389     0.06\n",
      "NOTE:     25   256    0.001            1.454     0.4063    0.02389     0.06\n",
      "NOTE:     26   256    0.001            1.485     0.4023    0.02389     0.06\n",
      "NOTE:     27   256    0.001            1.474     0.3555     0.0239     0.06\n",
      "NOTE:     28   256    0.001            1.502      0.418     0.0239     0.06\n",
      "NOTE:     29   256    0.001            1.421      0.375     0.0239     0.06\n",
      "NOTE:     30   256    0.001            1.454     0.3516     0.0239     0.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:     31   256    0.001            1.464     0.3867     0.0239     0.06\n",
      "NOTE:     32   256    0.001            1.424     0.4063     0.0239     0.07\n",
      "NOTE:     33   256    0.001            1.436     0.4102     0.0239     0.07\n",
      "NOTE:     34   256    0.001            1.383     0.3125     0.0239     0.06\n",
      "NOTE:     35   256    0.001             1.42     0.3594     0.0239     0.06\n",
      "NOTE:     36   256    0.001            1.433     0.3984     0.0239     0.06\n",
      "NOTE:     37   256    0.001            1.477     0.4102     0.0239     0.06\n",
      "NOTE:     38   256    0.001            1.452     0.3516     0.0239     0.06\n",
      "NOTE:     39   256    0.001            1.397     0.3438     0.0239     0.07\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  4         0.001           1.496     0.4001     2.56\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001            1.423     0.3906     0.0239     0.06\n",
      "NOTE:      1   256    0.001            1.465     0.4258     0.0239     0.06\n",
      "NOTE:      2   256    0.001            1.464     0.4336     0.0239     0.06\n",
      "NOTE:      3   256    0.001            1.436     0.3945     0.0239     0.06\n",
      "NOTE:      4   256    0.001            1.329     0.3438     0.0239     0.06\n",
      "NOTE:      5   256    0.001            1.409     0.4023     0.0239     0.06\n",
      "NOTE:      6   256    0.001            1.387     0.3633     0.0239     0.06\n",
      "NOTE:      7   256    0.001            1.407     0.3516     0.0239     0.06\n",
      "NOTE:      8   256    0.001            1.439      0.375     0.0239     0.06\n",
      "NOTE:      9   256    0.001            1.387     0.3359     0.0239     0.06\n",
      "NOTE:     10   256    0.001            1.345     0.3516     0.0239     0.06\n",
      "NOTE:     11   256    0.001            1.363     0.3398    0.02391     0.07\n",
      "NOTE:     12   256    0.001            1.394     0.3828    0.02391     0.06\n",
      "NOTE:     13   256    0.001            1.453     0.4219    0.02391     0.06\n",
      "NOTE:     14   256    0.001            1.293     0.3164    0.02391     0.06\n",
      "NOTE:     15   256    0.001            1.326     0.3438    0.02391     0.06\n",
      "NOTE:     16   256    0.001            1.451     0.3906    0.02391     0.06\n",
      "NOTE:     17   256    0.001            1.369     0.2891    0.02391     0.06\n",
      "NOTE:     18   256    0.001            1.343     0.3555    0.02391     0.06\n",
      "NOTE:     19   256    0.001            1.265     0.2773    0.02391     0.07\n",
      "NOTE:     20   256    0.001            1.331     0.3516    0.02391     0.06\n",
      "NOTE:     21   256    0.001            1.318     0.3281    0.02391     0.06\n",
      "NOTE:     22   256    0.001            1.337     0.3203    0.02391     0.06\n",
      "NOTE:     23   256    0.001            1.345     0.3477    0.02391     0.06\n",
      "NOTE:     24   256    0.001            1.344     0.3438    0.02391     0.06\n",
      "NOTE:     25   256    0.001            1.339     0.3633    0.02391     0.06\n",
      "NOTE:     26   256    0.001            1.361     0.3633    0.02391     0.06\n",
      "NOTE:     27   256    0.001            1.414      0.375    0.02391     0.06\n",
      "NOTE:     28   256    0.001            1.339     0.3633    0.02391     0.06\n",
      "NOTE:     29   256    0.001            1.331     0.3555    0.02391     0.06\n",
      "NOTE:     30   256    0.001              1.4     0.3945    0.02391     0.07\n",
      "NOTE:     31   256    0.001            1.388     0.4023    0.02391     0.07\n",
      "NOTE:     32   256    0.001            1.382     0.3906    0.02391     0.06\n",
      "NOTE:     33   256    0.001            1.343     0.3984    0.02391     0.06\n",
      "NOTE:     34   256    0.001            1.289     0.3555    0.02392     0.06\n",
      "NOTE:     35   256    0.001            1.333     0.3633    0.02392     0.06\n",
      "NOTE:     36   256    0.001            1.297     0.3086    0.02392     0.06\n",
      "NOTE:     37   256    0.001            1.328     0.3633    0.02392     0.06\n",
      "NOTE:     38   256    0.001            1.376     0.3438    0.02392     0.06\n",
      "NOTE:     39   256    0.001            1.293     0.3516    0.02392     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  5         0.001           1.366     0.3617     2.56\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001            1.307     0.3477    0.02392     0.06\n",
      "NOTE:      1   256    0.001             1.36     0.3906    0.02392     0.06\n",
      "NOTE:      2   256    0.001            1.308     0.3242    0.02392     0.06\n",
      "NOTE:      3   256    0.001            1.315      0.332    0.02392     0.06\n",
      "NOTE:      4   256    0.001            1.286     0.3438    0.02392     0.06\n",
      "NOTE:      5   256    0.001              1.3     0.3672    0.02392     0.06\n",
      "NOTE:      6   256    0.001            1.288      0.332    0.02392     0.06\n",
      "NOTE:      7   256    0.001            1.326     0.3711    0.02392     0.06\n",
      "NOTE:      8   256    0.001            1.306     0.3867    0.02392     0.07\n",
      "NOTE:      9   256    0.001            1.283     0.3203    0.02392     0.07\n",
      "NOTE:     10   256    0.001            1.293     0.3672    0.02392     0.06\n",
      "NOTE:     11   256    0.001            1.325      0.375    0.02392     0.06\n",
      "NOTE:     12   256    0.001             1.21     0.3242    0.02392     0.06\n",
      "NOTE:     13   256    0.001            1.316     0.3555    0.02392     0.06\n",
      "NOTE:     14   256    0.001            1.302     0.3398    0.02392     0.06\n",
      "NOTE:     15   256    0.001            1.306     0.3164    0.02392     0.06\n",
      "NOTE:     16   256    0.001            1.342     0.3281    0.02392     0.06\n",
      "NOTE:     17   256    0.001            1.281     0.3438    0.02393     0.06\n",
      "NOTE:     18   256    0.001            1.292     0.3477    0.02393     0.06\n",
      "NOTE:     19   256    0.001            1.226     0.3398    0.02393     0.06\n",
      "NOTE:     20   256    0.001            1.242     0.3164    0.02393     0.06\n",
      "NOTE:     21   256    0.001            1.259     0.3438    0.02393     0.07\n",
      "NOTE:     22   256    0.001            1.289     0.3984    0.02393     0.07\n",
      "NOTE:     23   256    0.001            1.263     0.3477    0.02393     0.06\n",
      "NOTE:     24   256    0.001            1.238     0.3359    0.02393     0.07\n",
      "NOTE:     25   256    0.001             1.22     0.2969    0.02393     0.06\n",
      "NOTE:     26   256    0.001            1.203     0.3008    0.02393     0.06\n",
      "NOTE:     27   256    0.001            1.291     0.3594    0.02393     0.07\n",
      "NOTE:     28   256    0.001            1.221     0.3203    0.02393     0.06\n",
      "NOTE:     29   256    0.001            1.223     0.3359    0.02393     0.06\n",
      "NOTE:     30   256    0.001            1.193     0.2969    0.02393     0.06\n",
      "NOTE:     31   256    0.001            1.212     0.3398    0.02393     0.06\n",
      "NOTE:     32   256    0.001            1.231     0.3125    0.02393     0.06\n",
      "NOTE:     33   256    0.001             1.27     0.3281    0.02393     0.06\n",
      "NOTE:     34   256    0.001            1.216     0.3086    0.02393     0.06\n",
      "NOTE:     35   256    0.001            1.244     0.3438    0.02393     0.06\n",
      "NOTE:     36   256    0.001            1.124     0.2461    0.02393     0.06\n",
      "NOTE:     37   256    0.001              1.2     0.3516    0.02393     0.06\n",
      "NOTE:     38   256    0.001            1.193     0.3359    0.02393     0.06\n",
      "NOTE:     39   256    0.001            1.208      0.332    0.02393     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  6         0.001           1.263     0.3376     2.57\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001            1.169     0.3047    0.02394     0.06\n",
      "NOTE:      1   256    0.001            1.162     0.3359    0.02394     0.06\n",
      "NOTE:      2   256    0.001            1.196      0.293    0.02394     0.06\n",
      "NOTE:      3   256    0.001            1.233     0.3477    0.02394     0.06\n",
      "NOTE:      4   256    0.001            1.165     0.2891    0.02394     0.06\n",
      "NOTE:      5   256    0.001            1.212     0.3086    0.02394     0.06\n",
      "NOTE:      6   256    0.001            1.116     0.2656    0.02394     0.07\n",
      "NOTE:      7   256    0.001            1.161     0.3125    0.02394     0.07\n",
      "NOTE:      8   256    0.001            1.224     0.3438    0.02394     0.06\n",
      "NOTE:      9   256    0.001            1.155     0.3125    0.02394     0.06\n",
      "NOTE:     10   256    0.001             1.26     0.3516    0.02394     0.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:     11   256    0.001            1.201     0.3594    0.02394     0.06\n",
      "NOTE:     12   256    0.001            1.164      0.293    0.02394     0.06\n",
      "NOTE:     13   256    0.001            1.233     0.3242    0.02394     0.06\n",
      "NOTE:     14   256    0.001             1.21     0.3672    0.02394     0.06\n",
      "NOTE:     15   256    0.001            1.179     0.3086    0.02394     0.06\n",
      "NOTE:     16   256    0.001            1.131     0.2773    0.02394     0.06\n",
      "NOTE:     17   256    0.001             1.21     0.3242    0.02394     0.06\n",
      "NOTE:     18   256    0.001             1.12     0.2656    0.02394     0.06\n",
      "NOTE:     19   256    0.001            1.125     0.3008    0.02394     0.06\n",
      "NOTE:     20   256    0.001            1.129     0.2813    0.02394     0.06\n",
      "NOTE:     21   256    0.001            1.168     0.3008    0.02394     0.06\n",
      "NOTE:     22   256    0.001            1.257     0.3398    0.02394     0.06\n",
      "NOTE:     23   256    0.001             1.17     0.3242    0.02394     0.06\n",
      "NOTE:     24   256    0.001             1.14     0.3008    0.02395     0.07\n",
      "NOTE:     25   256    0.001             1.08     0.2734    0.02395     0.08\n",
      "NOTE:     26   256    0.001            1.182     0.3516    0.02395     0.07\n",
      "NOTE:     27   256    0.001            1.048       0.25    0.02395     0.06\n",
      "NOTE:     28   256    0.001            1.164     0.2969    0.02395     0.06\n",
      "NOTE:     29   256    0.001            1.182     0.2969    0.02395     0.06\n",
      "NOTE:     30   256    0.001            1.103     0.2695    0.02395     0.07\n",
      "NOTE:     31   256    0.001            1.123     0.2891    0.02395     0.07\n",
      "NOTE:     32   256    0.001            1.171      0.293    0.02395     0.07\n",
      "NOTE:     33   256    0.001            1.094     0.2578    0.02395     0.06\n",
      "NOTE:     34   256    0.001            1.159     0.3477    0.02395     0.06\n",
      "NOTE:     35   256    0.001            1.149     0.3281    0.02395     0.06\n",
      "NOTE:     36   256    0.001            1.146     0.3203    0.02395     0.06\n",
      "NOTE:     37   256    0.001            1.213     0.3477    0.02395     0.06\n",
      "NOTE:     38   256    0.001            1.127     0.3203    0.02395     0.08\n",
      "NOTE:     39   256    0.001            1.138     0.3516    0.02395     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  7         0.001           1.164     0.3106     2.59\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001            1.157     0.3164    0.02395     0.06\n",
      "NOTE:      1   256    0.001            1.099      0.293    0.02395     0.06\n",
      "NOTE:      2   256    0.001            1.201     0.3281    0.02395     0.06\n",
      "NOTE:      3   256    0.001            1.032     0.2695    0.02395     0.07\n",
      "NOTE:      4   256    0.001            1.103     0.2578    0.02395     0.07\n",
      "NOTE:      5   256    0.001            1.193      0.332    0.02395     0.06\n",
      "NOTE:      6   256    0.001             1.09     0.2852    0.02395     0.06\n",
      "NOTE:      7   256    0.001            1.123     0.2891    0.02395     0.06\n",
      "NOTE:      8   256    0.001            1.118     0.2695    0.02396     0.06\n",
      "NOTE:      9   256    0.001            1.055     0.3164    0.02396     0.06\n",
      "NOTE:     10   256    0.001            1.187      0.332    0.02396     0.06\n",
      "NOTE:     11   256    0.001             1.03     0.2773    0.02396     0.08\n",
      "NOTE:     12   256    0.001            1.147     0.3281    0.02396     0.06\n",
      "NOTE:     13   256    0.001            1.084     0.3203    0.02396     0.06\n",
      "NOTE:     14   256    0.001            1.135     0.3555    0.02396     0.06\n",
      "NOTE:     15   256    0.001            1.048     0.2578    0.02396     0.06\n",
      "NOTE:     16   256    0.001            1.112     0.3203    0.02396     0.06\n",
      "NOTE:     17   256    0.001            1.082     0.2734    0.02396     0.06\n",
      "NOTE:     18   256    0.001            1.086     0.3125    0.02396     0.06\n",
      "NOTE:     19   256    0.001            1.088     0.3242    0.02396     0.06\n",
      "NOTE:     20   256    0.001             1.09     0.2969    0.02396     0.06\n",
      "NOTE:     21   256    0.001            1.137     0.3633    0.02396     0.06\n",
      "NOTE:     22   256    0.001            1.082      0.293    0.02396     0.07\n",
      "NOTE:     23   256    0.001            1.154     0.3438    0.02396     0.07\n",
      "NOTE:     24   256    0.001            1.005     0.2578    0.02396     0.07\n",
      "NOTE:     25   256    0.001            1.048     0.2695    0.02396     0.06\n",
      "NOTE:     26   256    0.001            1.053     0.2695    0.02396     0.06\n",
      "NOTE:     27   256    0.001            1.113     0.3164    0.02396     0.06\n",
      "NOTE:     28   256    0.001            1.122     0.3242    0.02396     0.06\n",
      "NOTE:     29   256    0.001            1.114      0.332    0.02396     0.06\n",
      "NOTE:     30   256    0.001            1.074     0.3125    0.02396     0.06\n",
      "NOTE:     31   256    0.001            1.047     0.3125    0.02396     0.06\n",
      "NOTE:     32   256    0.001             1.04     0.2852    0.02396     0.06\n",
      "NOTE:     33   256    0.001            1.051     0.3008    0.02397     0.06\n",
      "NOTE:     34   256    0.001            1.035     0.2852    0.02397     0.06\n",
      "NOTE:     35   256    0.001            1.069     0.2813    0.02397     0.06\n",
      "NOTE:     36   256    0.001             1.03     0.2852    0.02397     0.08\n",
      "NOTE:     37   256    0.001            1.036     0.2656    0.02397     0.06\n",
      "NOTE:     38   256    0.001            1.024     0.2656    0.02397     0.06\n",
      "NOTE:     39   256    0.001            1.067     0.2578    0.02397     0.07\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  8         0.001           1.089     0.2994     2.59\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001            1.029     0.2852    0.02397     0.07\n",
      "NOTE:      1   256    0.001            1.049     0.2773    0.02397     0.06\n",
      "NOTE:      2   256    0.001            1.024      0.293    0.02397     0.06\n",
      "NOTE:      3   256    0.001            1.092     0.3242    0.02397     0.06\n",
      "NOTE:      4   256    0.001            1.082     0.3359    0.02397     0.06\n",
      "NOTE:      5   256    0.001            1.124     0.3125    0.02397     0.06\n",
      "NOTE:      6   256    0.001           0.9369     0.2539    0.02397     0.06\n",
      "NOTE:      7   256    0.001             1.01       0.25    0.02397     0.07\n",
      "NOTE:      8   256    0.001           0.9786       0.25    0.02397     0.06\n",
      "NOTE:      9   256    0.001            1.017     0.2734    0.02397     0.08\n",
      "NOTE:     10   256    0.001            1.008       0.25    0.02397     0.06\n",
      "NOTE:     11   256    0.001            1.045     0.2969    0.02397     0.06\n",
      "NOTE:     12   256    0.001             1.03      0.293    0.02397     0.06\n",
      "NOTE:     13   256    0.001             1.02     0.2695    0.02397     0.06\n",
      "NOTE:     14   256    0.001            1.046     0.2656    0.02397     0.06\n",
      "NOTE:     15   256    0.001            1.087     0.2969    0.02397     0.06\n",
      "NOTE:     16   256    0.001            1.044     0.3047    0.02397     0.06\n",
      "NOTE:     17   256    0.001           0.9937     0.2422    0.02398     0.07\n",
      "NOTE:     18   256    0.001            1.001     0.2539    0.02398     0.06\n",
      "NOTE:     19   256    0.001            1.023     0.3477    0.02398     0.07\n",
      "NOTE:     20   256    0.001           0.9449     0.2461    0.02398     0.07\n",
      "NOTE:     21   256    0.001            1.026     0.3047    0.02398     0.08\n",
      "NOTE:     22   256    0.001            1.025     0.2656    0.02398     0.06\n",
      "NOTE:     23   256    0.001           0.9636     0.2266    0.02398     0.06\n",
      "NOTE:     24   256    0.001           0.9765     0.2383    0.02398     0.06\n",
      "NOTE:     25   256    0.001            1.008     0.3047    0.02398     0.06\n",
      "NOTE:     26   256    0.001            1.095     0.3281    0.02398     0.06\n",
      "NOTE:     27   256    0.001            1.108     0.3359    0.02398     0.06\n",
      "NOTE:     28   256    0.001           0.9512     0.2383    0.02398     0.06\n",
      "NOTE:     29   256    0.001             1.01     0.2773    0.02398     0.06\n",
      "NOTE:     30   256    0.001            1.036     0.3086    0.02398     0.06\n",
      "NOTE:     31   256    0.001           0.9778     0.2656    0.02398     0.06\n",
      "NOTE:     32   256    0.001           0.9917     0.2891    0.02398     0.07\n",
      "NOTE:     33   256    0.001            1.102     0.3477    0.02398     0.07\n",
      "NOTE:     34   256    0.001            1.044     0.2578    0.02398     0.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:     35   256    0.001            1.009     0.2695    0.02398     0.06\n",
      "NOTE:     36   256    0.001           0.9878     0.2734    0.02398     0.06\n",
      "NOTE:     37   256    0.001            1.043     0.2969    0.02398     0.07\n",
      "NOTE:     38   256    0.001           0.9814     0.2734    0.02398     0.06\n",
      "NOTE:     39   256    0.001            1.018     0.2813    0.02398     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  9         0.001           1.023     0.2826     2.62\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001           0.9323     0.2656    0.02398     0.06\n",
      "NOTE:      1   256    0.001           0.9685     0.2656    0.02398     0.06\n",
      "NOTE:      2   256    0.001           0.9651     0.2852    0.02398     0.06\n",
      "NOTE:      3   256    0.001            1.042     0.2734    0.02399     0.06\n",
      "NOTE:      4   256    0.001            0.974     0.2617    0.02399     0.06\n",
      "NOTE:      5   256    0.001           0.9676     0.2734    0.02399     0.06\n",
      "NOTE:      6   256    0.001           0.9995     0.2891    0.02399     0.06\n",
      "NOTE:      7   256    0.001           0.9239     0.2461    0.02399     0.07\n",
      "NOTE:      8   256    0.001            1.041     0.2852    0.02399     0.06\n",
      "NOTE:      9   256    0.001           0.9279     0.2266    0.02399     0.06\n",
      "NOTE:     10   256    0.001            1.051     0.3164    0.02399     0.06\n",
      "NOTE:     11   256    0.001           0.9806       0.25    0.02399     0.06\n",
      "NOTE:     12   256    0.001           0.8935      0.207    0.02399     0.06\n",
      "NOTE:     13   256    0.001           0.9697     0.3047    0.02399     0.06\n",
      "NOTE:     14   256    0.001           0.9461     0.2539    0.02399     0.06\n",
      "NOTE:     15   256    0.001           0.9803      0.293    0.02399     0.06\n",
      "NOTE:     16   256    0.001           0.9175     0.2031    0.02399     0.07\n",
      "NOTE:     17   256    0.001           0.9935     0.3164    0.02399     0.07\n",
      "NOTE:     18   256    0.001           0.9865     0.2695    0.02399     0.06\n",
      "NOTE:     19   256    0.001           0.9499       0.25    0.02399     0.08\n",
      "NOTE:     20   256    0.001            1.008     0.3438    0.02399     0.06\n",
      "NOTE:     21   256    0.001           0.9692     0.2773    0.02399     0.06\n",
      "NOTE:     22   256    0.001           0.9472     0.2734    0.02399     0.06\n",
      "NOTE:     23   256    0.001           0.9547     0.2773    0.02399     0.06\n",
      "NOTE:     24   256    0.001            0.993      0.293    0.02399     0.06\n",
      "NOTE:     25   256    0.001           0.9737      0.293    0.02399     0.06\n",
      "NOTE:     26   256    0.001           0.9874     0.2734    0.02399     0.06\n",
      "NOTE:     27   256    0.001            0.933     0.2656    0.02399     0.06\n",
      "NOTE:     28   256    0.001           0.9656     0.3086    0.02399     0.06\n",
      "NOTE:     29   256    0.001           0.8634     0.2344    0.02399     0.07\n",
      "NOTE:     30   256    0.001           0.9362     0.2422      0.024     0.06\n",
      "NOTE:     31   256    0.001           0.9456     0.2969      0.024     0.06\n",
      "NOTE:     32   256    0.001           0.9749     0.3008      0.024     0.08\n",
      "NOTE:     33   256    0.001           0.9148     0.2383      0.024     0.06\n",
      "NOTE:     34   256    0.001           0.9936     0.2891      0.024     0.07\n",
      "NOTE:     35   256    0.001           0.9373     0.2539      0.024     0.07\n",
      "NOTE:     36   256    0.001           0.9993     0.3047      0.024     0.07\n",
      "NOTE:     37   256    0.001           0.9169       0.25      0.024     0.06\n",
      "NOTE:     38   256    0.001            0.925       0.25      0.024     0.06\n",
      "NOTE:     39   256    0.001           0.8744      0.207      0.024     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  10        0.001          0.9606     0.2702     2.60\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001           0.9957      0.293      0.024     0.06\n",
      "NOTE:      1   256    0.001           0.9149     0.2734      0.024     0.06\n",
      "NOTE:      2   256    0.001            0.923     0.2852      0.024     0.06\n",
      "NOTE:      3   256    0.001            0.942       0.25      0.024     0.06\n",
      "NOTE:      4   256    0.001           0.9277     0.2695      0.024     0.07\n",
      "NOTE:      5   256    0.001           0.9388     0.2891      0.024     0.07\n",
      "NOTE:      6   256    0.001            1.011     0.3281      0.024     0.07\n",
      "NOTE:      7   256    0.001            0.991     0.2852      0.024     0.06\n",
      "NOTE:      8   256    0.001           0.9019     0.2461      0.024     0.06\n",
      "NOTE:      9   256    0.001           0.9298     0.2969      0.024     0.06\n",
      "NOTE:     10   256    0.001            0.855     0.2539      0.024     0.06\n",
      "NOTE:     11   256    0.001           0.8741     0.2461      0.024     0.07\n",
      "NOTE:     12   256    0.001           0.9241     0.2617      0.024     0.06\n",
      "NOTE:     13   256    0.001           0.9208       0.25      0.024     0.07\n",
      "NOTE:     14   256    0.001           0.9192     0.2344      0.024     0.07\n",
      "NOTE:     15   256    0.001           0.9504     0.2773      0.024     0.06\n",
      "NOTE:     16   256    0.001           0.8734     0.2422      0.024     0.06\n",
      "NOTE:     17   256    0.001            1.009     0.3086      0.024     0.08\n",
      "NOTE:     18   256    0.001           0.9085     0.2852    0.02401     0.06\n",
      "NOTE:     19   256    0.001           0.9845     0.3281    0.02401     0.07\n",
      "NOTE:     20   256    0.001           0.9479     0.2656    0.02401     0.06\n",
      "NOTE:     21   256    0.001           0.9056     0.2461    0.02401     0.07\n",
      "NOTE:     22   256    0.001           0.8324     0.2266    0.02401     0.06\n",
      "NOTE:     23   256    0.001           0.8557     0.2305    0.02401     0.07\n",
      "NOTE:     24   256    0.001           0.9497     0.3086    0.02401     0.07\n",
      "NOTE:     25   256    0.001           0.9364     0.2461    0.02401     0.06\n",
      "NOTE:     26   256    0.001           0.8866     0.2617    0.02401     0.06\n",
      "NOTE:     27   256    0.001           0.9867     0.3047    0.02401     0.06\n",
      "NOTE:     28   256    0.001           0.9367     0.2656    0.02401     0.06\n",
      "NOTE:     29   256    0.001           0.8626     0.2578    0.02401     0.07\n",
      "NOTE:     30   256    0.001           0.9018     0.2617    0.02401     0.07\n",
      "NOTE:     31   256    0.001           0.9702      0.293    0.02401     0.07\n",
      "NOTE:     32   256    0.001           0.9337     0.2734    0.02401     0.07\n",
      "NOTE:     33   256    0.001           0.9038     0.3008    0.02401     0.07\n",
      "NOTE:     34   256    0.001           0.8709     0.2422    0.02401     0.07\n",
      "NOTE:     35   256    0.001           0.9434     0.2969    0.02401     0.07\n",
      "NOTE:     36   256    0.001           0.7791     0.2305    0.02401     0.07\n",
      "NOTE:     37   256    0.001           0.8714     0.2852    0.02401     0.07\n",
      "NOTE:     38   256    0.001           0.9825     0.3125    0.02401     0.07\n",
      "NOTE:     39   256    0.001           0.9468      0.293    0.02401     0.07\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  11        0.001          0.9225     0.2727     2.63\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001           0.8726       0.25    0.02401     0.07\n",
      "NOTE:      1   256    0.001           0.7977     0.2266    0.02401     0.07\n",
      "NOTE:      2   256    0.001           0.8855     0.2656    0.02401     0.08\n",
      "NOTE:      3   256    0.001           0.8876       0.25    0.02401     0.06\n",
      "NOTE:      4   256    0.001           0.8731     0.2539    0.02401     0.06\n",
      "NOTE:      5   256    0.001            0.885     0.2734    0.02401     0.06\n",
      "NOTE:      6   256    0.001            0.884     0.2656    0.02401     0.06\n",
      "NOTE:      7   256    0.001           0.8762     0.2539    0.02402     0.06\n",
      "NOTE:      8   256    0.001           0.8387     0.2422    0.02402     0.06\n",
      "NOTE:      9   256    0.001           0.9503     0.2891    0.02402     0.07\n",
      "NOTE:     10   256    0.001           0.8259     0.2227    0.02402     0.07\n",
      "NOTE:     11   256    0.001           0.8986     0.2891    0.02402     0.06\n",
      "NOTE:     12   256    0.001           0.8341     0.2344    0.02402     0.06\n",
      "NOTE:     13   256    0.001             0.89     0.2461    0.02402     0.06\n",
      "NOTE:     14   256    0.001           0.8543     0.2461    0.02402     0.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:     15   256    0.001           0.9055     0.2813    0.02402     0.07\n",
      "NOTE:     16   256    0.001           0.8626     0.2422    0.02402     0.06\n",
      "NOTE:     17   256    0.001           0.8408     0.2539    0.02402     0.06\n",
      "NOTE:     18   256    0.001           0.8919       0.25    0.02402     0.07\n",
      "NOTE:     19   256    0.001           0.9155     0.2852    0.02402     0.06\n",
      "NOTE:     20   256    0.001           0.8465     0.2656    0.02402     0.06\n",
      "NOTE:     21   256    0.001           0.8858     0.2656    0.02402     0.06\n",
      "NOTE:     22   256    0.001           0.8891     0.2656    0.02402     0.06\n",
      "NOTE:     23   256    0.001           0.9437     0.2734    0.02402     0.06\n",
      "NOTE:     24   256    0.001           0.8488     0.2109    0.02402     0.06\n",
      "NOTE:     25   256    0.001           0.8619     0.2539    0.02402     0.06\n",
      "NOTE:     26   256    0.001           0.8259     0.2539    0.02402     0.06\n",
      "NOTE:     27   256    0.001           0.8676       0.25    0.02402     0.08\n",
      "NOTE:     28   256    0.001           0.8467     0.2773    0.02402     0.07\n",
      "NOTE:     29   256    0.001           0.7296     0.1953    0.02402     0.07\n",
      "NOTE:     30   256    0.001           0.8629     0.2383    0.02402     0.07\n",
      "NOTE:     31   256    0.001           0.9015     0.2539    0.02402     0.06\n",
      "NOTE:     32   256    0.001           0.8712     0.2617    0.02402     0.06\n",
      "NOTE:     33   256    0.001           0.8269     0.2383    0.02402     0.06\n",
      "NOTE:     34   256    0.001           0.9232     0.2969    0.02402     0.06\n",
      "NOTE:     35   256    0.001           0.9721     0.3281    0.02402     0.07\n",
      "NOTE:     36   256    0.001           0.8542     0.2578    0.02402     0.06\n",
      "NOTE:     37   256    0.001           0.8354     0.2305    0.02402     0.06\n",
      "NOTE:     38   256    0.001           0.7798     0.2227    0.02403     0.06\n",
      "NOTE:     39   256    0.001           0.8381     0.2539    0.02403     0.07\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  12        0.001           0.867     0.2554     2.63\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001           0.8236       0.25    0.02403     0.07\n",
      "NOTE:      1   256    0.001           0.8766     0.2773    0.02403     0.06\n",
      "NOTE:      2   256    0.001           0.9346     0.2969    0.02403     0.06\n",
      "NOTE:      3   256    0.001           0.7714     0.2266    0.02403     0.06\n",
      "NOTE:      4   256    0.001           0.9142     0.2852    0.02403     0.07\n",
      "NOTE:      5   256    0.001           0.8829     0.2578    0.02403     0.07\n",
      "NOTE:      6   256    0.001           0.8923     0.2813    0.02403     0.07\n",
      "NOTE:      7   256    0.001           0.8258     0.2422    0.02403     0.06\n",
      "NOTE:      8   256    0.001           0.8381     0.2539    0.02403     0.07\n",
      "NOTE:      9   256    0.001           0.8471       0.25    0.02403     0.06\n",
      "NOTE:     10   256    0.001           0.8841     0.2578    0.02403     0.06\n",
      "NOTE:     11   256    0.001           0.7572     0.2031    0.02403     0.06\n",
      "NOTE:     12   256    0.001           0.8202     0.2422    0.02403     0.08\n",
      "NOTE:     13   256    0.001            0.832     0.2461    0.02403     0.06\n",
      "NOTE:     14   256    0.001           0.7702     0.2227    0.02403     0.06\n",
      "NOTE:     15   256    0.001           0.7831     0.2344    0.02403     0.06\n",
      "NOTE:     16   256    0.001           0.8932     0.2773    0.02403     0.06\n",
      "NOTE:     17   256    0.001           0.8592     0.2852    0.02403     0.06\n",
      "NOTE:     18   256    0.001            0.822     0.2422    0.02403     0.06\n",
      "NOTE:     19   256    0.001           0.7796     0.2031    0.02403     0.06\n",
      "NOTE:     20   256    0.001           0.8441     0.2305    0.02403     0.06\n",
      "NOTE:     21   256    0.001           0.9048     0.3008    0.02403     0.06\n",
      "NOTE:     22   256    0.001           0.8111     0.2656    0.02403     0.06\n",
      "NOTE:     23   256    0.001           0.8475     0.2305    0.02403     0.06\n",
      "NOTE:     24   256    0.001            0.819     0.2734    0.02403     0.06\n",
      "NOTE:     25   256    0.001           0.8136     0.2344    0.02403     0.08\n",
      "NOTE:     26   256    0.001           0.8648     0.2305    0.02403     0.06\n",
      "NOTE:     27   256    0.001           0.8206     0.2773    0.02403     0.06\n",
      "NOTE:     28   256    0.001           0.8157     0.2422    0.02403     0.06\n",
      "NOTE:     29   256    0.001           0.8144     0.2578    0.02403     0.06\n",
      "NOTE:     30   256    0.001           0.7888     0.2031    0.02404     0.07\n",
      "NOTE:     31   256    0.001           0.9522     0.2773    0.02404     0.06\n",
      "NOTE:     32   256    0.001            0.826       0.25    0.02404     0.06\n",
      "NOTE:     33   256    0.001           0.8796     0.2617    0.02404     0.06\n",
      "NOTE:     34   256    0.001           0.8263     0.1992    0.02404     0.06\n",
      "NOTE:     35   256    0.001           0.7817     0.2227    0.02404     0.06\n",
      "NOTE:     36   256    0.001           0.8279     0.1992    0.02404     0.06\n",
      "NOTE:     37   256    0.001           0.9437     0.3125    0.02404     0.07\n",
      "NOTE:     38   256    0.001           0.8301     0.2617    0.02404     0.07\n",
      "NOTE:     39   256    0.001           0.7797     0.1992    0.02404     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  13        0.001            0.84     0.2491     2.60\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   256    0.001           0.8821     0.3008    0.02404     0.06\n",
      "NOTE:      1   256    0.001           0.8533       0.25    0.02404     0.06\n",
      "NOTE:      2   256    0.001           0.8277     0.2422    0.02404     0.06\n",
      "NOTE:      3   256    0.001           0.8502     0.2773    0.02404     0.06\n",
      "NOTE:      4   256    0.001           0.7677     0.2305    0.02404     0.07\n",
      "NOTE:      5   256    0.001           0.8868      0.293    0.02404     0.06\n",
      "NOTE:      6   256    0.001           0.8034     0.2461    0.02404     0.06\n",
      "NOTE:      7   256    0.001           0.8427     0.2344    0.02404     0.06\n",
      "NOTE:      8   256    0.001           0.7943     0.2227    0.02404     0.06\n",
      "NOTE:      9   256    0.001           0.8446       0.25    0.02404     0.06\n",
      "NOTE:     10   256    0.001           0.8191     0.2461    0.02404     0.08\n",
      "NOTE:     11   256    0.001           0.7657     0.2227    0.02404     0.06\n",
      "NOTE:     12   256    0.001           0.7999     0.2383    0.02404     0.06\n",
      "NOTE:     13   256    0.001           0.8456     0.2617    0.02404     0.06\n",
      "NOTE:     14   256    0.001           0.7609     0.2266    0.02404     0.06\n",
      "NOTE:     15   256    0.001            0.758      0.207    0.02404     0.06\n",
      "NOTE:     16   256    0.001           0.8359     0.2539    0.02404     0.06\n",
      "NOTE:     17   256    0.001           0.8432     0.2539    0.02404     0.06\n",
      "NOTE:     18   256    0.001           0.7753     0.2422    0.02404     0.06\n",
      "NOTE:     19   256    0.001           0.8606     0.2383    0.02404     0.06\n",
      "NOTE:     20   256    0.001           0.8525     0.3008    0.02404     0.06\n",
      "NOTE:     21   256    0.001           0.8027     0.2461    0.02404     0.06\n",
      "NOTE:     22   256    0.001           0.9232     0.2969    0.02404     0.06\n",
      "NOTE:     23   256    0.001           0.9137      0.332    0.02404     0.08\n",
      "NOTE:     24   256    0.001           0.7873     0.2461    0.02404     0.06\n",
      "NOTE:     25   256    0.001           0.7675     0.2461    0.02405     0.06\n",
      "NOTE:     26   256    0.001           0.9549     0.3516    0.02405     0.06\n",
      "NOTE:     27   256    0.001           0.8716     0.2891    0.02405     0.06\n",
      "NOTE:     28   256    0.001           0.8508     0.2539    0.02405     0.06\n",
      "NOTE:     29   256    0.001           0.8324       0.25    0.02405     0.06\n",
      "NOTE:     30   256    0.001           0.8677     0.2656    0.02405     0.06\n",
      "NOTE:     31   256    0.001           0.8347     0.2422    0.02405     0.06\n",
      "NOTE:     32   256    0.001           0.9168     0.3008    0.02405     0.06\n",
      "NOTE:     33   256    0.001           0.7948     0.2773    0.02405     0.06\n",
      "NOTE:     34   256    0.001           0.7527     0.2305    0.02405     0.06\n",
      "NOTE:     35   256    0.001           0.8276     0.2695    0.02405     0.06\n",
      "NOTE:     36   256    0.001           0.7856     0.2539    0.02405     0.08\n",
      "NOTE:     37   256    0.001           0.6856     0.1719    0.02405     0.06\n",
      "NOTE:     38   256    0.001           0.8968     0.2969    0.02405     0.06\n",
      "NOTE:     39   256    0.001           0.8536     0.2617    0.02405     0.06\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  14        0.001          0.8297      0.258     2.58\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is      38.96 (s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Name</td>\n",
       "      <td>lenet5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Type</td>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number of Layers</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number of Input Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number of Output Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Number of Convolutional Layers</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Number of Pooling Layers</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Number of Fully Connected Layers</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Number of FCMP Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Number of Weight Parameters</td>\n",
       "      <td>107454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Number of Bias Parameters</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Total Number of Model Parameters</td>\n",
       "      <td>107690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Approximate Memory Cost for Training (MB)</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OptIterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch\">Epoch</th>\n",
       "      <th title=\"LearningRate\">LearningRate</th>\n",
       "      <th title=\"Loss\">Loss</th>\n",
       "      <th title=\"FitError\">FitError</th>\n",
       "      <th title=\"L2Norm\">L2Norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.320703</td>\n",
       "      <td>0.839844</td>\n",
       "      <td>0.023856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.027957</td>\n",
       "      <td>0.678906</td>\n",
       "      <td>0.023857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.840856</td>\n",
       "      <td>0.551855</td>\n",
       "      <td>0.023865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.657178</td>\n",
       "      <td>0.462012</td>\n",
       "      <td>0.023877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.495914</td>\n",
       "      <td>0.400098</td>\n",
       "      <td>0.023892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.365940</td>\n",
       "      <td>0.361719</td>\n",
       "      <td>0.023909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.262792</td>\n",
       "      <td>0.337598</td>\n",
       "      <td>0.023926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.164276</td>\n",
       "      <td>0.310645</td>\n",
       "      <td>0.023943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.089045</td>\n",
       "      <td>0.299414</td>\n",
       "      <td>0.023960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.023368</td>\n",
       "      <td>0.282617</td>\n",
       "      <td>0.023976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.960597</td>\n",
       "      <td>0.270215</td>\n",
       "      <td>0.023991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.922474</td>\n",
       "      <td>0.272656</td>\n",
       "      <td>0.024006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.867012</td>\n",
       "      <td>0.255371</td>\n",
       "      <td>0.024019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.839970</td>\n",
       "      <td>0.249121</td>\n",
       "      <td>0.024032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.829734</td>\n",
       "      <td>0.258008</td>\n",
       "      <td>0.024044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(weshiz)</td>\n",
       "      <td>lenet5_weights</td>\n",
       "      <td>107690</td>\n",
       "      <td>3</td>\n",
       "      <td>CASTable('lenet5_weights', caslib='CASUSER(wes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 39s</span> &#183; <span class=\"cas-user\">user 302s</span> &#183; <span class=\"cas-sys\">sys 0.775s</span> &#183; <span class=\"cas-memory\">mem 60.6MB</span></small></p>"
      ],
      "text/plain": [
       "[ModelInfo]\n",
       "\n",
       "                                         Descr                         Value\n",
       " 0                                  Model Name                        lenet5\n",
       " 1                                  Model Type  Convolutional Neural Network\n",
       " 2                            Number of Layers                             9\n",
       " 3                      Number of Input Layers                             1\n",
       " 4                     Number of Output Layers                             1\n",
       " 5              Number of Convolutional Layers                             2\n",
       " 6                    Number of Pooling Layers                             2\n",
       " 7            Number of Fully Connected Layers                             2\n",
       " 8                       Number of FCMP Layers                             1\n",
       " 9                 Number of Weight Parameters                        107454\n",
       " 10                  Number of Bias Parameters                           236\n",
       " 11           Total Number of Model Parameters                        107690\n",
       " 12  Approximate Memory Cost for Training (MB)                            43\n",
       "\n",
       "[OptIterHistory]\n",
       "\n",
       "     Epoch  LearningRate      Loss  FitError    L2Norm\n",
       " 0       1         0.001  2.320703  0.839844  0.023856\n",
       " 1       2         0.001  2.027957  0.678906  0.023857\n",
       " 2       3         0.001  1.840856  0.551855  0.023865\n",
       " 3       4         0.001  1.657178  0.462012  0.023877\n",
       " 4       5         0.001  1.495914  0.400098  0.023892\n",
       " 5       6         0.001  1.365940  0.361719  0.023909\n",
       " 6       7         0.001  1.262792  0.337598  0.023926\n",
       " 7       8         0.001  1.164276  0.310645  0.023943\n",
       " 8       9         0.001  1.089045  0.299414  0.023960\n",
       " 9      10         0.001  1.023368  0.282617  0.023976\n",
       " 10     11         0.001  0.960597  0.270215  0.023991\n",
       " 11     12         0.001  0.922474  0.272656  0.024006\n",
       " 12     13         0.001  0.867012  0.255371  0.024019\n",
       " 13     14         0.001  0.839970  0.249121  0.024032\n",
       " 14     15         0.001  0.829734  0.258008  0.024044\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib            Name    Rows  Columns  \\\n",
       " 0  CASUSER(weshiz)  lenet5_weights  107690        3   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('lenet5_weights', caslib='CASUSER(wes...  \n",
       "\n",
       "+ Elapsed: 39s, user: 302s, sys: 0.775s, mem: 60.6mb"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data=test, optimizer=optimizer, n_threads=8, target='_label_', inputs='_image_', nominals='_label_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f50a81daa58>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.plot_training_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 8.9e-05s</span> &#183; <span class=\"cas-sys\">sys 8e-05s</span> &#183; <span class=\"cas-memory\">mem 0.203MB</span></small></p>"
      ],
      "text/plain": [
       "+ Elapsed: 8.9e-05s, sys: 8e-05s, mem: 0.203mb"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.endsession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
